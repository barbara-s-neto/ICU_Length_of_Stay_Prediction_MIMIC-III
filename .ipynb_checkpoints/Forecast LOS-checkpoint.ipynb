{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1d1427",
   "metadata": {},
   "source": [
    "# ICU Length of Stay Prediction - MIMIC-III Pipeline\n",
    "\n",
    "## üéØ Objective\n",
    "Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n",
    "\n",
    "## üìä Data & Constraints\n",
    "- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n",
    "- **Filters**: \n",
    "        - Patient Age 18-80\n",
    "        - LOS 0.1-15 days\n",
    "        - Valid time sequences\n",
    "- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n",
    "\n",
    "\n",
    "## üåÄ Big Data Processing\n",
    "\n",
    "- **CHARTEVENTS**: Chart Events table has +330 million rows\n",
    "- **Parquet**: Converted to Parquet format for efficient storage and processing\n",
    "- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n",
    "\n",
    "## üîß Features (39 total)\n",
    "- **Demographics (2)**: Age, gender\n",
    "- **Admission (8)**: Emergency/elective, timing, insurance\n",
    "- **ICU Units (6)**: Care unit types, transfers\n",
    "- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n",
    "- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n",
    "- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n",
    "\n",
    "## ü§ñ Models & Results\n",
    "- **Linear Regression**: \n",
    "- **Random Forest**: \n",
    "\n",
    "## ‚òÅÔ∏è Infrastructure\n",
    "- **GCP Dataproc**: 6x e2-highmem-4 workers (28 vCPUs, 224GB RAM)\n",
    "- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8375f-7f35-415f-8288-2ff2193e6af0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed6638-09bf-4620-89fe-2bb2c00d86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Machine Learning imports\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression #, GBTRegressor\n",
    "#from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator #, MulticlassClassificationEvaluator\n",
    "#from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "#from pyspark.ml import Pipeline\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully!\")\n",
    "print(f\"‚è∞ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1810-67b2-471e-97d2-b8fda7db9728",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f1fca-5bf8-4e10-bdca-a6faa11ca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Forecast-LOS\") \\\n",
    "#     .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#     .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "#     .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "#     .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\") \\\n",
    "#     .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n",
    "#     .config(\"spark.sql.files.maxPartitionBytes\", \"64MB\") \\\n",
    "#     .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "#     .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "#     .config(\"spark.executor.memory\", \"12g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"2\") \\\n",
    "#     .config(\"spark.executor.instances\", \"8\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "#     .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n",
    "#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#     .config(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\") \\\n",
    "#     .config(\"spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin\", \"0.2\") \\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "#     .config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    "#     .config(\"spark.rpc.askTimeout\", \"600s\") \\\n",
    "#     .config(\"spark.network.timeoutInterval\", \"120s\") \\\n",
    "#     .config(\"spark.storage.blockManagerSlaveTimeoutMs\", \"120000\") \\\n",
    "#     .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "#     .config(\"spark.network.timeout\", \"120s\") \\\n",
    "#     .config(\"spark.rpc.lookupTimeout\", \"120s\") \\\n",
    "#     .config(\"spark.shuffle.registration.timeout\", \"120000\") \\\n",
    "#     .config(\"spark.shuffle.registration.maxAttempts\", \"5\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Forecast-LOS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session created successfully!\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"üîß Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üíæ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\n‚è∞ Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13163e-55dc-4046-95b0-4815723d0581",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62936c54-45ab-4c03-a8ec-fc3d87f68721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CHARTEVENTS (filtered): 57,973 rows\n",
      "üìä LABEVENTS (filtered): 5,895 rows\n",
      "\n",
      "‚è∞ Data loaded at: 2025-06-02 13:59:58\n"
     ]
    }
   ],
   "source": [
    "# Configuration flags\n",
    "SAMPLE_ENABLE = True\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "#MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n",
    "#MIMIC_PATH = \"./mimic-data\"\n",
    "MIMIC_PATH =  \"./mimic-db-short\"\n",
    "\n",
    "print(\"üè• Loading MIMIC-III CSV files...\")\n",
    "\n",
    "# Step 1: Load and sample ICUSTAYS first\n",
    "print(\"üìÇ Loading ICUSTAYS table...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "\n",
    "if SAMPLE_ENABLE:\n",
    "    print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "    icustays_df = icustays_df.limit(SAMPLE_SIZE) \n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "    print(f\"‚úÖ Sampled {actual_sample_size} ICU stays\")\n",
    "\n",
    "# Step 2: Get required IDs for filtering other tables\n",
    "print(\"üìã Extracting required IDs...\")\n",
    "icu_ids = icustays_df.select(\"ICUSTAY_ID\").rdd.map(lambda row: row[0]).collect()\n",
    "hadm_ids = icustays_df.select(\"HADM_ID\").rdd.map(lambda row: row[0]).collect()\n",
    "subject_ids = icustays_df.select(\"SUBJECT_ID\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "print(f\"üìä IDs to filter: {len(icu_ids)} ICUSTAY_IDs, {len(hadm_ids)} HADM_IDs, {len(subject_ids)} SUBJECT_IDs\")\n",
    "\n",
    "# Step 3: Load other tables with pre-filtering\n",
    "print(\"üìÇ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "patients_df = patients_df.filter(col(\"SUBJECT_ID\").isin(subject_ids))\n",
    "\n",
    "print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.filter(col(\"HADM_ID\").isin(hadm_ids))\n",
    "\n",
    "print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "diagnoses_df = diagnoses_df.filter(col(\"HADM_ID\").isin(hadm_ids))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Load large tables (CHARTEVENTS, LABEVENTS) with aggressive filtering\n",
    "print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "\n",
    "try:\n",
    "    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "except:\n",
    "    print(f\" Converting CHARTEVENTS.csv.gz to parquet...\")\n",
    "    chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "    chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "    \n",
    "icu_ids_df = spark.createDataFrame([(id,) for id in icu_ids], [\"ICUSTAY_ID\"])\n",
    "chartevents_df = chartevents_df \\\n",
    "    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "    .join(broadcast(icu_ids_df), \"ICUSTAY_ID\", \"inner\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "\n",
    "try:\n",
    "    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "except:\n",
    "    print(f\" Converting LABEVENTS.csv.gz to parquet...\")\n",
    "    labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "    labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "\n",
    "    \n",
    "labevents_df = labevents_df \\\n",
    "                .filter(col(\"HADM_ID\").isin(hadm_ids))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display final information\n",
    "print(\"\\n‚úÖ Tables loaded and filtered successfully!\")\n",
    "if SAMPLE_ENABLE:\n",
    "    print(f\"üéØ Using sample of {actual_sample_size} ICU stays\")\n",
    "    print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "    print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "    print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "    print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "    print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "    print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "\n",
    "print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e7157-1230-41d3-8f41-1117b62fdb55",
   "metadata": {},
   "source": [
    "## Features Engineering\n",
    "\n",
    "Current features for regression:\n",
    "\n",
    "- Demographics (age, gender)\n",
    "- Admission characteristics (emergency vs elective, timing)\n",
    "- ICU unit types and transfers\n",
    "- Time-based features (weekend, night admissions)\n",
    "- Medical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763e6d7-4ba5-439b-8e2b-ba16cf607a3e",
   "metadata": {},
   "source": [
    "## Extracting Data From ICUSTAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d7adb-eac0-4071-b252-a04268f0c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 1: Creating base ICU dataset with patient demographics...\")\n",
    "\n",
    "base_icu_df = icustays_df.alias(\"icu\") \\\n",
    "    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n",
    "    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        # ICU stay identifiers\n",
    "        col(\"icu.ICUSTAY_ID\"),\n",
    "        col(\"icu.SUBJECT_ID\"), \n",
    "        col(\"icu.HADM_ID\"),\n",
    "        \n",
    "        # Target variable - Length of Stay in ICU (days)\n",
    "        col(\"icu.LOS\").alias(\"ICU_LOS\"),\n",
    "        \n",
    "        # ICU characteristics\n",
    "        col(\"icu.FIRST_CAREUNIT\"),\n",
    "        col(\"icu.LAST_CAREUNIT\"), \n",
    "        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n",
    "        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n",
    "        \n",
    "        # Patient demographics\n",
    "        col(\"pat.GENDER\"),\n",
    "        col(\"pat.DOB\"),\n",
    "        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n",
    "        \n",
    "        # Admission details\n",
    "        col(\"adm.ADMITTIME\"),\n",
    "        col(\"adm.DISCHTIME\"), \n",
    "        col(\"adm.ADMISSION_TYPE\"),\n",
    "        col(\"adm.ADMISSION_LOCATION\"),\n",
    "        col(\"adm.INSURANCE\"),\n",
    "        col(\"adm.ETHNICITY\"),\n",
    "        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n",
    "        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n",
    "    )\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n",
    "                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n",
    "                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Created base ICU dataset!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595638e6-74f7-4f2f-a7e5-1fc692089206",
   "metadata": {},
   "source": [
    "## Extracting Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d34e6-83c3-40ae-95d0-71f9929397a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base ICU Dataset - Categorical Features\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 2: Engineering categorical features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"ETHNICITY_ENCODED\",\n",
    "                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1) \\\n",
    "                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2) \\\n",
    "                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3) \\\n",
    "                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4) \\\n",
    "                .otherwise(5))\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Categorical Features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca22bc8-8dd5-4d30-9cb9-0f366da21d76",
   "metadata": {},
   "source": [
    "## Extracting ICU Unit Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617c3f2-75b9-4fa0-93a8-7161d0c2dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 3: Creating ICU unit type features...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"FIRST_UNIT_ENCODED\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"CHANGED_ICU_UNIT\", \n",
    "                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Unit Type Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74662ab6-f263-402e-913f-dc04804eadff",
   "metadata": {},
   "source": [
    "## Extracting Time-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef431386-85f1-4867-9aa1-f5bb84d7c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 4: Creating time-based features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n",
    "print(\"‚úÖ Base ICU Dataset - Time Based Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ac96b-6e5f-4fdd-84a2-043a0beaa06b",
   "metadata": {},
   "source": [
    "## Remove Outliers (Excessive Length Of Stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà ICU Length of Stay Statistics (Days):\")\n",
    "base_icu_df.select(\"ICU_LOS\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5f9c2",
   "metadata": {},
   "source": [
    "We kept every icu stay that had duration between 0.0 and 9.1 days, considered normal legnths since:\n",
    "\n",
    "| Statistic                | Value (days)                                    |\n",
    "| ------------------------ | ----------------------------------------------- |\n",
    "| **Minimum**              | 0.0 (can be admission + discharge on same day)  |\n",
    "| **25th percentile (Q1)** | \\~1.1                                           |\n",
    "| **Median (Q2)**          | \\~2.1                                           |\n",
    "| **75th percentile (Q3)** | \\~4.3                                           |\n",
    "| **Maximum**              | \\~88 (but can go slightly higher in edge cases) |\n",
    "| **Mean**                 | \\~3.3‚Äì3.5                                       |\n",
    "\n",
    "Using interquartile range (IQR) method:\n",
    "\n",
    "* IQR = Q3 - Q1 = 4.3 - 1.1 = ~3.2\n",
    "\n",
    "* Upper Bound for outliers = Q3 + 1.5 √ó IQR ‚âà 4.3 + 4.8 = ~9.1 days\n",
    "\n",
    "* Lower Bound = Q1 - 1.5 √ó IQR ‚âà 1.1 - 4.8 = < 0, which is ignored since LOS can‚Äôt be negative\n",
    "\n",
    "So:\n",
    "\n",
    "* Typical ICU LOS: 1.1 to 4.3 days\n",
    "\n",
    "* Outliers: ICU stays longer than ~9.1 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce0e40-182e-4fbd-bb35-2bb23ed0d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 5: Cleaning target variable...\")\n",
    "\n",
    "# List of desired columns\n",
    "selected_columns = [\n",
    "    \"ICUSTAY_ID\", \"SUBJECT_ID\", \"HADM_ID\", \"ICU_LOS\", \"ICU_INTIME\", \"ICU_OUTTIME\", \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\", \"CAME_FROM_ER\",\n",
    "    \"HAS_INSURANCE\", \"ADMISSION_TYPE_ENCODED\", \"ETHNICITY_ENCODED\",\n",
    "    \"FIRST_UNIT_ENCODED\", \"CHANGED_ICU_UNIT\"\n",
    "]\n",
    "\n",
    "# Apply filter and select columns\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_LOS\").between(0.0, 9.1)) \\\n",
    "    .select(*selected_columns) \\\n",
    "    .cache()\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Remove Outliers\")\n",
    "\n",
    "print(\"\\nüìã Sample of ICU stay records:\")\n",
    "base_icu_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268129f5-72e2-40a5-b676-cbd825ae84c8",
   "metadata": {},
   "source": [
    "## Extracting Clinical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eeb2a4",
   "metadata": {},
   "source": [
    "Get top 20 more common chart events, usually vital signs, calculate the avg of each test in the first 24h in the ICU. If a person did not do that test then the resulting value should be read -1 and not null, this ensures compatibility with ML algorithms that don‚Äôt handle missing values well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc92782-4f16-4c82-bd8a-44ed78e92965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Identifying top 20 most frequent tests from CHARTEVENTS...\")\n",
    "\n",
    "# Get frequency count of each ITEMID in CHARTEVENTS\n",
    "itemid_counts = chartevents_df \\\n",
    "    .filter(col(\"ICUSTAY_ID\").isin(icu_ids_list)) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .groupBy(\"ITEMID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .collect()\n",
    "\n",
    "# Create mapping dictionary for top 20 items\n",
    "top_20_items = {row[\"ITEMID\"]: f\"VITAL_{row['ITEMID']}\" for row in itemid_counts}\n",
    "print(f\"üéØ Top 20 chart items selected: {top_20_items}\")\n",
    "\n",
    "print(\"üìä Filtering CHARTEVENTS for top 20 items...\")\n",
    "\n",
    "chartevents_top20 = chartevents_df \\\n",
    "    .filter(col(\"ITEMID\").isin(list(top_20_items.keys()))) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"ICUSTAY_ID\").isin(icu_ids_list)) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(col(\"CHARTTIME\").between(col(\"ICU_INTIME\"), col(\"ICU_OUTTIME\"))) \\\n",
    "    .select(\"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\")\n",
    "\n",
    "# Process first 24 hours\n",
    "vitals_24h_top20 = chartevents_top20.alias(\"ce\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(\n",
    "        col(\"ce.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\"), \n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"üìä Calculating aggregates for top 20 vitals...\")\n",
    "\n",
    "# Initialize with ICUSTAY_ID\n",
    "vitals_features_top20 = base_icu_df.select(\"ICUSTAY_ID\")\n",
    "\n",
    "# Process each vital sign\n",
    "for itemid, name in top_20_items.items():\n",
    "    #print(f\"Processing {name} (ITEMID={itemid})...\")\n",
    "    \n",
    "    vital_stats = vitals_24h_top20 \\\n",
    "        .filter(col(\"ITEMID\") == itemid) \\\n",
    "        .groupBy(\"ICUSTAY_ID\") \\\n",
    "        .agg(avg(\"VALUENUM\").alias(f\"{name}_AVG\"))\n",
    "    \n",
    "    # Left join (without filling NULLs yet)\n",
    "    vitals_features_top20 = vitals_features_top20.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Fill ALL NULL values with -1 AFTER all joins are done\n",
    "vitals_features_top20 = vitals_features_top20.na.fill(-1)\n",
    "\n",
    "# Cleanup\n",
    "chartevents_df.unpersist()\n",
    "vitals_24h_top20.unpersist()\n",
    "\n",
    "# Verify no NULLs remain\n",
    "print(f\"‚úÖ Created {len(top_20_items)} features from top 20 vital signs (NULLs replaced with -1)\")\n",
    "vitals_features_top20.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11b458",
   "metadata": {},
   "source": [
    "Get top 20 more common lab events, calculate the avg of each test in the first 24h in the ICU and the 6h prior to it. If a person did not do that test then the resulting value should be read -1 and not null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ Creating laboratory features from LABEVENTS...\")\n",
    "\n",
    "# Step 1: Identify top 20 most frequent lab items\n",
    "print(\"üìä Identifying top 20 most frequent lab items...\")\n",
    "top_20_lab_items = labevents_df \\\n",
    "    .filter(col(\"HADM_ID\").isin([row[\"HADM_ID\"] for row in base_icu_df.select(\"HADM_ID\").collect()])) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\") > 0) \\\n",
    "    .groupBy(\"ITEMID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .collect()\n",
    "\n",
    "# Create mapping dictionary (using ITEMID as name if no mapping exists)\n",
    "lab_items = {row[\"ITEMID\"]: f\"LAB_{row['ITEMID']}\" for row in top_20_lab_items}\n",
    "print(f\"üéØ Top 20 lab items selected: {list(lab_items.keys())}\")\n",
    "\n",
    "# Step 2: Filter lab events within first 24 hours of ICU stay\n",
    "print(\"üìä Filtering LABEVENTS for top 20 items...\")\n",
    "labs_24h = labevents_df.alias(\"le\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n",
    "    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n",
    "    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"le.VALUENUM\") > 0) \\\n",
    "    .filter(\n",
    "        col(\"le.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Step 3: Calculate lab statistics with NULL handling\n",
    "print(\"üìä Calculating laboratory statistics...\")\n",
    "labs_features = base_icu_df.select(\"ICUSTAY_ID\")\n",
    "\n",
    "for itemid, name in lab_items.items():\n",
    "    #print(f\"   Processing {name} (ITEMID={itemid})...\")\n",
    "    \n",
    "    item_stats = labs_24h \\\n",
    "        .filter(col(\"ITEMID\") == itemid) \\\n",
    "        .groupBy(\"ICUSTAY_ID\") \\\n",
    "        .agg(\n",
    "            coalesce(avg(\"VALUENUM\"), lit(-1)).alias(f\"{name}_AVG\")\n",
    "        )\n",
    "    \n",
    "    labs_features = labs_features.join(item_stats, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Final NULL fill as safeguard (though coalesce should have handled it)\n",
    "labs_features = labs_features.na.fill(-1)\n",
    "\n",
    "# Cleanup\n",
    "labevents_df.unpersist()\n",
    "labs_24h.unpersist()\n",
    "\n",
    "print(f\"‚úÖ Created {len(lab_items)} lab features for {labs_features.count():,} ICU stays\")\n",
    "\n",
    "# Show sample of features\n",
    "print(\"üìä Sample features:\")\n",
    "labs_features.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüè• Creating diagnosis features from ICD codes...\")\n",
    "\n",
    "# Step 1: Identify top 10 most frequent ICD9 codes\n",
    "print(\"üìä Identifying top 10 most frequent diagnoses...\")\n",
    "top_10_diagnoses = diagnoses_df \\\n",
    "    .groupBy(\"ICD9_CODE\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .collect()\n",
    "\n",
    "top_10_codes = [row[\"ICD9_CODE\"] for row in top_10_diagnoses]\n",
    "print(f\"üéØ Top 10 ICD9 codes: {top_10_codes}\")\n",
    "\n",
    "# Step 2: Count total diagnoses per admission (comorbidity burden)\n",
    "diagnosis_features = diagnoses_df.groupBy(\"HADM_ID\") \\\n",
    "    .agg(\n",
    "        count(\"ICD9_CODE\").alias(\"TOTAL_DIAGNOSES\"),\n",
    "        collect_list(\"ICD9_CODE\").alias(\"DIAGNOSIS_CODES\")\n",
    "    )\n",
    "\n",
    "# Step 3: Create binary features for top 10 diagnoses\n",
    "for code in top_10_codes:\n",
    "    diagnosis_features = diagnosis_features.withColumn(\n",
    "        f\"HAS_{code}\",\n",
    "        when(array_contains(col(\"DIAGNOSIS_CODES\"), code), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# Drop the raw codes list\n",
    "diagnosis_features = diagnosis_features.drop(\"DIAGNOSIS_CODES\")\n",
    "\n",
    "print(f\"‚úÖ Created {len(top_10_codes)} diagnosis features for {diagnosis_features.count():,} admissions\")\n",
    "\n",
    "# Show sample of features\n",
    "print(\"üìä Sample features:\")\n",
    "diagnosis_features.show(5, truncate=False)\n",
    "\n",
    "print(f\"\\n‚è∞ Clinical features completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d9140-230d-4bc0-91ae-fd9db043e5a5",
   "metadata": {},
   "source": [
    "# Joining All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Joining all features...\")\n",
    "\n",
    "# Start with base ICU dataset and join all features\n",
    "final_dataset = base_icu_df \\\n",
    "    .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(diagnosis_features, \"HADM_ID\", \"left\")\n",
    "\n",
    "# Cleanup\n",
    "base_icu_df.unpersist()\n",
    "vitals_features_top20.unpersist()\n",
    "labs_features.unpersist()\n",
    "diagnosis_features.unpersist()\n",
    "\n",
    "print(f\"‚úÖ All features joined! Final dataset has {final_dataset.count()} records\")\n",
    "print(\"üìä Sample of final dataset:\")\n",
    "final_dataset.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6f328-17d8-40a6-ba45-769f22203346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã Step 3: Selecting final features for regression modeling...\")\n",
    "\n",
    "# Define feature columns for modeling\n",
    "feature_columns = [\n",
    "    # Demographics\n",
    "    \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\",\n",
    "    \n",
    "    # Admission characteristics\n",
    "    \"IS_EMERGENCY_ADMISSION\", \"IS_ELECTIVE_ADMISSION\", \"CAME_FROM_ER\",\n",
    "    \"HAS_MEDICARE\", \"IS_WHITE_ETHNICITY\", \"ADMISSION_TO_ICU_HOURS\",\n",
    "    \"WEEKEND_ADMISSION\", \"NIGHT_ADMISSION\",\n",
    "    \n",
    "    # ICU unit features\n",
    "    \"FIRST_UNIT_MICU\", \"FIRST_UNIT_SICU\", \"FIRST_UNIT_CSRU\", \n",
    "    \"FIRST_UNIT_CCU\", \"FIRST_UNIT_TSICU\", \"CHANGED_ICU_UNIT\",\n",
    "    \n",
    "    # Vital signs (averages)\n",
    "    \"HEART_RATE_AVG\", \"SBP_AVG\", \"DBP_AVG\", \"RESP_RATE_AVG\", \n",
    "    \"TEMPERATURE_AVG\", \"SPO2_AVG\",\n",
    "    \n",
    "    # Vital signs (variability)\n",
    "    \"HEART_RATE_STD\", \"SBP_STD\", \"DBP_STD\", \"RESP_RATE_STD\", \"SPO2_STD\",\n",
    "    \n",
    "    # Laboratory values\n",
    "    \"CREATININE_FIRST\", \"GLUCOSE_FIRST\", \"SODIUM_FIRST\", \"POTASSIUM_FIRST\",\n",
    "    \"HEMOGLOBIN_FIRST\", \"PLATELET_FIRST\", \"WBC_FIRST\", \"PH_FIRST\",\n",
    "    \n",
    "    # Diagnosis features\n",
    "    \"TOTAL_DIAGNOSES\", \"HAS_SEPSIS\", \"HAS_RESPIRATORY_FAILURE\", \"HAS_CARDIAC_ARREST\"\n",
    "]\n",
    "\n",
    "# Create modeling dataset with selected features\n",
    "modeling_dataset = final_dataset.select(\n",
    "    [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\"] + feature_columns\n",
    ")\n",
    "\n",
    "# Remove any remaining nulls and invalid records\n",
    "modeling_dataset = modeling_dataset.filter(col(\"ICU_LOS_DAYS\").isNotNull()) \\\n",
    "    .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n",
    "    .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n",
    "\n",
    "# Cache the final dataset\n",
    "#modeling_dataset = modeling_dataset.repartition()\n",
    "#modeling_dataset.cache()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Final modeling dataset prepared!\")\n",
    "print(f\"üìè Final dataset: {modeling_dataset.count():,} ICU stays\")\n",
    "print(f\"üìä Total features: {len(feature_columns)} predictive features\")\n",
    "print(f\"üéØ Target variable: ICU_LOS_DAYS (continuous)\")\n",
    "\n",
    "# Show feature summary\n",
    "print(f\"\\nüìã Feature categories:\")\n",
    "print(f\"   üë§ Demographics: 2 features\")\n",
    "print(f\"   üè• Admission: 8 features\") \n",
    "print(f\"   üè¢ ICU Unit: 6 features\")\n",
    "print(f\"   ü´Ä Vital Signs: 11 features\")\n",
    "print(f\"   üß™ Laboratory: 8 features\")\n",
    "print(f\"   ü©∫ Diagnoses: 4 features\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "#print(f\"\\nüìã Sample of final modeling dataset:\")\n",
    "#modeling_dataset.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"AGE_AT_ICU_ADMISSION\", \n",
    "#                        \"HEART_RATE_AVG\", \"CREATININE_FIRST\", \"HAS_SEPSIS\").show(5)\n",
    "\n",
    "# Basic statistics of target variable\n",
    "#print(f\"\\nüìà Final ICU Length of Stay Statistics:\")\n",
    "#modeling_dataset.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "print(f\"\\n‚è∞ Dataset preparation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üöÄ Ready for train/test split and model training!\")\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f975-668b-4ab3-a228-4af82187778e",
   "metadata": {},
   "source": [
    "## Preparing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa7d59-2d01-4dcf-868f-900431b44be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 1: Creating train/test split...\")\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache both datasets for performance\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Data split completed:\")\n",
    "#print(f\"   üìà Training set: {train_data.count():,} ICU stays ({train_data.count()/modeling_dataset.count()*100:.1f}%)\")\n",
    "#print(f\"   üìä Test set: {test_data.count():,} ICU stays ({test_data.count()/modeling_dataset.count()*100:.1f}%)\")\n",
    "\n",
    "# Show target variable distribution in both sets\n",
    "print(f\"\\nüìà Target variable distribution:\")\n",
    "print(f\"Training set LOS statistics:\")\n",
    "train_data.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "print(f\"Test set LOS statistics:\")\n",
    "test_data.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE VECTOR ASSEMBLY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîß Step 2: Assembling feature vectors...\")\n",
    "\n",
    "# Create feature vector assembler\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Apply feature assembler to training data\n",
    "train_assembled = feature_assembler.transform(train_data)\n",
    "test_assembled = feature_assembler.transform(test_data)\n",
    "\n",
    "\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Feature vectors assembled:\")\n",
    "print(f\"   üìä Feature vector size: {len(feature_columns)} dimensions\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Step 3: Scaling features...\")\n",
    "\n",
    "# Create StandardScaler to normalize features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler_model = scaler.fit(train_assembled)\n",
    "train_scaled = scaler_model.transform(train_assembled)\n",
    "test_scaled = scaler_model.transform(test_assembled)\n",
    "\n",
    "# Cache the final processed datasets\n",
    "train_scaled.cache()\n",
    "test_scaled.cache()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Feature scaling completed:\")\n",
    "print(f\"   üìä Features standardized (mean=0, std=1)\")\n",
    "print(f\"   üîß Scaler fitted on training data only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1aa2af-6e65-4688-84ac-724a79f8ba00",
   "metadata": {},
   "source": [
    "## Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2397a40-5d63-475b-a2dc-4b9f2fe4233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìã Step 4: Preparing final ML datasets...\")\n",
    "\n",
    "# Select columns needed for modeling\n",
    "ml_columns = [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features\"]\n",
    "\n",
    "train_final = train_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n",
    "test_final = test_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n",
    "\n",
    "#train_final = train_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n",
    "#test_final = test_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n",
    "\n",
    "\n",
    "# train_final = train_assembled.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features_raw\") \\\n",
    "#     .withColumnRenamed(\"ICU_LOS_DAYS\", \"label\") \\\n",
    "#     .withColumnRenamed(\"features_raw\", \"features\")\n",
    "\n",
    "# test_final = test_assembled.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features_raw\") \\\n",
    "#     .withColumnRenamed(\"ICU_LOS_DAYS\", \"label\") \\\n",
    "#     .withColumnRenamed(\"features_raw\", \"features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüìã Caching...\")\n",
    "\n",
    "# Cache final datasets\n",
    "train_final.cache()\n",
    "test_final.cache()\n",
    "\n",
    "print(f\"‚úÖ Final ML datasets prepared:\")\n",
    "print(f\"   üéØ Target variable: 'label' (ICU_LOS_DAYS)\")\n",
    "print(f\"   üìä Features: 'features' (scaled vector)\")\n",
    "print(f\"   üîë Identifier: 'ICUSTAY_ID'\")\n",
    "\n",
    "# Show sample of final datasets\n",
    "print(f\"\\nüìã Sample of training data structure:\")\n",
    "train_final.select(\"ICUSTAY_ID\", \"label\").show(3)\n",
    "\n",
    "print(f\"\\nüìã Feature vector example (first 10 features):\")\n",
    "# Show first few elements of feature vector for one sample\n",
    "sample_features = train_final.select(\"features\").take(1)[0][\"features\"]\n",
    "print(f\"   üìä Feature vector sample: {sample_features.toArray()[:10]}...\")\n",
    "print(f\"   üìè Total feature dimensions: {len(sample_features.toArray())}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîç Step 5: Final data quality checks...\")\n",
    "\n",
    "# Check for any remaining nulls\n",
    "train_nulls = train_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n",
    "test_nulls = test_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n",
    "\n",
    "print(f\"   üîç Null values in training set: {train_nulls}\")\n",
    "print(f\"   üîç Null values in test set: {test_nulls}\")\n",
    "\n",
    "# Show target variable ranges\n",
    "# train_stats = train_final.agg(\n",
    "#     min(\"label\").alias(\"min_los\"),\n",
    "#     max(\"label\").alias(\"max_los\"), \n",
    "#     avg(\"label\").alias(\"mean_los\"),\n",
    "#     stddev(\"label\").alias(\"std_los\")\n",
    "# ).collect()[0]\n",
    "\n",
    "# print(f\"\\nüìä Final training set target statistics:\")\n",
    "# print(f\"   üìâ Min LOS: {train_stats['min_los']:.2f} days\")\n",
    "# print(f\"   üìà Max LOS: {train_stats['max_los']:.2f} days\") \n",
    "# print(f\"   üìä Mean LOS: {train_stats['mean_los']:.2f} days\")\n",
    "# print(f\"   üìè Std LOS: {train_stats['std_los']:.2f} days\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preprocessing completed successfully!\")\n",
    "print(f\"üöÄ Ready for model training with {len(feature_columns)} features\")\n",
    "\n",
    "\n",
    "print(f\"‚è∞ Preprocessing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c98beb-ae1b-445a-aa7f-9002b4d9a012",
   "metadata": {},
   "source": [
    "## Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7d1c4-c1ef-4261-a3b5-e05db415626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 1: Setting up evaluation metrics...\")\n",
    "\n",
    "# Create regression evaluators\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured: RMSE, MAE, R¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f74323-98d0-44fd-b21f-f2e543449457",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25b20c-1f60-4782-b27c-6fda0121af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà Step 2: Training Linear Regression model...\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=200,                    # Increased for better convergence\n",
    "    regParam=0.001,                 # Lower regularization for healthcare data\n",
    "    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n",
    "    tol=1e-8,                       # Tighter tolerance for precision\n",
    "    standardization=False,          # We're doing manual scaling\n",
    "    fitIntercept=True,\n",
    "    aggregationDepth=3,             # Better for distributed training\n",
    "    loss=\"squaredError\",\n",
    "    solver=\"normal\"                 # Best for small-medium datasets\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"   üîÑ Training Linear Regression...\")\n",
    "lr_model = lr.fit(train_final)\n",
    "\n",
    "print(\"   üîÑ Linear Regression - Making predictions (test data)...\")\n",
    "lr_predictions = lr_model.transform(test_final)\n",
    "\n",
    "print(\"   üîÑ Linear Regression - Evaluation...\")\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"‚úÖ Linear Regression Results:\")\n",
    "print(f\"   üìâ RMSE: {lr_rmse:.3f} days\")\n",
    "print(f\"   üìä MAE: {lr_mae:.3f} days\")\n",
    "print(f\"   üìà R¬≤: {lr_r2:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Linear Regression Predictions\n",
    "\n",
    "print(\"\\nüìà Linear Regression Predictions (Sample 20):\")\n",
    "lr_display = lr_predictions.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ").orderBy(\"ICUSTAY_ID\")\n",
    "\n",
    "lr_display.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847f98b-4da8-41ff-b26f-f2943c8baa38",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52707ea4-eb6a-449d-8160-7013057b2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüå≤ Step 3: Training Random Forest model...\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=300,                   # More trees = better accuracy (if enough cores/memory)\n",
    "    maxDepth=12,                    # Deeper trees capture more complexity\n",
    "    minInstancesPerNode=2,          # Allows more granular splits\n",
    "    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n",
    "    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n",
    "    maxBins=64,                     # More bins = better numeric split precision\n",
    "    impurity=\"variance\",            # Required for regression\n",
    "    maxMemoryInMB=512,              # Give more memory per node for splits\n",
    "    cacheNodeIds=True,              # Improves tree building performance\n",
    "    checkpointInterval=5,           # Frequent checkpoints = safer on big jobs\n",
    "    seed=42                         # Reproducibility\n",
    ")\n",
    "\n",
    "print(\"   üîÑ Training Random Forest...\")\n",
    "rf_model = rf.fit(train_final)\n",
    "\n",
    "print(\"   üîÑ Random Forest - Making predictions (test data)...\")\n",
    "rf_predictions = rf_model.transform(test_final)\n",
    "\n",
    "print(\"   üîÑ Random Forest - Evaluation...\")\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = mae_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"‚úÖ Random Forest Results:\")\n",
    "print(f\"   üìâ RMSE: {rf_rmse:.3f} days\")\n",
    "print(f\"   üìä MAE: {rf_mae:.3f} days\")\n",
    "print(f\"   üìà R¬≤: {rf_r2:.3f}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Random Forest Predictions\n",
    "print(\"\\nüå≤ Random Forest Predictions (Sample 20):\")\n",
    "rf_display = rf_predictions.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ").orderBy(\"ICUSTAY_ID\")\n",
    "\n",
    "rf_display.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0d610-37c0-4c99-9fc0-289846b54033",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff000d-87eb-4b26-976a-65db81b057f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèÜ Step 5: Model Performance Comparison...\")\n",
    "\n",
    "# Create comparison summary\n",
    "results_data = [\n",
    "    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n",
    "    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n",
    "]\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n",
    "\n",
    "print(\"üìä Model Performance Summary:\")\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Find best model\n",
    "import operator\n",
    "import builtins\n",
    "best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n",
    "best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n",
    "\n",
    "print(f\"\\nü•á Best Models:\")\n",
    "print(f\"   üéØ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n",
    "print(f\"   üìà Highest R¬≤: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e166a-1c18-4e22-938c-d6a5569bd296",
   "metadata": {},
   "source": [
    "## Display Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fa69e-a7fb-40f2-a287-6300bf41e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linear Regression Predictions\n",
    "\n",
    "print(\"\\nüìà Linear Regression Predictions (Sample 20):\")\n",
    "lr_display = lr_predictions.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ").orderBy(\"ICUSTAY_ID\")\n",
    "\n",
    "lr_display.show(20, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Predictions\n",
    "print(\"\\nüå≤ Random Forest Predictions (Sample 20):\")\n",
    "rf_display = rf_predictions.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ").orderBy(\"ICUSTAY_ID\")\n",
    "\n",
    "rf_display.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e733a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
