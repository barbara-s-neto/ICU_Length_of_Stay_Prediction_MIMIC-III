{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1d1427",
   "metadata": {},
   "source": [
    "# ICU Length of Stay Prediction - MIMIC-III Pipeline\n",
    "\n",
    "## üéØ Objective\n",
    "Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n",
    "\n",
    "## üìä Data & Constraints\n",
    "- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n",
    "- **Filters**: \n",
    "        - Patient Age 18-80\n",
    "        - LOS 0.1-15 days\n",
    "        - Valid time sequences\n",
    "- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n",
    "\n",
    "\n",
    "## üåÄ Big Data Processing\n",
    "\n",
    "- **Storage**: We used Google Cloud Dataproc and Google Storage Buckets for MIMIC-III storage \n",
    "- **CHARTEVENTS**: Chart Events table has +330 million rows\n",
    "- **Parquet**: Converted \"CHARTEVENTS\" and \"LABEVENTS\" tables to Parquet format for efficient storage and processing\n",
    "- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n",
    "\n",
    "## üîß Features (39 total)\n",
    "- **Demographics (2)**: Age, gender\n",
    "- **Admission (8)**: Emergency/elective, timing, insurance\n",
    "- **ICU Units (6)**: Care unit types, transfers\n",
    "- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n",
    "- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n",
    "- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n",
    "\n",
    "## ü§ñ Models & Results\n",
    "- **Linear Regression**: \n",
    "- **Random Forest**: \n",
    "\n",
    "## ‚òÅÔ∏è Infrastructure\n",
    "- **GCP Dataproc**: 1x Master and 2x Workers, n2-standard-4  (12 vCPUs, 48GB RAM, 400GB Disk Storage)\n",
    "- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a166c4",
   "metadata": {},
   "source": [
    "## Cenas a acresentar no relatorio:\n",
    "\n",
    "* justificar o pq de cada uma das colunas\n",
    "* dar tune aos hiperparametros do modelo\n",
    "* referencias e bibliografias :\n",
    "    *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8375f-7f35-415f-8288-2ff2193e6af0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "89ed6638-09bf-4620-89fe-2bb2c00d86e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All imports loaded successfully!\n",
      "‚è∞ Notebook started at: 2025-06-05 13:58:26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üì¶ PySpark Core Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# üî¢ Data Processing & Feature Engineering\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StandardScaler,\n",
    "    StringIndexer,\n",
    "    MinMaxScaler,\n",
    "    Imputer\n",
    ")\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "# ü§ñ Machine Learning Models\n",
    "from pyspark.ml.regression import (\n",
    "    RandomForestRegressor,\n",
    "    LinearRegression\n",
    "    # GBTRegressor\n",
    ")\n",
    "\n",
    "\n",
    "# üìä Model Evaluation & Tuning\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import operator\n",
    "import builtins\n",
    "\n",
    "\n",
    "# ‚è±Ô∏è Date/Time Utilities\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ All imports loaded successfully!\")\n",
    "print(f\"‚è∞ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1810-67b2-471e-97d2-b8fda7db9728",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a15f1fca-5bf8-4e10-bdca-a6faa11ca17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created successfully!\n",
      "üìä Spark Version: 4.0.0\n",
      "üîß Application Name: Forecast-LOS\n",
      "üíæ Available cores: 8\n",
      "\n",
      "‚è∞ Spark session initialised at: 2025-06-05 13:58:26\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Forecast-LOS\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"32MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"300s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"300s\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memoryOffHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOffHeap.size\", \"1g\") \\\n",
    "    \\\n",
    "    .getOrCreate()\n",
    "print(\"‚úÖ Spark session created successfully!\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"üîß Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üíæ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\n‚è∞ Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13163e-55dc-4046-95b0-4815723d0581",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4de5cb",
   "metadata": {},
   "source": [
    "Strategy: Pre-filter CHARTEVENTS to find ICU stays with required vital signs, then efficiently load all tables using broadcast joins and lookup tables.\n",
    "Key Steps:\n",
    "\n",
    "- Filter for ICU stays with ‚â•1 of 6 vital signs (HR, BP, RR, Temp, SpO2)\n",
    "- Create lookup tables for ICUSTAY_ID, HADM_ID, SUBJECT_ID\n",
    "- Load all tables with pre-filtering using broadcast joins\n",
    "- Convert large files to \"Parquet\" for performance\n",
    "\n",
    "Result: Memory-efficient loading of only relevant data with quality assurance that all ICU stays have vital signs measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "82afa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration flags\n",
    "#SAMPLE_ENABLE = False\n",
    "#SAMPLE_SIZE = 20000\n",
    "#MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n",
    "#\n",
    "#\n",
    "#\n",
    "#print(\"üè• Loading MIMIC-III data...\")\n",
    "#\n",
    "## Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "#print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "#\n",
    "#try:\n",
    "#    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "#    print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "#except:\n",
    "#    print(\"üìÑ Converting CHARTEVENTS.csv.gz to parquet...\")\n",
    "#    chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n",
    "#    chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "#    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "#    print(\"‚úÖ Converted and loaded CHARTEVENTS\")\n",
    "#\n",
    "#\n",
    "#\n",
    "## Step 2: Load ICUSTAYS \n",
    "#print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "#icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n",
    "#\n",
    "#\n",
    "#\n",
    "## Step 3: Apply sampling if enabled\n",
    "#if SAMPLE_ENABLE:\n",
    "#    print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "#    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "#    icustays_df.cache()\n",
    "#    actual_sample_size = icustays_df.count()\n",
    "#    print(f\"‚úÖ Final sample: {actual_sample_size} ICU stays\")\n",
    "#else:\n",
    "#    icustays_df.cache()\n",
    "#    actual_sample_size = icustays_df.count()\n",
    "#\n",
    "#    \n",
    "#    \n",
    "## Step 4: Create efficient lookup tables\n",
    "#print(\"üìã Creating ID lookup tables...\")\n",
    "#icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "#hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "#subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "#\n",
    "#icu_lookup.count()  # Trigger caching\n",
    "#hadm_lookup.count()\n",
    "#subject_lookup.count()\n",
    "#\n",
    "## Step 5: Load other tables with optimized joins\n",
    "#print(\"üìÇ Loading PATIENTS table...\")\n",
    "#patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n",
    "#patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "#\n",
    "#print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "#admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n",
    "#admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "#print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "#diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n",
    "#diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "## Step 6: Load and filter CHARTEVENTS efficiently\n",
    "#print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "#chartevents_df = chartevents_df \\\n",
    "#    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "#    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "#\n",
    "## Step 7: Load LABEVENTS\n",
    "#print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "#try:\n",
    "#    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "#except:\n",
    "#    print(\"üìÑ Converting LABEVENTS.csv.gz to parquet...\")\n",
    "#    labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n",
    "#    labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "#    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "#\n",
    "#labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "## Final summary\n",
    "#print(\"\\n‚úÖ Data loading complete!\")\n",
    "#print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "#print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "#print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "#print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "#print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "#print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "#print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0db241",
   "metadata": {},
   "source": [
    "TIRAR ISTO ANTES DE ENTREGAR: ISTO E PARA CORRER LOCALMENTE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "62936c54-45ab-4c03-a8ec-fc3d87f68721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Loading MIMIC-III data...\n",
      "üìÇ Loading CHARTEVENTS...\n",
      "‚úÖ Loaded CHARTEVENTS from parquet\n",
      "\n",
      "üìÇ Loading and filtering ICUSTAYS...\n",
      "üìã Creating ID lookup tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:58:27 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/05 13:58:27 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/05 13:58:27 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/05 13:58:27 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading PATIENTS table...\n",
      "üìÇ Loading ADMISSIONS table...\n",
      "üìÇ Loading DIAGNOSES_ICD table...\n",
      "üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\n",
      "üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\n",
      "\n",
      "‚úÖ Data loading complete!\n",
      "üìä ICUSTAYS: 20 rows\n",
      "üìä PATIENTS: 20 rows\n",
      "üìä ADMISSIONS: 20 rows\n",
      "üìä DIAGNOSES_ICD: 212 rows\n",
      "üìä CHARTEVENTS (filtered): 57,973 rows\n",
      "üìä LABEVENTS (filtered): 5,895 rows\n",
      "\n",
      "‚è∞ Data loaded at: 2025-06-05 13:58:28\n"
     ]
    }
   ],
   "source": [
    "#Configuration flags\n",
    "SAMPLE_ENABLE = False\n",
    "SAMPLE_SIZE = 20000\n",
    "MIMIC_PATH = \"mimic-db-short\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"üè• Loading MIMIC-III data...\")\n",
    "\n",
    "# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "\n",
    "\n",
    "chartevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load ICUSTAYS \n",
    "print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Apply sampling if enabled\n",
    "if SAMPLE_ENABLE:\n",
    "    print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "    print(f\"‚úÖ Final sample: {actual_sample_size} ICU stays\")\n",
    "else:\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "\n",
    "    \n",
    "    \n",
    "# Step 4: Create efficient lookup tables\n",
    "print(\"üìã Creating ID lookup tables...\")\n",
    "icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "\n",
    "icu_lookup.count()  # Trigger caching\n",
    "hadm_lookup.count()\n",
    "subject_lookup.count()\n",
    "\n",
    "# Step 5: Load other tables with optimized joins\n",
    "print(\"üìÇ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Step 6: Load and filter CHARTEVENTS efficiently\n",
    "print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "chartevents_df = chartevents_df \\\n",
    "    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "\n",
    "# Step 7: Load LABEVENTS\n",
    "print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "\n",
    "labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "\n",
    "\n",
    "labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n‚úÖ Data loading complete!\")\n",
    "print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e7157-1230-41d3-8f41-1117b62fdb55",
   "metadata": {},
   "source": [
    "# Features Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763e6d7-4ba5-439b-8e2b-ba16cf607a3e",
   "metadata": {},
   "source": [
    "## Extracting Data From ICUSTAYS\n",
    "\n",
    "**Purpose**: Create comprehensive ICU dataset by joining ICU stays with patient demographics and admission details.\n",
    "\n",
    "**Key Features**:\n",
    "- **Target Variable**: ICU_LOS_DAYS (length of stay)\n",
    "- **Demographics**: Age (18-80), gender, ethnicity\n",
    "- **Clinical**: Care units, admission type/location, insurance\n",
    "- **Outcomes**: Hospital/patient death flags\n",
    "- **Identifiers**: ICUSTAY_ID, SUBJECT_ID, HADM_ID\n",
    "\n",
    "**Age Filter**: Adults only (18-80 years) to exclude pediatric/very elderly edge cases.\n",
    "\n",
    "**Alive Filter**: Only include people who did survive the ICU stay.\n",
    "\n",
    "**LOS Filter**: Get only LOS values within a range that does'nt include outliers.\n",
    "\n",
    "**Result**: Clean base dataset ready for vital signs feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9f1d7adb-eac0-4071-b252-a04268f0c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 1: Creating base ICU dataset with patient demographics...\n",
      "‚úÖ Created base ICU dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 1: Creating base ICU dataset with patient demographics...\")\n",
    "\n",
    "base_icu_df = icustays_df.alias(\"icu\") \\\n",
    "    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n",
    "    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        # ICU stay identifiers\n",
    "        col(\"icu.ICUSTAY_ID\"),\n",
    "        col(\"icu.SUBJECT_ID\"), \n",
    "        col(\"icu.HADM_ID\"),\n",
    "        \n",
    "        # Target variable - Length of Stay in ICU (days)\n",
    "        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n",
    "        \n",
    "        # ICU characteristics\n",
    "        col(\"icu.FIRST_CAREUNIT\"),\n",
    "        col(\"icu.LAST_CAREUNIT\"), \n",
    "        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n",
    "        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n",
    "        \n",
    "        # Patient demographics\n",
    "        col(\"pat.GENDER\"),\n",
    "        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n",
    "        col(\"pat.DOB\"),\n",
    "        \n",
    "        # Admission details\n",
    "        col(\"adm.ADMITTIME\"),\n",
    "        col(\"adm.DISCHTIME\"), \n",
    "        col(\"adm.ADMISSION_TYPE\"),\n",
    "        col(\"adm.ADMISSION_LOCATION\"),\n",
    "        col(\"adm.INSURANCE\"),\n",
    "        col(\"adm.ETHNICITY\"),\n",
    "        col(\"adm.MARITAL_STATUS\"),\n",
    "        col(\"adm.RELIGION\"),\n",
    "        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n",
    "        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n",
    "    )\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n",
    "                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n",
    "                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80)) \\\n",
    "                                    .filter(col(\"PATIENT_DIED\").isin(0))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Created base ICU dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "51b6ccac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà ICU Length of Stay Statistics (Days):\n",
      "+-------+-----------------+\n",
      "|summary|     ICU_LOS_DAYS|\n",
      "+-------+-----------------+\n",
      "|  count|               12|\n",
      "|   mean|2.360116666666667|\n",
      "| stddev|2.264151572918985|\n",
      "|    min|            0.848|\n",
      "|    max|           8.9163|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà ICU Length of Stay Statistics (Days):\")\n",
    "base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b3ede",
   "metadata": {},
   "source": [
    "We kept every ICU STAY that had duration (LOS) between 0.0 and 9.1 days, considered normal legnths since:\n",
    "\n",
    "| Statistic                | Value (days)                                    |\n",
    "| ------------------------ | ----------------------------------------------- |\n",
    "| **Minimum**              | 0.0 (can be admission + discharge on same day)  |\n",
    "| **25th percentile (Q1)** | \\~1.1                                           |\n",
    "| **Median (Q2)**          | \\~2.1                                           |\n",
    "| **75th percentile (Q3)** | \\~4.3                                           |\n",
    "| **Maximum**              | \\~88 (but can go slightly higher in edge cases) |\n",
    "| **Mean**                 | \\~3.3‚Äì3.5                                       |\n",
    "\n",
    "Using interquartile range (IQR) method:\n",
    "\n",
    "* IQR = Q3 - Q1 = 4.3 - 1.1 = ~3.2\n",
    "\n",
    "* Upper Bound for outliers = Q3 + 1.5 √ó IQR ‚âà 4.3 + 4.8 = ~9.1 days\n",
    "\n",
    "* Lower Bound = Q1 - 1.5 √ó IQR ‚âà 1.1 - 4.8 = < 0, which is ignored since LOS can‚Äôt be negative\n",
    "\n",
    "So:\n",
    "\n",
    "* Typical ICU LOS: 1.1 to 4.3 days\n",
    "\n",
    "* Outliers: ICU stays longer than ~9.1 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fc345d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing LOS outliers: 12\n",
      "üìä Cleaning target variable...\n",
      "‚úÖ Base ICU Dataset - Outliers Removed\n",
      "Number of rows after removing LOS outliers: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:58:28 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Print initial dataset size\n",
    "print(f\"Number of rows before removing LOS outliers: {base_icu_df.count()}\")\n",
    "\n",
    "print(\"üìä Cleaning target variable...\")\n",
    "\n",
    "# Filter to keep only records with ICU_LOS_DAYS between 0 and 9.1 days\n",
    "base_icu_df = base_icu_df.filter(\n",
    "    (col(\"ICU_LOS_DAYS\") >= 0.0) & \n",
    "    (col(\"ICU_LOS_DAYS\") <= 9.1)\n",
    ").cache()\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Outliers Removed\")\n",
    "\n",
    "# Print filtered dataset size\n",
    "print(f\"Number of rows after removing LOS outliers: {base_icu_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595638e6-74f7-4f2f-a7e5-1fc692089206",
   "metadata": {},
   "source": [
    "## Extracting Categorical Features\n",
    "\n",
    "**Features Created**:\n",
    "- **GENDER_BINARY**: Male = 1, Female = 0\n",
    "- **CAME_FROM_ER**: Emergency admission = 1\n",
    "- **HAS_INSURANCE**: Medicare = 1, other = 0\n",
    "- **ADMISSION_TYPE_ENCODED**: Emergency=1, Elective=2, Urgent=3, Other=0\n",
    "- **ETHNICITY_ENCODED**: White=1, Black=2, Hispanic=3, Asian=4, Other=5\n",
    "- **MARITAL_STATUS_ENCODED**: Married=1, Single=2, Divorced=3, Widowed=4, Separated=5, LifePartener=6, Other=0\n",
    "- **RELIGION_ENCODED**: Catholic=1, Protestant=2, Jewish=3, Other=0\n",
    "\n",
    "\n",
    "\n",
    "**Result**: Categorical variables converted to numerical format for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d28d34e6-83c3-40ae-95d0-71f9929397a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 2: Engineering categorical features...\n",
      "‚úÖ Base ICU Dataset - Categorical Features\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 2: Engineering categorical features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"ETHNICITY_ENCODED\",\n",
    "                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1)\n",
    "                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2)\n",
    "                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3)\n",
    "                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4)\n",
    "                .otherwise(5)) \\\n",
    "    .withColumn(\"MARITAL_STATUS_ENCODED\",\n",
    "                when(col(\"MARITAL_STATUS\") == \"MARRIED\", 1)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"SINGLE\", 2)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"DIVORCED\", 3)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"WIDOWED\", 4)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"SEPARATED\", 5)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"LIFE PARTNER\", 6)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"RELIGION_ENCODED\",\n",
    "                when(col(\"RELIGION\").contains(\"CATHOLIC\"), 1)\n",
    "                .when(col(\"RELIGION\").contains(\"PROTESTANT\"), 2)\n",
    "                .when(col(\"RELIGION\").contains(\"JEWISH\"), 3)\n",
    "                .otherwise(0))\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Categorical Features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca22bc8-8dd5-4d30-9cb9-0f366da21d76",
   "metadata": {},
   "source": [
    "## Extracting ICU Unit Types\n",
    "\n",
    "**Purpose**: Create categorical features for ICU unit types and transfers.\n",
    "\n",
    "**Features Created**:\n",
    "- **FIRST_UNIT_ENCODED**: Numerical encoding of ICU units\n",
    " - MICU (Medical) = 1\n",
    " - SICU (Surgical) = 2  \n",
    " - CSRU (Cardiac Surgery) = 3\n",
    " - CCU (Coronary Care) = 4\n",
    " - TSICU (Trauma Surgical) = 5\n",
    " - Other = 0\n",
    "- **CHANGED_ICU_UNIT**: Binary flag (1 if patient transferred between units)\n",
    "\n",
    "**Clinical Significance**: Different ICU types have varying complexity and typical LOS patterns. Unit transfers often indicate complications.\n",
    "\n",
    "**Result**: Enhanced dataset with ICU unit complexity and transfer indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6617c3f2-75b9-4fa0-93a8-7161d0c2dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 3: Creating ICU unit type features...\n",
      "‚úÖ Base ICU Dataset - Unit Type Features\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 3: Creating ICU unit type features...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"FIRST_UNIT_ENCODED\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"CHANGED_ICU_UNIT\", \n",
    "                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Unit Type Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74662ab6-f263-402e-913f-dc04804eadff",
   "metadata": {},
   "source": [
    "## Extracting Time-based Features\n",
    "\n",
    "**Action**: Filter out invalid records where INTIME >= OUTTIME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ef431386-85f1-4867-9aa1-f5bb84d7c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 4: Creating time-based features...\n",
      "‚úÖ Base ICU Dataset - Time Based Features\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 4: Creating time-based features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n",
    "print(\"‚úÖ Base ICU Dataset - Time Based Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a29626",
   "metadata": {},
   "source": [
    "Select useful columns brom the base df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "224c495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------------+--------------+-------------+-------------------+-------------------+------+------------+-------------------+-------------------+-------------------+--------------+--------------------+---------+--------------------+--------------+-----------------+--------------+--------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "|ICUSTAY_ID|SUBJECT_ID|HADM_ID|ICU_LOS_DAYS|FIRST_CAREUNIT|LAST_CAREUNIT|         ICU_INTIME|        ICU_OUTTIME|GENDER|PATIENT_DIED|                DOB|          ADMITTIME|          DISCHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|INSURANCE|           ETHNICITY|MARITAL_STATUS|         RELIGION|HOSPITAL_DEATH| ADMISSION_DIAGNOSIS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|\n",
      "+----------+----------+-------+------------+--------------+-------------+-------------------+-------------------+------+------------+-------------------+-------------------+-------------------+--------------+--------------------+---------+--------------------+--------------+-----------------+--------------+--------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "|    231977|      8470| 184688|      0.9792|          MICU|         MICU|2174-09-01 18:14:58|2174-09-02 17:45:00|     F|           0|2144-02-03 00:00:00|2174-08-29 20:44:00|2174-09-04 15:45:00|     EMERGENCY|PHYS REFERRAL/NOR...|  Private|               ASIAN|        SINGLE|         BUDDHIST|             0|       FIBROID BLEED|                  30|            0|           0|            0|                     1|                4|                     2|               0|                 1|               0|\n",
      "|    264061|     22862| 108676|      1.0576|          CSRU|         CSRU|2178-08-07 20:44:01|2178-08-08 22:06:54|     M|           0|2127-06-24 00:00:00|2178-08-07 20:43:00|2178-08-11 18:45:00|     EMERGENCY|CLINIC REFERRAL/P...| Medicaid|               WHITE|        SINGLE|         CATHOLIC|             0|PNEUMONIA;CONGEST...|                  51|            1|           0|            0|                     1|                1|                     2|               1|                 3|               0|\n",
      "|    248205|     18322| 163177|        4.05|          MICU|         MICU|2103-06-30 15:27:26|2103-07-04 16:39:27|     F|           0|2055-11-14 00:00:00|2103-06-30 15:25:00|2103-07-04 16:35:00|     EMERGENCY|EMERGENCY ROOM ADMIT| Medicare|               WHITE|      DIVORCED|         CATHOLIC|             0|            GI BLEED|                  47|            0|           1|            1|                     1|                1|                     3|               1|                 1|               0|\n",
      "|    298190|     45871| 178380|      1.2597|          CSRU|         CSRU|2192-10-25 11:14:14|2192-10-26 17:28:12|     F|           0|2119-01-11 00:00:00|2192-10-22 18:34:00|2192-10-30 11:44:00|     EMERGENCY|TRANSFER FROM HOS...| Medicare|               WHITE|      DIVORCED|         CATHOLIC|             0|CORONARY ARTERY D...|                  73|            0|           0|            1|                     1|                1|                     3|               1|                 3|               0|\n",
      "|    271202|     41841| 125602|      1.7742|          MICU|         MICU|2175-03-02 21:22:54|2175-03-04 15:57:42|     F|           0|2115-07-09 00:00:00|2175-03-02 07:15:00|2175-03-06 11:05:00|      ELECTIVE|PHYS REFERRAL/NOR...|  Private|               WHITE|        SINGLE|PROTESTANT QUAKER|             0| CONN'S SYNDROME/SDA|                  59|            0|           0|            0|                     2|                1|                     2|               2|                 1|               0|\n",
      "|    234929|     54636| 181763|      1.2198|          CSRU|         CSRU|2158-04-17 10:23:02|2158-04-18 15:39:37|     M|           0|2105-09-02 00:00:00|2158-04-17 07:15:00|2158-04-21 14:37:00|      ELECTIVE|PHYS REFERRAL/NOR...|  Private|               WHITE|       MARRIED|         CATHOLIC|             0|CORONARY ARTERY D...|                  52|            1|           0|            0|                     2|                1|                     1|               1|                 3|               0|\n",
      "|    207525|     52076| 109131|      1.2488|          CSRU|         CSRU|2112-06-27 09:22:35|2112-06-28 15:20:52|     M|           0|2043-04-18 00:00:00|2112-06-26 13:37:00|2112-07-01 16:25:00|      ELECTIVE|PHYS REFERRAL/NOR...| Medicare|               WHITE|       MARRIED|           JEWISH|             0|AORTIC STENOSIS\\A...|                  69|            1|           0|            1|                     2|                1|                     1|               3|                 3|               0|\n",
      "|    290009|     93336| 109820|      2.8701|          MICU|         MICU|2120-04-16 19:27:43|2120-04-19 16:20:39|     M|           0|2053-01-19 00:00:00|2120-04-16 19:26:00|2120-04-19 16:15:00|     EMERGENCY|CLINIC REFERRAL/P...| Medicare|BLACK/AFRICAN AME...|      DIVORCED|PROTESTANT QUAKER|             0|           PNEUMONIA|                  67|            1|           0|            1|                     1|                2|                     3|               2|                 1|               0|\n",
      "|    252713|     68825| 135117|       0.848|          MICU|         MICU|2124-08-07 19:19:53|2124-08-08 15:41:01|     F|           0|2061-10-13 00:00:00|2124-08-07 19:18:00|2124-08-08 15:45:00|     EMERGENCY|EMERGENCY ROOM ADMIT|  Private|               WHITE|       MARRIED|    NOT SPECIFIED|             0|             CHOKING|                  62|            0|           1|            0|                     1|                1|                     1|               0|                 1|               0|\n",
      "|    253828|     83258| 152943|      8.9163|          SICU|         SICU|2140-07-19 20:56:24|2140-07-28 18:55:53|     F|           0|2071-05-28 00:00:00|2140-07-19 20:55:00|2140-08-05 18:00:00|     EMERGENCY|TRANSFER FROM HOS...| Medicaid|               ASIAN|       MARRIED|     UNOBTAINABLE|             0|LEFT CEBELLAR ISC...|                  69|            0|           0|            0|                     1|                4|                     1|               0|                 2|               0|\n",
      "|    259725|     85962| 117313|      1.8064|          MICU|         MICU|2198-09-09 21:53:56|2198-09-11 17:15:09|     M|           0|2118-09-08 00:00:00|2198-09-09 20:23:00|2198-09-12 17:03:00|     EMERGENCY|CLINIC REFERRAL/P...| Medicare|               WHITE|       MARRIED|    NOT SPECIFIED|             0|               FEVER|                  80|            1|           0|            1|                     1|                1|                     1|               0|                 1|               0|\n",
      "|    235298|     65589| 110972|      2.2913|          CSRU|         CSRU|2189-07-29 11:15:10|2189-07-31 18:14:35|     M|           0|2111-04-12 00:00:00|2189-07-22 14:29:00|2189-08-10 14:00:00|     EMERGENCY|TRANSFER FROM HOS...| Medicare|               WHITE|       WIDOWED|PROTESTANT QUAKER|             0|        ENDOCARDITIS|                  78|            1|           0|            1|                     1|                1|                     4|               2|                 3|               0|\n",
      "+----------+----------+-------+------------+--------------+-------------+-------------------+-------------------+------+------------+-------------------+-------------------+-------------------+--------------+--------------------+---------+--------------------+--------------+-----------------+--------------+--------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_icu_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "79066063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 5: Dropping useless columns...\n",
      "‚úÖ Base ICU Dataset - Finalized\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "|ICUSTAY_ID|SUBJECT_ID|HADM_ID|ICU_LOS_DAYS|         ICU_INTIME|        ICU_OUTTIME|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "|    231977|      8470| 184688|      0.9792|2174-09-01 18:14:58|2174-09-02 17:45:00|                  30|            0|           0|            0|                     1|                4|                     2|               0|                 1|               0|\n",
      "|    264061|     22862| 108676|      1.0576|2178-08-07 20:44:01|2178-08-08 22:06:54|                  51|            1|           0|            0|                     1|                1|                     2|               1|                 3|               0|\n",
      "|    248205|     18322| 163177|        4.05|2103-06-30 15:27:26|2103-07-04 16:39:27|                  47|            0|           1|            1|                     1|                1|                     3|               1|                 1|               0|\n",
      "|    298190|     45871| 178380|      1.2597|2192-10-25 11:14:14|2192-10-26 17:28:12|                  73|            0|           0|            1|                     1|                1|                     3|               1|                 3|               0|\n",
      "|    271202|     41841| 125602|      1.7742|2175-03-02 21:22:54|2175-03-04 15:57:42|                  59|            0|           0|            0|                     2|                1|                     2|               2|                 1|               0|\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 5: Dropping useless columns...\")\n",
    "\n",
    "# List of columns to drop (fixed syntax)\n",
    "drop_cols = [\n",
    "    \"FIRST_CAREUNIT\",\n",
    "    \"LAST_CAREUNIT\",\n",
    "    \"GENDER\",\n",
    "    \"PATIENT_DIED\",\n",
    "    \"DOB\",\n",
    "    \"ADMITTIME\",\n",
    "    \"DISCHTIME\",\n",
    "    \"ADMISSION_TYPE\",\n",
    "    \"ADMISSION_LOCATION\",\n",
    "    \"INSURANCE\",\n",
    "    \"ETHNICITY\",\n",
    "    \"MARITAL_STATUS\",\n",
    "    \"RELIGION\",\n",
    "    \"HOSPITAL_DEATH\",\n",
    "    \"ADMISSION_DIAGNOSIS\"\n",
    "]\n",
    "\n",
    "# Keep all columns except those in drop_cols\n",
    "base_icu_df = base_icu_df.drop(*drop_cols)\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Finalized\")\n",
    "base_icu_df.show(5)  # Showing first 5 rows for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268129f5-72e2-40a5-b676-cbd825ae84c8",
   "metadata": {},
   "source": [
    "## Extracting Clinical Events\n",
    "\n",
    "**Purpose**: Extract top 20 most common CHARTEVENTS as features for ML models.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Find 20 most frequent CHARTEVENTS (typically vital signs)\n",
    "2. **Calculate**: Average value of each test in first 24 hours of ICU stay\n",
    "3. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n",
    "\n",
    "**Time Window**: First 24 hours after ICU admission (INTIME + 24h)\n",
    "\n",
    "**Result**: 20 vital signs features with consistent **-1** encoding for missing data, ensuring ML algorithm compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9dc92782-4f16-4c82-bd8a-44ed78e92965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Identifying top 20 most frequent tests from CHARTEVENTS...\n",
      "üéØ Top 20 chart items selected: {220045: 'VITAL_220045', 220277: 'VITAL_220277', 220210: 'VITAL_220210', 220181: 'VITAL_220181', 220179: 'VITAL_220179', 220180: 'VITAL_220180', 211: 'VITAL_211', 742: 'VITAL_742', 618: 'VITAL_618', 646: 'VITAL_646', 223901: 'VITAL_223901', 220739: 'VITAL_220739', 223900: 'VITAL_223900', 220052: 'VITAL_220052', 220050: 'VITAL_220050', 220051: 'VITAL_220051', 223753: 'VITAL_223753', 8441: 'VITAL_8441', 455: 'VITAL_455', 456: 'VITAL_456'}\n",
      "üìä Filtering CHARTEVENTS for top 20 items...\n",
      "üìä Calculating aggregates for top 20 vitals...\n",
      "‚úÖ Created 20 features from top 20 vital signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+------------------+-----------------+------------------+-----------------+-------------+-------------+-------------+-------------+-----------------+------------------+----------------+-----------------+------------------+-----------------+-----------------+--------------+-------------+-------------+\n",
      "|ICUSTAY_ID| VITAL_220045_AVG|VITAL_220277_AVG|  VITAL_220210_AVG| VITAL_220181_AVG|  VITAL_220179_AVG| VITAL_220180_AVG|VITAL_211_AVG|VITAL_742_AVG|VITAL_618_AVG|VITAL_646_AVG| VITAL_223901_AVG|  VITAL_220739_AVG|VITAL_223900_AVG| VITAL_220052_AVG|  VITAL_220050_AVG| VITAL_220051_AVG| VITAL_223753_AVG|VITAL_8441_AVG|VITAL_455_AVG|VITAL_456_AVG|\n",
      "+----------+-----------------+----------------+------------------+-----------------+------------------+-----------------+-------------+-------------+-------------+-------------+-----------------+------------------+----------------+-----------------+------------------+-----------------+-----------------+--------------+-------------+-------------+\n",
      "|    234929|90.36538461538461|98.4423076923077|18.333333333333332|             72.5|            108.25|             63.5|         NULL|         NULL|         NULL|         NULL|4.333333333333333|2.6666666666666665|             3.0|76.84313725490196|106.17647058823529|61.94117647058823|             NULL|          NULL|         NULL|         NULL|\n",
      "|    298190|             84.5|99.1923076923077|23.615384615384617|84.33333333333333|119.66666666666667|76.66666666666667|         NULL|         NULL|         NULL|         NULL|              5.0|               3.0|             1.8| 74.5925925925926|109.11111111111111|55.73076923076923|3.909090909090909|          NULL|         NULL|         NULL|\n",
      "|    271202|             70.6|           96.76|12.038461538461538|             78.7|             134.6|             61.8|         NULL|         NULL|         NULL|         NULL|              6.0|               4.0|             5.0|             NULL|              NULL|             NULL|              4.0|          NULL|         NULL|         NULL|\n",
      "|    264061|           81.125|            91.0|18.434782608695652|64.26315789473684| 99.81818181818181|56.42857142857143|         NULL|         NULL|         NULL|         NULL|              6.0|               4.0|             5.0|             NULL|              NULL|             NULL|             NULL|          NULL|         NULL|         NULL|\n",
      "|    231977|             NULL|            NULL|              NULL|             NULL|              NULL|             NULL|        89.36|          1.0|        16.64|        100.0|             NULL|              NULL|            NULL|             NULL|              NULL|             NULL|             NULL|          NULL|         NULL|         NULL|\n",
      "+----------+-----------------+----------------+------------------+-----------------+------------------+-----------------+-------------+-------------+-------------+-------------+-----------------+------------------+----------------+-----------------+------------------+-----------------+-----------------+--------------+-------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Identifying top 20 most frequent tests from CHARTEVENTS...\")\n",
    "\n",
    "\n",
    "# Get frequency count of each ITEMID in CHARTEVENTS\n",
    "itemid_counts = chartevents_df \\\n",
    "    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .groupBy(\"ITEMID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .collect()\n",
    "\n",
    "# Create mapping dictionary for top 20 items\n",
    "top_20_items = {row[\"ITEMID\"]: f\"VITAL_{row['ITEMID']}\" for row in itemid_counts}\n",
    "print(f\"üéØ Top 20 chart items selected: {top_20_items}\")\n",
    "\n",
    "print(\"üìä Filtering CHARTEVENTS for top 20 items...\")\n",
    "\n",
    "chartevents_top20 = chartevents_df \\\n",
    "    .filter(col(\"ITEMID\").isin(list(top_20_items.keys()))) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(col(\"CHARTTIME\").between(col(\"ICU_INTIME\"), col(\"ICU_OUTTIME\"))) \\\n",
    "    .select(\"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\")\n",
    "\n",
    "# Process first 24 hours\n",
    "vitals_24h_top20 = chartevents_top20.alias(\"ce\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(\n",
    "        col(\"ce.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\"), \n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"üìä Calculating aggregates for top 20 vitals...\")\n",
    "\n",
    "# Initialize with ICUSTAY_ID\n",
    "vitals_features_top20 = base_icu_df.select(\"ICUSTAY_ID\")\n",
    "\n",
    "# Process each vital sign\n",
    "for itemid, name in top_20_items.items():\n",
    "    #print(f\"Processing {name} (ITEMID={itemid})...\")\n",
    "    \n",
    "    vital_stats = vitals_24h_top20 \\\n",
    "        .filter(col(\"ITEMID\") == itemid) \\\n",
    "        .groupBy(\"ICUSTAY_ID\") \\\n",
    "        .agg(avg(\"VALUENUM\").alias(f\"{name}_AVG\"))\n",
    "    \n",
    "    # Left join (without filling NULLs yet)\n",
    "    vitals_features_top20 = vitals_features_top20.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Cleanup\n",
    "chartevents_df.unpersist()\n",
    "vitals_24h_top20.unpersist()\n",
    "\n",
    "# Verify no NULLs remain\n",
    "print(f\"‚úÖ Created {len(top_20_items)} features from top 20 vital signs\")\n",
    "vitals_features_top20.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08c810-405c-48ca-a141-1a48e125635c",
   "metadata": {},
   "source": [
    "## Extracting Laboratory Events\n",
    "\n",
    "**Purpose**: Extract top 20 most common lab tests as features for ML models.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Find 20 most frequent LABEVENTS (blood tests, chemistry panels)\n",
    "2. **Time Window**: 6 hours before ICU admission + first 24 hours in ICU (30h total)\n",
    "3. **Calculate**: Average value of each lab test within the 30-hour window\n",
    "\n",
    "**Time Range**: ICU_INTIME - 6h to ICU_INTIME + 24h\n",
    "\n",
    "**Result**: 20 lab test features with consistent -1 encoding for missing data, capturing pre-ICU and early ICU clinical status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a676e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Creating laboratory features from LABEVENTS...\n",
      "üìä Identifying top 20 most frequent lab items...\n",
      "üéØ Top 20 lab items selected: [51221, 50983, 51301, 51265, 50971, 51248, 51250, 51222, 51249, 51279, 51277, 50912, 51006, 50902, 50882, 50868, 50931, 50960, 51275, 51237]\n",
      "üìä Filtering LABEVENTS for top 20 items...\n",
      "üìä Calculating laboratory statistics...\n",
      "‚úÖ Created 20 lab features for 12 ICU stays\n",
      "üìä Sample features:\n",
      "+----------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ICUSTAY_ID|LAB_51221_AVG     |LAB_50983_AVG     |LAB_51301_AVG    |LAB_51265_AVG     |LAB_50971_AVG     |LAB_51248_AVG     |LAB_51250_AVG    |LAB_51222_AVG     |LAB_51249_AVG|LAB_51279_AVG     |LAB_51277_AVG     |LAB_50912_AVG     |LAB_51006_AVG|LAB_50902_AVG     |LAB_50882_AVG     |LAB_50868_AVG     |LAB_50931_AVG     |LAB_50960_AVG     |LAB_51275_AVG     |LAB_51237_AVG     |\n",
      "+----------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|234929    |31.157142857142855|138.0             |15.15            |203.8             |4.15              |29.1              |86.25            |10.625            |33.75        |3.665             |13.825000000000001|0.6499999999999999|12.0         |108.0             |25.0              |9.0               |108.0             |NULL              |35.975            |1.25              |\n",
      "|298190    |32.349999999999994|140.0             |25.73333333333333|219.66666666666666|5.0               |28.733333333333334|83.33333333333333|11.233333333333334|34.6         |3.9               |17.333333333333332|0.6499999999999999|17.0         |110.0             |20.5              |16.0              |143.0             |1.8               |29.9              |1.4               |\n",
      "|271202    |32.53333333333333 |137.85714285714286|8.6              |187.66666666666666|3.314285714285714 |30.03333333333333 |89.0             |11.033333333333333|33.8         |3.6633333333333336|13.299999999999999|0.5166666666666667|5.0          |105.66666666666667|25.666666666666668|10.166666666666666|131.16666666666666|2.085714285714286 |23.299999999999997|1.1               |\n",
      "|264061    |36.900000000000006|137.0             |11.5             |268.5             |4.35              |30.299999999999997|92.5             |12.15             |32.9         |3.995             |15.649999999999999|3.5               |65.5         |95.0              |25.5              |21.0              |132.0             |2.4               |NULL              |NULL              |\n",
      "|231977    |28.080000000000002|137.25            |11.28            |119.8             |3.5800000000000005|30.32             |86.8             |9.860000000000001 |35.1         |3.25              |17.259999999999998|0.325             |4.0          |106.5             |24.25             |10.0              |85.33333333333333 |1.4249999999999998|32.48             |1.3800000000000001|\n",
      "+----------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüß™ Creating laboratory features from LABEVENTS...\")\n",
    "\n",
    "# Step 1: Identify top 20 most frequent lab items\n",
    "print(\"üìä Identifying top 20 most frequent lab items...\")\n",
    "top_20_lab_items = labevents_df \\\n",
    "    .filter(col(\"HADM_ID\").isin([row[\"HADM_ID\"] for row in base_icu_df.select(\"HADM_ID\").collect()])) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\") > 0) \\\n",
    "    .groupBy(\"ITEMID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .collect()\n",
    "\n",
    "# Create mapping dictionary with clean LAB_[ITEMID] format\n",
    "lab_items = {row[\"ITEMID\"]: f\"LAB_{row['ITEMID']}\" for row in top_20_lab_items}\n",
    "print(f\"üéØ Top 20 lab items selected: {list(lab_items.keys())}\")\n",
    "\n",
    "# Step 2: Filter lab events within first 24 hours of ICU stay\n",
    "print(\"üìä Filtering LABEVENTS for top 20 items...\")\n",
    "labs_24h = labevents_df.alias(\"le\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n",
    "    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n",
    "    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"le.VALUENUM\") > 0) \\\n",
    "    .filter(\n",
    "        col(\"le.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Step 3: Calculate lab statistics with clean column names\n",
    "print(\"üìä Calculating laboratory statistics...\")\n",
    "labs_features = base_icu_df.select(\"ICUSTAY_ID\")\n",
    "\n",
    "for itemid, name in lab_items.items():\n",
    "    item_stats = labs_24h \\\n",
    "        .filter(col(\"ITEMID\") == itemid) \\\n",
    "        .groupBy(\"ICUSTAY_ID\") \\\n",
    "        .agg(\n",
    "            avg(\"VALUENUM\").alias(f\"{name}_AVG\")  # Simple alias without coalesce in the name\n",
    "        )\n",
    "    \n",
    "    labs_features = labs_features.join(item_stats, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Cleanup\n",
    "labevents_df.unpersist()\n",
    "labs_24h.unpersist()\n",
    "\n",
    "print(f\"‚úÖ Created {len(lab_items)} lab features for {labs_features.count():,} ICU stays\")\n",
    "\n",
    "# Show sample of features with clean column names\n",
    "print(\"üìä Sample features:\")\n",
    "labs_features.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    *[col for col in labs_features.columns if col != \"ICUSTAY_ID\"]\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cd034-b129-46b0-b33f-cf903ee438ae",
   "metadata": {},
   "source": [
    "## Diagnosis ICD\n",
    "\n",
    "**Purpose**: Extract diagnosis patterns as ML features from ICD-9 codes.\n",
    "\n",
    "**Process**:\n",
    "1. **Top 3**: Get top 3 diagnoses by person, using HADM_ID, to future join with other tables. \n",
    "2. **Encode**: Encode the ICD9 diagnoses into a wide range of diagnoses.\n",
    "3. **Pivot**: Pivot to create the 3 columns with the encoded diagnose type.\n",
    "4. **Handle Missing Values**: Input -1 in the NULL entries of the table.\n",
    "\n",
    "\n",
    "**Features Created**:\n",
    "- **TOTAL_DIAGNOSES**: Count of all diagnoses (comorbidity indicator)\n",
    "- **PRIMARY_DIAGNOSIS**: Most significant diagnose, encoded.\n",
    "- **SECONDARY_DIAGNOSIS**: Second most significant diagnose, encoded.\n",
    "- **TERCIARY_DIAGNOSIS**: Third most significant diagnose, encoded.\n",
    "\n",
    "**Result**: ??????????????????????????????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4b3682a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icd9_to_chapter(code):\n",
    "    # Convert to string and clean\n",
    "    code_str = str(code).strip()\n",
    "    \n",
    "    # Handle V codes (supplementary classification)\n",
    "    if code_str.startswith('V'):\n",
    "        return 18 #'Supplemental'\n",
    "    \n",
    "    # Handle E codes (external causes of injury)\n",
    "    if code_str.startswith('E'):\n",
    "        return 19 #'External_Injury'\n",
    "    \n",
    "    # Extract first 3 digits for numeric codes\n",
    "    try:\n",
    "        # Handle codes like '4280' (convert to 428) or '486' (stays 486)\n",
    "        numeric_part = code_str.split('.')[0] if '.' in code_str else code_str\n",
    "        code_num = float(numeric_part[:3])\n",
    "    except:\n",
    "        return 0 #'Unknown'\n",
    "    \n",
    "    # Map to chapters\n",
    "    if 1 <= code_num <= 139: return 1 #'Infectious'\n",
    "    elif 140 <= code_num <= 239: return 2 # 'Neoplasms'\n",
    "    elif 240 <= code_num <= 279: return 3 #'Endocrine'\n",
    "    elif 280 <= code_num <= 289: return 4 #'Blood'\n",
    "    elif 290 <= code_num <= 319: return 5 #'Mental'\n",
    "    elif 320 <= code_num <= 389: return 6 #'Nervous'\n",
    "    elif 390 <= code_num <= 459: return 7 #'Circulatory'\n",
    "    elif 460 <= code_num <= 519: return 8 #'Respiratory'\n",
    "    elif 520 <= code_num <= 579: return 9 #'Digestive'\n",
    "    elif 580 <= code_num <= 629: return 10 #'Genitourinary'\n",
    "    elif 630 <= code_num <= 679: return 11 #'Pregnancy'\n",
    "    elif 680 <= code_num <= 709: return 12 #'Skin'\n",
    "    elif 710 <= code_num <= 739: return 13 #'Musculoskeletal'\n",
    "    elif 740 <= code_num <= 759: return 14 #'Congenital'\n",
    "    elif 760 <= code_num <= 779: return 15 #'Perinatal'\n",
    "    elif 780 <= code_num <= 799: return 16 #'Ill-defined'\n",
    "    elif 800 <= code_num <= 999: return 17 #'Injury'\n",
    "    else: return 20 #'Other' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "94a414c1-7139-4e27-a5e4-767919a3eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè• Creating diagnosis features (optimized pipeline)...\n",
      "üìä Optimized diagnosis features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:58:35 WARN CacheManager: Asked to cache already cached data.\n",
      "[Stage 29153:=========>   (24 + 8) / 32][Stage 29155:>             (0 + 0) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-----------------+-------------------+------------------+\n",
      "|HADM_ID|TOTAL_DIAGNOSES|PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|\n",
      "+-------+---------------+-----------------+-------------------+------------------+\n",
      "|152943 |7              |7                |6                  |6                 |\n",
      "|163177 |7              |9                |8                  |5                 |\n",
      "|110159 |12             |17               |9                  |8                 |\n",
      "|109820 |11             |1                |8                  |8                 |\n",
      "|181763 |12             |7                |17                 |4                 |\n",
      "|150954 |6              |7                |8                  |18                |\n",
      "|177309 |16             |1                |12                 |10                |\n",
      "|110972 |13             |17               |7                  |17                |\n",
      "|197549 |15             |17               |7                  |17                |\n",
      "|109131 |12             |7                |2                  |7                 |\n",
      "|135117 |5              |17               |14                 |19                |\n",
      "|178506 |6              |17               |8                  |16                |\n",
      "|184688 |2              |2                |4                  |-1                |\n",
      "|108676 |14             |10               |8                  |13                |\n",
      "|178380 |18             |7                |7                  |17                |\n",
      "|117313 |12             |1                |8                  |17                |\n",
      "|123482 |16             |1                |16                 |10                |\n",
      "|187714 |6              |18               |15                 |15                |\n",
      "|156406 |9              |1                |2                  |3                 |\n",
      "|125602 |13             |2                |8                  |3                 |\n",
      "+-------+---------------+-----------------+-------------------+------------------+\n",
      "\n",
      "‚è∞ Completed in: 1181.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nüè• Creating diagnosis features (optimized pipeline)...\")\n",
    "\n",
    "# 1. First filter to only top 3 diagnoses per admission\n",
    "window_spec = Window.partitionBy(\"HADM_ID\").orderBy(\"SEQ_NUM\")\n",
    "\n",
    "top_3_filtered = diagnoses_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") <= 3) \\\n",
    "    .cache()\n",
    "\n",
    "# 2. Register UDF with Integer return type\n",
    "icd9_chapter_udf = udf(icd9_to_chapter, IntegerType())  # Changed to IntegerType\n",
    "\n",
    "# 3. Encode ONLY the top 3 diagnoses\n",
    "top_3_encoded = top_3_filtered.withColumn(\n",
    "    \"DISEASE_CHAPTER\", \n",
    "    icd9_chapter_udf(col(\"ICD9_CODE\"))\n",
    ")\n",
    "\n",
    "# 4. Pivot to create columns\n",
    "diagnosis_features = top_3_encoded \\\n",
    "    .groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"row_num\", [1, 2, 3]) \\\n",
    "    .agg(first(\"DISEASE_CHAPTER\")) \\\n",
    "    .select(\n",
    "        \"HADM_ID\",\n",
    "        col(\"1\").alias(\"PRIMARY_DIAGNOSIS\").cast(IntegerType()),\n",
    "        col(\"2\").alias(\"SECONDARY_DIAGNOSIS\").cast(IntegerType()),\n",
    "        col(\"3\").alias(\"TERTIARY_DIAGNOSIS\").cast(IntegerType())\n",
    "    ) \\\n",
    "    .join(diagnosis_counts, \"HADM_ID\", \"left\")\n",
    "\n",
    "# 5. Fill NULLs and ensure consistent types\n",
    "diagnosis_features = diagnosis_features.fillna(-1, subset=[\n",
    "    \"PRIMARY_DIAGNOSIS\",\n",
    "    \"SECONDARY_DIAGNOSIS\",\n",
    "    \"TERTIARY_DIAGNOSIS\"\n",
    "])\n",
    "\n",
    "\n",
    "print(\"üìä Optimized diagnosis features:\")\n",
    "diagnosis_features.select(\n",
    "    \"HADM_ID\",\n",
    "    \"TOTAL_DIAGNOSES\",\n",
    "    \"PRIMARY_DIAGNOSIS\",\n",
    "    \"SECONDARY_DIAGNOSIS\",\n",
    "    \"TERTIARY_DIAGNOSIS\"\n",
    ").show(20, truncate=False)\n",
    "\n",
    "print(f\"‚è∞ Completed in: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d9140-230d-4bc0-91ae-fd9db043e5a5",
   "metadata": {},
   "source": [
    "## Joining All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1fe76c11-fb64-4b12-9a37-8efccf76d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Joining all features and selecting final features for regression modeling...\n",
      "‚úÖ Final modeling dataset created with 12 records\n",
      "üìã Sample of final modeling dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:58:43 WARN DAGScheduler: Broadcasting large task binary with size 1671.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+-------------+------------------+-----------------+-----------------+------------------+----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|VITAL_220045_AVG |VITAL_220277_AVG|VITAL_220210_AVG  |VITAL_220181_AVG |VITAL_220179_AVG |VITAL_220180_AVG |VITAL_211_AVG     |VITAL_742_AVG|VITAL_618_AVG     |VITAL_646_AVG    |VITAL_223901_AVG |VITAL_220739_AVG  |VITAL_223900_AVG|VITAL_220052_AVG |VITAL_220050_AVG  |VITAL_220051_AVG |VITAL_223753_AVG|VITAL_8441_AVG   |VITAL_455_AVG     |VITAL_456_AVG    |LAB_51221_AVG     |LAB_50983_AVG     |LAB_51301_AVG|LAB_51265_AVG     |LAB_50971_AVG     |LAB_51248_AVG     |LAB_51250_AVG|LAB_51222_AVG     |LAB_51249_AVG|LAB_51279_AVG     |LAB_51277_AVG     |LAB_50912_AVG     |LAB_51006_AVG|LAB_50902_AVG     |LAB_50882_AVG     |LAB_50868_AVG     |LAB_50931_AVG     |LAB_50960_AVG     |LAB_51275_AVG     |LAB_51237_AVG     |PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+-------------+------------------+-----------------+-----------------+------------------+----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n",
      "|0.9792      |30                  |0            |0           |0            |1                     |4                |2                     |0               |1                 |0               |NULL             |NULL            |NULL              |NULL             |NULL             |NULL             |89.36             |1.0          |16.64             |100.0            |NULL             |NULL              |NULL            |NULL             |NULL              |NULL             |NULL            |NULL             |NULL              |NULL             |28.080000000000002|137.25            |11.28        |119.8             |3.5800000000000005|30.32             |86.8         |9.860000000000001 |35.1         |3.25              |17.259999999999998|0.325             |4.0          |106.5             |24.25             |10.0              |85.33333333333333 |1.4249999999999998|32.48             |1.3800000000000001|2                |4                  |-1                |2              |\n",
      "|1.2198      |52                  |1            |0           |0            |2                     |1                |1                     |1               |3                 |0               |90.36538461538461|98.4423076923077|18.333333333333332|72.5             |108.25           |63.5             |NULL              |NULL         |NULL              |NULL             |4.333333333333333|2.6666666666666665|3.0             |76.84313725490196|106.17647058823529|61.94117647058823|NULL            |NULL             |NULL              |NULL             |31.157142857142855|138.0             |15.15        |203.8             |4.15              |29.1              |86.25        |10.625            |33.75        |3.665             |13.825000000000001|0.6499999999999999|12.0         |108.0             |25.0              |9.0               |108.0             |NULL              |35.975            |1.25              |7                |17                 |4                 |12             |\n",
      "|4.05        |47                  |0            |1           |1            |1                     |1                |3                     |1               |1                 |0               |NULL             |NULL            |NULL              |NULL             |NULL             |NULL             |106.26923076923077|1.0          |17.115384615384617|99.46153846153847|NULL             |NULL              |NULL            |NULL             |NULL              |NULL             |NULL            |82.11538461538461|135.03846153846155|99.75636907724234|34.925            |136.0             |22.1         |251.0             |5.15              |29.5              |85.0         |12.4              |34.6         |4.22              |15.7              |0.9               |24.0         |101.0             |25.0              |15.0              |94.0              |2.3               |20.15             |1.0               |9                |8                  |5                 |7              |\n",
      "|1.0576      |51                  |1            |0           |0            |1                     |1                |2                     |1               |3                 |0               |81.125           |91.0            |18.434782608695652|64.26315789473684|99.81818181818181|56.42857142857143|NULL              |NULL         |NULL              |NULL             |6.0              |4.0               |5.0             |NULL             |NULL              |NULL             |NULL            |NULL             |NULL              |NULL             |36.900000000000006|137.0             |11.5         |268.5             |4.35              |30.299999999999997|92.5         |12.15             |32.9         |3.995             |15.649999999999999|3.5               |65.5         |95.0              |25.5              |21.0              |132.0             |2.4               |NULL              |NULL              |10               |8                  |13                |14             |\n",
      "|1.7742      |59                  |0            |0           |0            |2                     |1                |2                     |2               |1                 |0               |70.6             |96.76           |12.038461538461538|78.7             |134.6            |61.8             |NULL              |NULL         |NULL              |NULL             |6.0              |4.0               |5.0             |NULL             |NULL              |NULL             |4.0             |NULL             |NULL              |NULL             |32.53333333333333 |137.85714285714286|8.6          |187.66666666666666|3.314285714285714 |30.03333333333333 |89.0         |11.033333333333333|33.8         |3.6633333333333336|13.299999999999999|0.5166666666666667|5.0          |105.66666666666667|25.666666666666668|10.166666666666666|131.16666666666666|2.085714285714286 |23.299999999999997|1.1               |2                |8                  |3                 |13             |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+-------------+------------------+-----------------+-----------------+------------------+----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+-------------+------------------+------------------+------------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Joining all features and selecting final features for regression modeling...\")\n",
    "\n",
    "# Define feature columns to exclude\n",
    "exclude_columns = {\"ICUSTAY_ID\", \"HADM_ID\", \"SUBJECT_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"}\n",
    "\n",
    "# Join all features and immediately select desired columns\n",
    "modeling_dataset = base_icu_df \\\n",
    "    .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "    .select(*[name for name in base_icu_df \\\n",
    "        .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n",
    "        .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n",
    "        .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "        .columns if name not in exclude_columns])\n",
    "\n",
    "# Cleanup\n",
    "base_icu_df.unpersist()\n",
    "vitals_features_top20.unpersist()\n",
    "labs_features.unpersist()\n",
    "diagnosis_features.unpersist()\n",
    "\n",
    "# Display final info\n",
    "print(f\"‚úÖ Final modeling dataset created with {modeling_dataset.count()} records\")\n",
    "print(\"üìã Sample of final modeling dataset:\")\n",
    "modeling_dataset.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8fbbc",
   "metadata": {},
   "source": [
    "## Normalization & Handeling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87356b3",
   "metadata": {},
   "source": [
    "Display Missing Values by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0a7a7674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:58:49 WARN DAGScheduler: Broadcasting large task binary with size 1774.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ICU_LOS_DAYS': 0, 'AGE_AT_ICU_ADMISSION': 0, 'GENDER_BINARY': 0, 'CAME_FROM_ER': 0, 'HAS_INSURANCE': 0, 'ADMISSION_TYPE_ENCODED': 0, 'ETHNICITY_ENCODED': 0, 'MARITAL_STATUS_ENCODED': 0, 'RELIGION_ENCODED': 0, 'FIRST_UNIT_ENCODED': 0, 'CHANGED_ICU_UNIT': 0, 'VITAL_220045_AVG': 2, 'VITAL_220277_AVG': 2, 'VITAL_220210_AVG': 2, 'VITAL_220181_AVG': 3, 'VITAL_220179_AVG': 3, 'VITAL_220180_AVG': 3, 'VITAL_211_AVG': 10, 'VITAL_742_AVG': 10, 'VITAL_618_AVG': 10, 'VITAL_646_AVG': 10, 'VITAL_223901_AVG': 2, 'VITAL_220739_AVG': 2, 'VITAL_223900_AVG': 2, 'VITAL_220052_AVG': 7, 'VITAL_220050_AVG': 7, 'VITAL_220051_AVG': 7, 'VITAL_223753_AVG': 7, 'VITAL_8441_AVG': 11, 'VITAL_455_AVG': 11, 'VITAL_456_AVG': 11, 'LAB_51221_AVG': 0, 'LAB_50983_AVG': 0, 'LAB_51301_AVG': 0, 'LAB_51265_AVG': 0, 'LAB_50971_AVG': 0, 'LAB_51248_AVG': 0, 'LAB_51250_AVG': 0, 'LAB_51222_AVG': 0, 'LAB_51249_AVG': 0, 'LAB_51279_AVG': 0, 'LAB_51277_AVG': 0, 'LAB_50912_AVG': 0, 'LAB_51006_AVG': 0, 'LAB_50902_AVG': 0, 'LAB_50882_AVG': 0, 'LAB_50868_AVG': 0, 'LAB_50931_AVG': 0, 'LAB_50960_AVG': 2, 'LAB_51275_AVG': 1, 'LAB_51237_AVG': 1, 'PRIMARY_DIAGNOSIS': 0, 'SECONDARY_DIAGNOSIS': 0, 'TERTIARY_DIAGNOSIS': 0, 'TOTAL_DIAGNOSES': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "null_counts = modeling_dataset.select(\n",
    "    [spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in modeling_dataset.columns]\n",
    ").collect()[0]\n",
    "\n",
    "\n",
    "null_counts_dict = {col: null_counts[col] for col in modeling_dataset.columns}\n",
    "print(null_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a097880",
   "metadata": {},
   "source": [
    "we chose min max std beacause -1 will be corresponding to missing values, and if using standardization (aproximation to gaussian distribuction) -1 would correspond to an actual result of a test and not a outlier/ not existing test result. We only aplied this to float columns since others are binary or int(in case of age), final results have a max of 3 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2414bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Filling NULL entries with -1...\n",
      "üìä Computing min-max scaling in _AVG columns, excluding -1 entries...\n",
      "‚úÖ Data set ready for Machine Learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 14:05:24 WARN DAGScheduler: Broadcasting large task binary with size 1917.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----------------+-------------------+------------------+---------------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|VITAL_220045_AVG|VITAL_220277_AVG|VITAL_220210_AVG|VITAL_220181_AVG|VITAL_220179_AVG|VITAL_220180_AVG|VITAL_211_AVG|VITAL_742_AVG|VITAL_618_AVG|VITAL_646_AVG|VITAL_223901_AVG|VITAL_220739_AVG|VITAL_223900_AVG|VITAL_220052_AVG|VITAL_220050_AVG|VITAL_220051_AVG|VITAL_223753_AVG|VITAL_8441_AVG|VITAL_455_AVG|VITAL_456_AVG|LAB_51221_AVG|LAB_50983_AVG|LAB_51301_AVG|LAB_51265_AVG|LAB_50971_AVG|LAB_51248_AVG|LAB_51250_AVG|LAB_51222_AVG|LAB_51249_AVG|LAB_51279_AVG|LAB_51277_AVG|LAB_50912_AVG|LAB_51006_AVG|LAB_50902_AVG|LAB_50882_AVG|LAB_50868_AVG|LAB_50931_AVG|LAB_50960_AVG|LAB_51275_AVG|LAB_51237_AVG|PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----------------+-------------------+------------------+---------------+\n",
      "|1.2488      |69                  |1            |0           |1            |2                     |1                |1                     |3               |3                 |0               |0.50322         |0.82698         |0.25879         |-1.0            |-1.0            |-1.0            |-1.0         |-1.0         |-1.0         |-1.0         |0.4             |0.55            |0.8             |0.21421         |0.19998         |0.49351         |-1.0            |-1.0          |-1.0         |-1.0         |0.72         |0.44118      |0.4356       |0.0          |1.0          |0.11789      |0.22222      |0.65957      |0.35266      |0.66044      |0.84828      |0.32808      |0.27642      |0.88235      |0.24444      |0.22222      |0.17636      |1.0          |0.38863      |0.66667      |7                |2                  |7                 |12             |\n",
      "|0.9792      |30                  |0            |0           |0            |1                     |4                |2                     |0               |1                 |0               |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |0.0          |0.0          |0.0          |1.0          |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0          |-1.0         |-1.0         |0.2944       |0.36765      |0.3725       |0.04069      |0.11625      |0.57561      |0.45         |0.45957      |1.0          |0.2757       |0.98483      |0.0          |0.0          |0.67647      |0.5          |0.08333      |0.0          |0.0          |0.77915      |0.95         |2                |4                  |-1                |2              |\n",
      "|1.2198      |52                  |1            |0           |0            |2                     |1                |1                     |1               |3                 |0               |1.0             |0.88863         |0.54374         |0.43926         |0.22915         |0.56661         |-1.0         |-1.0         |-1.0         |-1.0         |0.0             |0.0             |0.5             |0.26539         |0.0             |0.72543         |-1.0            |-1.0          |-1.0         |-1.0         |0.54057      |0.41176      |0.54052      |0.40172      |0.36563      |0.42683      |0.42708      |0.62234      |0.6087       |0.46963      |0.27414      |0.10236      |0.13008      |0.76471      |0.6          |0.0          |0.24727      |-1.0         |1.0          |0.625        |7                |17                 |4                 |12             |\n",
      "|2.2913      |78                  |1            |0           |1            |1                     |1                |4                     |2               |3                 |0               |0.70547         |1.0             |0.35569         |0.0             |0.0             |0.0             |-1.0         |-1.0         |-1.0         |-1.0         |1.0             |1.0             |0.95            |0.0             |0.12705         |0.0             |0.30769         |-1.0          |-1.0         |-1.0         |0.02667      |0.0          |0.13025      |0.48854      |0.475        |0.41463      |0.64583      |0.11702      |0.0          |0.08411      |0.45517      |0.07087      |0.21951      |0.23529      |0.86667      |0.08333      |0.14909      |-1.0         |0.83096      |0.0          |17               |7                  |17                |13             |\n",
      "|4.05        |47                  |0            |1           |1            |1                     |1                |3                     |1               |1                 |0               |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |1.0          |0.0          |1.0          |0.0          |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |0.0           |0.0          |0.0          |0.842        |0.29412      |0.84226      |0.60458      |0.80313      |0.47561      |0.375        |1.0          |0.85507      |0.72897      |0.66207      |0.1811       |0.3252       |0.35294      |0.6          |0.5          |0.09455      |0.81395      |0.0          |0.0          |9                |8                  |5                 |7              |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-----------------+-------------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "Final DataSet shape: (12, 55)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Filling NULL entries with -1...\")\n",
    "std_columns = [c for c in modeling_dataset.columns if c.endswith('_AVG')]\n",
    "\n",
    "modeling_dataset = modeling_dataset.na.fill(-1)\n",
    "\n",
    "print(\"üìä Computing min-max scaling in _AVG columns, excluding -1 entries...\")\n",
    "min_max_values = {}\n",
    "for col_name in std_columns:\n",
    "    stats = modeling_dataset.filter(col(col_name) != -1.0).agg(\n",
    "        spark_min(col(col_name)).alias(\"min\"),\n",
    "        spark_max(col(col_name)).alias(\"max\")\n",
    "    ).first()\n",
    "    min_max_values[col_name] = (stats[\"min\"], stats[\"max\"])\n",
    "\n",
    "for col_name in std_columns:\n",
    "    min_val, max_val = min_max_values[col_name]\n",
    "    range_val = max_val - min_val if max_val != min_val else 1.0\n",
    "    modeling_dataset = modeling_dataset.withColumn(\n",
    "        col_name, \n",
    "        when(col(col_name) == -1.0, -1.0).otherwise(\n",
    "            round((col(col_name) - min_val) / range_val, 5)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Data set ready for Machine Learning!\")\n",
    "modeling_dataset.show(5, truncate=False)\n",
    "\n",
    "num_rows = modeling_dataset.count()\n",
    "num_cols = len(modeling_dataset.columns)\n",
    "print(f\"Final DataSet shape: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31c2db",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f975-668b-4ab3-a228-4af82187778e",
   "metadata": {},
   "source": [
    "## Preparing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "50fa7d59-2d01-4dcf-868f-900431b44be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 1: Creating train/test split...\n",
      "‚úÖ Data split completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:24 WARN DAGScheduler: Broadcasting large task binary with size 1789.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÜ Training samples: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:32 WARN DAGScheduler: Broadcasting large task binary with size 1789.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üß™ Test samples: 2\n",
      "Feature columns: ['AGE_AT_ICU_ADMISSION', 'GENDER_BINARY', 'CAME_FROM_ER', 'HAS_INSURANCE', 'ADMISSION_TYPE_ENCODED', 'ETHNICITY_ENCODED', 'MARITAL_STATUS_ENCODED', 'RELIGION_ENCODED', 'FIRST_UNIT_ENCODED', 'CHANGED_ICU_UNIT', 'VITAL_220045_AVG', 'VITAL_220277_AVG', 'VITAL_220210_AVG', 'VITAL_220181_AVG', 'VITAL_220179_AVG', 'VITAL_220180_AVG', 'VITAL_211_AVG', 'VITAL_742_AVG', 'VITAL_618_AVG', 'VITAL_646_AVG', 'VITAL_223901_AVG', 'VITAL_220739_AVG', 'VITAL_223900_AVG', 'VITAL_220052_AVG', 'VITAL_220050_AVG', 'VITAL_220051_AVG', 'VITAL_223753_AVG', 'VITAL_8441_AVG', 'VITAL_455_AVG', 'VITAL_456_AVG', 'LAB_51221_AVG', 'LAB_50983_AVG', 'LAB_51301_AVG', 'LAB_51265_AVG', 'LAB_50971_AVG', 'LAB_51248_AVG', 'LAB_51250_AVG', 'LAB_51222_AVG', 'LAB_51249_AVG', 'LAB_51279_AVG', 'LAB_51277_AVG', 'LAB_50912_AVG', 'LAB_51006_AVG', 'LAB_50902_AVG', 'LAB_50882_AVG', 'LAB_50868_AVG', 'LAB_50931_AVG', 'LAB_50960_AVG', 'LAB_51275_AVG', 'LAB_51237_AVG', 'PRIMARY_DIAGNOSIS', 'SECONDARY_DIAGNOSIS', 'TERTIARY_DIAGNOSIS', 'TOTAL_DIAGNOSES']\n",
      "Target column: ICU_LOS_DAYS\n",
      "üìä Step 2: Creating the final vectorized train/test datasets...\n",
      "‚úÖ Final datasets prepared:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1848.9 KiB\n",
      "25/06/05 13:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1854.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÜ Training features shape: (9, 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1848.9 KiB\n",
      "25/06/05 13:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1854.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üß™ Test features shape: (3, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 1: Creating train/test split...\")\n",
    "train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "print(\"‚úÖ Data split completed.\")\n",
    "print(f\"   üöÜ Training samples: {train_data.count()}\")\n",
    "print(f\"   üß™ Test samples: {test_data.count()}\")\n",
    "\n",
    "\n",
    "feature_columns = [col for col in modeling_dataset.columns if col != 'ICU_LOS_DAYS']\n",
    "print(\"Feature columns:\", feature_columns)\n",
    "target_column = 'ICU_LOS_DAYS'\n",
    "print(\"Target column:\", target_column)\n",
    "\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,  \n",
    "    outputCol=\"features\"     \n",
    ")\n",
    "\n",
    "print(\"üìä Step 2: Creating the final vectorized train/test datasets...\")\n",
    "train_final = feature_assembler.transform(train_data).select(\n",
    "    \"features\", \n",
    "    target_column\n",
    ").withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "test_final = feature_assembler.transform(test_data).select(\n",
    "    \"features\", \n",
    "    target_column\n",
    ").withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "train_final.cache()\n",
    "test_final.cache()\n",
    "\n",
    "print(\"‚úÖ Final datasets prepared:\")\n",
    "print(f\"   üöÜ Training features shape: ({train_final.count()}, {len(feature_columns)})\")\n",
    "print(f\"   üß™ Test features shape: ({test_final.count()}, {len(feature_columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c98beb-ae1b-445a-aa7f-9002b4d9a012",
   "metadata": {},
   "source": [
    "## Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "eda7d1c4-c1ef-4261-a3b5-e05db415626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 1: Setting up evaluation metrics...\n",
      "‚úÖ Evaluation metrics configured: RMSE, MAE, R¬≤\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 1: Setting up evaluation metrics...\")\n",
    "\n",
    "# Create regression evaluators\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured: RMSE, MAE, R¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f74323-98d0-44fd-b21f-f2e543449457",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4a25b20c-1f60-4782-b27c-6fda0121af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Step 2: Training Linear Regression model...\n",
      "üïê Started at: 13:59:49\n",
      "   üîÑ Training Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1870.7 KiB\n",
      "25/06/05 13:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1871.7 KiB\n",
      "25/06/05 13:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1874.8 KiB\n",
      "25/06/05 13:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1875.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Linear Regression - Making predictions (test data)...\n",
      "   üîÑ Linear Regression - Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1876.9 KiB\n",
      "25/06/05 13:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1878.0 KiB\n",
      "25/06/05 13:59:54 WARN DAGScheduler: Broadcasting large task binary with size 1876.9 KiB\n",
      "25/06/05 13:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1878.0 KiB\n",
      "25/06/05 13:59:55 WARN DAGScheduler: Broadcasting large task binary with size 1876.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Linear Regression Results:\n",
      "   üìâ RMSE: 5.078 days\n",
      "   üìä MAE: 3.272 days\n",
      "   üìà R¬≤: -1.120\n",
      "üïê Completed at: 13:59:56\n",
      "‚è±Ô∏è Total elapsed time: 7.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1878.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà Step 2: Training Linear Regression model...\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=200,                    # Increased for better convergence\n",
    "    regParam=0.001,                 # Lower regularization for healthcare data\n",
    "    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n",
    "    tol=1e-8,                       # Tighter tolerance for precision\n",
    "    standardization=False,          # We're doing manual scaling\n",
    "    fitIntercept=True,\n",
    "    aggregationDepth=3,             # Better for distributed training\n",
    "    loss=\"squaredError\",\n",
    "    solver=\"normal\"                 # Best for small-medium datasets\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"   üîÑ Training Linear Regression...\")\n",
    "lr_model = lr.fit(train_final)\n",
    "\n",
    "print(\"   üîÑ Linear Regression - Making predictions (test data)...\")\n",
    "lr_predictions = lr_model.transform(test_final)\n",
    "\n",
    "print(\"   üîÑ Linear Regression - Evaluation...\")\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"‚úÖ Linear Regression Results:\")\n",
    "print(f\"   üìâ RMSE: {lr_rmse:.3f} days\")\n",
    "print(f\"   üìä MAE: {lr_mae:.3f} days\")\n",
    "print(f\"   üìà R¬≤: {lr_r2:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847f98b-4da8-41ff-b26f-f2943c8baa38",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "52707ea4-eb6a-449d-8160-7013057b2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ Step 3: Training Random Forest model...\n",
      "üïê Started at: 13:59:56\n",
      "   üîÑ Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 13:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1868.8 KiB\n",
      "25/06/05 13:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1868.8 KiB\n",
      "25/06/05 13:59:57 WARN DAGScheduler: Broadcasting large task binary with size 1868.8 KiB\n",
      "25/06/05 13:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1868.7 KiB\n",
      "25/06/05 13:59:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 9 (= number of training instances)\n",
      "25/06/05 13:59:58 WARN DAGScheduler: Broadcasting large task binary with size 1871.4 KiB\n",
      "25/06/05 13:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1899.4 KiB\n",
      "25/06/05 14:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1961.3 KiB\n",
      "25/06/05 14:00:00 WARN DAGScheduler: Broadcasting large task binary with size 1986.6 KiB\n",
      "25/06/05 14:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1941.5 KiB\n",
      "25/06/05 14:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1879.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Random Forest - Making predictions (test data)...\n",
      "   üîÑ Random Forest - Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 14:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1871.4 KiB\n",
      "25/06/05 14:00:04 WARN DAGScheduler: Broadcasting large task binary with size 1872.5 KiB\n",
      "25/06/05 14:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1871.4 KiB\n",
      "25/06/05 14:00:05 WARN DAGScheduler: Broadcasting large task binary with size 1872.5 KiB\n",
      "25/06/05 14:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1871.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random Forest Results:\n",
      "   üìâ RMSE: 4.207 days\n",
      "   üìä MAE: 2.719 days\n",
      "   üìà R¬≤: -0.455\n",
      "üïê Completed at: 14:00:07\n",
      "‚è±Ô∏è Total elapsed time: 10.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 14:00:07 WARN DAGScheduler: Broadcasting large task binary with size 1872.5 KiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüå≤ Step 3: Training Random Forest model...\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=200,                   # More trees = better accuracy (if enough cores/memory)\n",
    "    maxDepth=12,                    # Deeper trees capture more complexity\n",
    "    minInstancesPerNode=2,          # Allows more granular splits\n",
    "    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n",
    "    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n",
    "    seed=42                         # Reproducibility\n",
    ")\n",
    "\n",
    "print(\"   üîÑ Training Random Forest...\")\n",
    "rf_model = rf.fit(train_final)\n",
    "\n",
    "print(\"   üîÑ Random Forest - Making predictions (test data)...\")\n",
    "rf_predictions = rf_model.transform(test_final)\n",
    "\n",
    "print(\"   üîÑ Random Forest - Evaluation...\")\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = mae_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"‚úÖ Random Forest Results:\")\n",
    "print(f\"   üìâ RMSE: {rf_rmse:.3f} days\")\n",
    "print(f\"   üìä MAE: {rf_mae:.3f} days\")\n",
    "print(f\"   üìà R¬≤: {rf_r2:.3f}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28b690",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e59901b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Linear Regression Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 14:00:08 WARN DAGScheduler: Broadcasting large task binary with size 1868.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------+-------------+\n",
      "|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+-------------+--------------+-------------+\n",
      "|1.2597    |2.244        |0.985         |78.17        |\n",
      "|8.9163    |0.176        |8.74          |98.02        |\n",
      "|1.8064    |1.716        |0.09          |5.01         |\n",
      "+----------+-------------+--------------+-------------+\n",
      "\n",
      "\n",
      "üå≤ Random Forest Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/05 14:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1862.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------+-------------+\n",
      "|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+-------------+--------------+-------------+\n",
      "|1.2597    |1.97         |0.71          |56.39        |\n",
      "|8.9163    |1.668        |7.249         |81.3         |\n",
      "|1.8064    |1.608        |0.198         |10.97        |\n",
      "+----------+-------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n",
    "\n",
    "print(\"\\nüìà Linear Regression Predictions:\")\n",
    "lr_display = lr_predictions.select(\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ")\n",
    "\n",
    "lr_display.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Predictions\n",
    "print(\"\\nüå≤ Random Forest Predictions:\")\n",
    "rf_display = rf_predictions.select(\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ")\n",
    "\n",
    "rf_display.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0d610-37c0-4c99-9fc0-289846b54033",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e3ff000d-87eb-4b26-976a-65db81b057f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Step 5: Model Performance Comparison...\n",
      "üìä Model Performance Summary:\n",
      "+-----------------+-----------------+------------------+-------------------+\n",
      "|Model            |RMSE             |MAE               |R2                 |\n",
      "+-----------------+-----------------+------------------+-------------------+\n",
      "|Linear Regression|5.078344674303861|3.2717855085856535|-1.1202130831027075|\n",
      "|Random Forest    |4.206614476189922|2.7190549194444436|-0.4547909365739031|\n",
      "+-----------------+-----------------+------------------+-------------------+\n",
      "\n",
      "\n",
      "ü•á Best Models:\n",
      "   üéØ Lowest RMSE: Random Forest (4.207 days)\n",
      "   üìà Highest R¬≤: Random Forest (-0.455)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüèÜ Step 5: Model Performance Comparison...\")\n",
    "\n",
    "# Create comparison summary\n",
    "results_data = [\n",
    "    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n",
    "    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n",
    "]\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n",
    "\n",
    "print(\"üìä Model Performance Summary:\")\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "\n",
    "best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n",
    "best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n",
    "\n",
    "print(f\"\\nü•á Best Models:\")\n",
    "print(f\"   üéØ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n",
    "print(f\"   üìà Highest R¬≤: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
