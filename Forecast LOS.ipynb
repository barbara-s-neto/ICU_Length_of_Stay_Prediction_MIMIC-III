{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1d1427",
   "metadata": {},
   "source": [
    "# ICU Length of Stay Prediction - MIMIC-III Pipeline\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n",
    "\n",
    "## ðŸ“Š Data & Constraints\n",
    "- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n",
    "- **Filters**: \n",
    "        - Patient Age 18-80\n",
    "        - LOS 0.1-15 days\n",
    "        - Valid time sequences\n",
    "- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n",
    "\n",
    "\n",
    "## ðŸŒ€ Big Data Processing\n",
    "\n",
    "- **Storage**: We used Google Cloud Dataproc and Google Storage Buckets for MIMIC-III storage \n",
    "- **CHARTEVENTS**: Chart Events table has +330 million rows\n",
    "- **Parquet**: Converted \"CHARTEVENTS\" and \"LABEVENTS\" tables to Parquet format for efficient storage and processing\n",
    "- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n",
    "\n",
    "## ðŸ”§ Features (39 total)\n",
    "- **Demographics (2)**: Age, gender\n",
    "- **Admission (8)**: Emergency/elective, timing, insurance\n",
    "- **ICU Units (6)**: Care unit types, transfers\n",
    "- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n",
    "- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n",
    "- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n",
    "\n",
    "## ðŸ¤– Models & Results\n",
    "- **Linear Regression**: \n",
    "- **Random Forest**: \n",
    "\n",
    "## â˜ï¸ Infrastructure\n",
    "- **GCP Dataproc**: 1x Master and 2x Workers, n2-standard-4  (12 vCPUs, 48GB RAM, 400GB Disk Storage)\n",
    "- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8375f-7f35-415f-8288-2ff2193e6af0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed6638-09bf-4620-89fe-2bb2c00d86e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports loaded successfully!\n",
      "â° Notebook started at: 2025-06-03 20:28:05\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ PySpark Core Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count as spark_count, collect_list, \n",
    "    when, min as spark_min, max as spark_max, \n",
    "    udf, round, abs\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# ðŸ”¢ Data Processing & Feature Engineering\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StandardScaler,\n",
    "    StringIndexer,\n",
    "    MinMaxScaler,\n",
    "    Imputer\n",
    ")\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "# ðŸ¤– Machine Learning Models\n",
    "from pyspark.ml.regression import (\n",
    "    RandomForestRegressor,\n",
    "    LinearRegression\n",
    "    # GBTRegressor\n",
    ")\n",
    "\n",
    "\n",
    "# ðŸ“Š Model Evaluation & Tuning\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# â±ï¸ Date/Time Utilities\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"\\nâœ… All imports loaded successfully!\")\n",
    "print(f\"â° Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1810-67b2-471e-97d2-b8fda7db9728",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a15f1fca-5bf8-4e10-bdca-a6faa11ca17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session created successfully!\n",
      "ðŸ“Š Spark Version: 4.0.0\n",
      "ðŸ”§ Application Name: Forecast-LOS\n",
      "ðŸ’¾ Available cores: 8\n",
      "\n",
      "â° Spark session initialised at: 2025-06-03 20:28:05\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Forecast-LOS\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"32MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"300s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"300s\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memoryOffHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOffHeap.size\", \"1g\") \\\n",
    "    \\\n",
    "    .getOrCreate()\n",
    "print(\"âœ… Spark session created successfully!\")\n",
    "print(f\"ðŸ“Š Spark Version: {spark.version}\")\n",
    "print(f\"ðŸ”§ Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"ðŸ’¾ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\nâ° Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13163e-55dc-4046-95b0-4815723d0581",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4de5cb",
   "metadata": {},
   "source": [
    "Strategy: Pre-filter CHARTEVENTS to find ICU stays with required vital signs, then efficiently load all tables using broadcast joins and lookup tables.\n",
    "Key Steps:\n",
    "\n",
    "- Filter for ICU stays with â‰¥1 of 6 vital signs (HR, BP, RR, Temp, SpO2)\n",
    "- Create lookup tables for ICUSTAY_ID, HADM_ID, SUBJECT_ID\n",
    "- Load all tables with pre-filtering using broadcast joins\n",
    "- Convert large files to \"Parquet\" for performance\n",
    "\n",
    "Result: Memory-efficient loading of only relevant data with quality assurance that all ICU stays have vital signs measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration flags\n",
    "SAMPLE_ENABLE = False\n",
    "SAMPLE_SIZE = 20000\n",
    "MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"ðŸ¥ Loading MIMIC-III data...\")\n",
    "\n",
    "# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "print(\"ðŸ“‚ Loading CHARTEVENTS...\")\n",
    "\n",
    "try:\n",
    "    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "    print(\"âœ… Loaded CHARTEVENTS from parquet\")\n",
    "except:\n",
    "    print(\"ðŸ“„ Converting CHARTEVENTS.csv.gz to parquet...\")\n",
    "    chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n",
    "    chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "    print(\"âœ… Converted and loaded CHARTEVENTS\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load ICUSTAYS \n",
    "print(\"\\nðŸ“‚ Loading and filtering ICUSTAYS...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Apply sampling if enabled\n",
    "if SAMPLE_ENABLE:\n",
    "    print(f\"ðŸŽ¯ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "    print(f\"âœ… Final sample: {actual_sample_size} ICU stays\")\n",
    "else:\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "\n",
    "    \n",
    "    \n",
    "# Step 4: Create efficient lookup tables\n",
    "print(\"ðŸ“‹ Creating ID lookup tables...\")\n",
    "icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "\n",
    "icu_lookup.count()  # Trigger caching\n",
    "hadm_lookup.count()\n",
    "subject_lookup.count()\n",
    "\n",
    "# Step 5: Load other tables with optimized joins\n",
    "print(\"ðŸ“‚ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n",
    "patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "\n",
    "print(\"ðŸ“‚ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n",
    "admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"ðŸ“‚ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n",
    "diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Step 6: Load and filter CHARTEVENTS efficiently\n",
    "print(\"ðŸ“‚ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "chartevents_df = chartevents_df \\\n",
    "    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "\n",
    "# Step 7: Load LABEVENTS\n",
    "print(\"ðŸ“‚ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "try:\n",
    "    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "except:\n",
    "    print(\"ðŸ“„ Converting LABEVENTS.csv.gz to parquet...\")\n",
    "    labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n",
    "    labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "\n",
    "labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nâœ… Data loading complete!\")\n",
    "print(f\"ðŸ“Š ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "print(f\"ðŸ“Š PATIENTS: {patients_df.count():,} rows\") \n",
    "print(f\"ðŸ“Š ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "print(f\"ðŸ“Š DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "print(f\"ðŸ“Š CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "print(f\"ðŸ“Š LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "print(f\"\\nâ° Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0db241",
   "metadata": {},
   "source": [
    "TIRAR ISTO ANTES DE ENTREGAR: ISTO E PARA CORRER LOCALMENTE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62936c54-45ab-4c03-a8ec-fc3d87f68721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥ Loading MIMIC-III data...\n",
      "ðŸ“‚ Loading CHARTEVENTS...\n",
      "âœ… Loaded CHARTEVENTS from parquet\n",
      "\n",
      "ðŸ“‚ Loading and filtering ICUSTAYS...\n",
      "ðŸ“‹ Creating ID lookup tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:28:05 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/03 20:28:05 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/03 20:28:05 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/03 20:28:05 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading PATIENTS table...\n",
      "ðŸ“‚ Loading ADMISSIONS table...\n",
      "ðŸ“‚ Loading DIAGNOSES_ICD table...\n",
      "ðŸ“‚ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\n",
      "ðŸ“‚ Loading LABEVENTS table... [FILTERING BY HADM_ID]\n",
      "\n",
      "âœ… Data loading complete!\n",
      "ðŸ“Š ICUSTAYS: 20 rows\n",
      "ðŸ“Š PATIENTS: 20 rows\n",
      "ðŸ“Š ADMISSIONS: 20 rows\n",
      "ðŸ“Š DIAGNOSES_ICD: 212 rows\n",
      "ðŸ“Š CHARTEVENTS (filtered): 57,973 rows\n",
      "ðŸ“Š LABEVENTS (filtered): 5,895 rows\n",
      "\n",
      "â° Data loaded at: 2025-06-03 20:28:06\n"
     ]
    }
   ],
   "source": [
    "# Configuration flags\n",
    "#SAMPLE_ENABLE = False\n",
    "#SAMPLE_SIZE = 20000\n",
    "#MIMIC_PATH = \"mimic-db-short\"\n",
    "#\n",
    "#\n",
    "#\n",
    "#print(\"ðŸ¥ Loading MIMIC-III data...\")\n",
    "#\n",
    "## Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "#print(\"ðŸ“‚ Loading CHARTEVENTS...\")\n",
    "#\n",
    "#\n",
    "#chartevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "#print(\"âœ… Loaded CHARTEVENTS from parquet\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "## Step 2: Load ICUSTAYS \n",
    "#print(\"\\nðŸ“‚ Loading and filtering ICUSTAYS...\")\n",
    "#icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "#\n",
    "#\n",
    "#\n",
    "## Step 3: Apply sampling if enabled\n",
    "#if SAMPLE_ENABLE:\n",
    "#    print(f\"ðŸŽ¯ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "#    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "#    icustays_df.cache()\n",
    "#    actual_sample_size = icustays_df.count()\n",
    "#    print(f\"âœ… Final sample: {actual_sample_size} ICU stays\")\n",
    "#else:\n",
    "#    icustays_df.cache()\n",
    "#    actual_sample_size = icustays_df.count()\n",
    "#\n",
    "#    \n",
    "#    \n",
    "## Step 4: Create efficient lookup tables\n",
    "#print(\"ðŸ“‹ Creating ID lookup tables...\")\n",
    "#icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "#hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "#subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "#\n",
    "#icu_lookup.count()  # Trigger caching\n",
    "#hadm_lookup.count()\n",
    "#subject_lookup.count()\n",
    "#\n",
    "## Step 5: Load other tables with optimized joins\n",
    "#print(\"ðŸ“‚ Loading PATIENTS table...\")\n",
    "#patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "#patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "#\n",
    "#print(\"ðŸ“‚ Loading ADMISSIONS table...\")\n",
    "#admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "#admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "#print(\"ðŸ“‚ Loading DIAGNOSES_ICD table...\")\n",
    "#diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "#diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "## Step 6: Load and filter CHARTEVENTS efficiently\n",
    "#print(\"ðŸ“‚ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "#chartevents_df = chartevents_df \\\n",
    "#    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "#    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "#\n",
    "## Step 7: Load LABEVENTS\n",
    "#print(\"ðŸ“‚ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "#\n",
    "#labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "#\n",
    "#\n",
    "#labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "## Final summary\n",
    "#print(\"\\nâœ… Data loading complete!\")\n",
    "#print(f\"ðŸ“Š ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "#print(f\"ðŸ“Š PATIENTS: {patients_df.count():,} rows\") \n",
    "#print(f\"ðŸ“Š ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "#print(f\"ðŸ“Š DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "#print(f\"ðŸ“Š CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "#print(f\"ðŸ“Š LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "#print(f\"\\nâ° Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e7157-1230-41d3-8f41-1117b62fdb55",
   "metadata": {},
   "source": [
    "# Features Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763e6d7-4ba5-439b-8e2b-ba16cf607a3e",
   "metadata": {},
   "source": [
    "## Extracting Data From ICUSTAYS\n",
    "\n",
    "**Purpose**: Create comprehensive ICU dataset by joining ICU stays with patient demographics and admission details.\n",
    "\n",
    "**Key Features**:\n",
    "- **Target Variable**: ICU_LOS_DAYS (length of stay)\n",
    "- **Demographics**: Age (18-80), gender, ethnicity\n",
    "- **Clinical**: Care units, admission type/location, insurance\n",
    "- **Outcomes**: Hospital/patient death flags\n",
    "- **Identifiers**: ICUSTAY_ID, SUBJECT_ID, HADM_ID\n",
    "\n",
    "**Age Filter**: Adults only (18-80 years) to exclude pediatric/very elderly edge cases.\n",
    "\n",
    "**Result**: Clean base dataset ready for vital signs feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f1d7adb-eac0-4071-b252-a04268f0c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 1: Creating base ICU dataset with patient demographics...\n",
      "âœ… Created base ICU dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 1: Creating base ICU dataset with patient demographics...\")\n",
    "\n",
    "base_icu_df = icustays_df.alias(\"icu\") \\\n",
    "    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n",
    "    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        # ICU stay identifiers\n",
    "        col(\"icu.ICUSTAY_ID\"),\n",
    "        col(\"icu.SUBJECT_ID\"), \n",
    "        col(\"icu.HADM_ID\"),\n",
    "        \n",
    "        # Target variable - Length of Stay in ICU (days)\n",
    "        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n",
    "        \n",
    "        # ICU characteristics\n",
    "        col(\"icu.FIRST_CAREUNIT\"),\n",
    "        col(\"icu.LAST_CAREUNIT\"), \n",
    "        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n",
    "        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n",
    "        \n",
    "        # Patient demographics\n",
    "        col(\"pat.GENDER\"),\n",
    "        col(\"pat.DOB\"),\n",
    "        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n",
    "        \n",
    "        # Admission details\n",
    "        col(\"adm.ADMITTIME\"),\n",
    "        col(\"adm.DISCHTIME\"), \n",
    "        col(\"adm.ADMISSION_TYPE\"),\n",
    "        col(\"adm.ADMISSION_LOCATION\"),\n",
    "        col(\"adm.INSURANCE\"),\n",
    "        col(\"adm.ETHNICITY\"),\n",
    "        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n",
    "        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n",
    "    )\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n",
    "                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n",
    "                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n",
    "\n",
    "\n",
    "print(\"âœ… Created base ICU dataset!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595638e6-74f7-4f2f-a7e5-1fc692089206",
   "metadata": {},
   "source": [
    "## Extracting Categorical Features\n",
    "\n",
    "**Features Created**:\n",
    "- **GENDER_BINARY**: Male = 1, Female = 0\n",
    "- **CAME_FROM_ER**: Emergency admission = 1\n",
    "- **HAS_INSURANCE**: Medicare = 1, other = 0\n",
    "- **ADMISSION_TYPE_ENCODED**: Emergency=1, Elective=2, Urgent=3, Other=0\n",
    "- **ETHNICITY_ENCODED**: White=1, Black=2, Hispanic=3, Asian=4, Other=5\n",
    "\n",
    "**Result**: Categorical variables converted to numerical format for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d28d34e6-83c3-40ae-95d0-71f9929397a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 2: Engineering categorical features...\n",
      "âœ… Base ICU Dataset - Categorical Features\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 2: Engineering categorical features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"ETHNICITY_ENCODED\",\n",
    "                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1) \\\n",
    "                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2) \\\n",
    "                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3) \\\n",
    "                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4) \\\n",
    "                .otherwise(5))\n",
    "\n",
    "print(\"âœ… Base ICU Dataset - Categorical Features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca22bc8-8dd5-4d30-9cb9-0f366da21d76",
   "metadata": {},
   "source": [
    "## Extracting ICU Unit Types\n",
    "\n",
    "**Purpose**: Create categorical features for ICU unit types and transfers.\n",
    "\n",
    "**Features Created**:\n",
    "- **FIRST_UNIT_ENCODED**: Numerical encoding of ICU units\n",
    " - MICU (Medical) = 1\n",
    " - SICU (Surgical) = 2  \n",
    " - CSRU (Cardiac Surgery) = 3\n",
    " - CCU (Coronary Care) = 4\n",
    " - TSICU (Trauma Surgical) = 5\n",
    " - Other = 0\n",
    "- **CHANGED_ICU_UNIT**: Binary flag (1 if patient transferred between units)\n",
    "\n",
    "**Clinical Significance**: Different ICU types have varying complexity and typical LOS patterns. Unit transfers often indicate complications.\n",
    "\n",
    "**Result**: Enhanced dataset with ICU unit complexity and transfer indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6617c3f2-75b9-4fa0-93a8-7161d0c2dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 3: Creating ICU unit type features...\n",
      "âœ… Base ICU Dataset - Unit Type Features\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 3: Creating ICU unit type features...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"FIRST_UNIT_ENCODED\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"CHANGED_ICU_UNIT\", \n",
    "                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n",
    "\n",
    "\n",
    "print(\"âœ… Base ICU Dataset - Unit Type Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74662ab6-f263-402e-913f-dc04804eadff",
   "metadata": {},
   "source": [
    "## Extracting Time-based Features\n",
    "\n",
    "**Action**: Filter out invalid records where INTIME >= OUTTIME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef431386-85f1-4867-9aa1-f5bb84d7c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 4: Creating time-based features...\n",
      "âœ… Base ICU Dataset - Time Based Features\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 4: Creating time-based features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n",
    "print(\"âœ… Base ICU Dataset - Time Based Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ac96b-6e5f-4fdd-84a2-043a0beaa06b",
   "metadata": {},
   "source": [
    "## Remove Outliers (Excessive Length Of Stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6ce0e40-182e-4fbd-bb35-2bb23ed0d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ ICU Length of Stay Statistics (Days):\n",
      "+-------+-----------------+\n",
      "|summary|     ICU_LOS_DAYS|\n",
      "+-------+-----------------+\n",
      "|  count|               16|\n",
      "|   mean|        2.2869125|\n",
      "| stddev|2.035743709106494|\n",
      "|    min|            0.758|\n",
      "|    max|           8.9163|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“ˆ ICU Length of Stay Statistics (Days):\")\n",
    "base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4867ef-642f-4ebe-b1f4-f5a85ddf3e64",
   "metadata": {},
   "source": [
    "We kept every ICU STAY that had duration (LOS) between 0.0 and 9.1 days, considered normal legnths since:\n",
    "\n",
    "| Statistic                | Value (days)                                    |\n",
    "| ------------------------ | ----------------------------------------------- |\n",
    "| **Minimum**              | 0.0 (can be admission + discharge on same day)  |\n",
    "| **25th percentile (Q1)** | \\~1.1                                           |\n",
    "| **Median (Q2)**          | \\~2.1                                           |\n",
    "| **75th percentile (Q3)** | \\~4.3                                           |\n",
    "| **Maximum**              | \\~88 (but can go slightly higher in edge cases) |\n",
    "| **Mean**                 | \\~3.3â€“3.5                                       |\n",
    "\n",
    "Using interquartile range (IQR) method:\n",
    "\n",
    "* IQR = Q3 - Q1 = 4.3 - 1.1 = ~3.2\n",
    "\n",
    "* Upper Bound for outliers = Q3 + 1.5 Ã— IQR â‰ˆ 4.3 + 4.8 = ~9.1 days\n",
    "\n",
    "* Lower Bound = Q1 - 1.5 Ã— IQR â‰ˆ 1.1 - 4.8 = < 0, which is ignored since LOS canâ€™t be negative\n",
    "\n",
    "So:\n",
    "\n",
    "* Typical ICU LOS: 1.1 to 4.3 days\n",
    "\n",
    "* Outliers: ICU stays longer than ~9.1 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "534ab904-5a6a-4f71-899e-e6b49620b0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 5: Cleaning target variable...\n",
      "âœ… Base ICU Dataset - Remove Outliers\n",
      "\n",
      "ðŸ“‹ Sample of ICU stay records:\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+\n",
      "|ICUSTAY_ID|SUBJECT_ID|HADM_ID|ICU_LOS_DAYS|         ICU_INTIME|        ICU_OUTTIME|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+\n",
      "|    255819|      1452| 156406|       0.758|2129-05-10 21:36:40|2129-05-11 15:48:11|                  56|            0|           0|            0|                     1|                1|                 1|               1|\n",
      "|    231977|      8470| 184688|      0.9792|2174-09-01 18:14:58|2174-09-02 17:45:00|                  30|            0|           0|            0|                     1|                4|                 1|               0|\n",
      "|    264061|     22862| 108676|      1.0576|2178-08-07 20:44:01|2178-08-08 22:06:54|                  51|            1|           0|            0|                     1|                1|                 3|               0|\n",
      "|    248205|     18322| 163177|        4.05|2103-06-30 15:27:26|2103-07-04 16:39:27|                  47|            0|           1|            1|                     1|                1|                 1|               0|\n",
      "|    279243|     44303| 110159|      1.6863|2167-02-11 22:05:57|2167-02-13 14:34:15|                  29|            1|           0|            0|                     1|                1|                 1|               0|\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 5: Cleaning target variable...\")\n",
    "\n",
    "# List of desired columns\n",
    "selected_columns = [\n",
    "    \"ICUSTAY_ID\", \"SUBJECT_ID\", \"HADM_ID\", \"ICU_LOS_DAYS\", \"ICU_INTIME\", \"ICU_OUTTIME\", \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\", \"CAME_FROM_ER\",\n",
    "    \"HAS_INSURANCE\", \"ADMISSION_TYPE_ENCODED\", \"ETHNICITY_ENCODED\",\n",
    "    \"FIRST_UNIT_ENCODED\", \"CHANGED_ICU_UNIT\"\n",
    "]\n",
    "\n",
    "# Apply filter and select columns\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_LOS_DAYS\").between(0.0, 9.1)) \\\n",
    "    .select(*selected_columns) \\\n",
    "    .cache()\n",
    "\n",
    "print(\"âœ… Base ICU Dataset - Remove Outliers\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample of ICU stay records:\")\n",
    "base_icu_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268129f5-72e2-40a5-b676-cbd825ae84c8",
   "metadata": {},
   "source": [
    "## Extracting Clinical Events\n",
    "\n",
    "**Purpose**: Extract top 20 most common CHARTEVENTS as features for ML models.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Find 20 most frequent CHARTEVENTS (typically vital signs)\n",
    "2. **Calculate**: Average value of each test in first 24 hours of ICU stay\n",
    "3. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n",
    "\n",
    "**Time Window**: First 24 hours after ICU admission (INTIME + 24h)\n",
    "\n",
    "**Result**: 20 vital signs features with consistent **-1** encoding for missing data, ensuring ML algorithm compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9dc92782-4f16-4c82-bd8a-44ed78e92965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Identifying top 20 most frequent tests from CHARTEVENTS...\n",
      "ðŸŽ¯ Top 20 chart items selected: {220045: 'VITAL_220045', 220277: 'VITAL_220277', 220210: 'VITAL_220210', 220181: 'VITAL_220181', 220179: 'VITAL_220179', 220180: 'VITAL_220180', 211: 'VITAL_211', 742: 'VITAL_742', 618: 'VITAL_618', 646: 'VITAL_646', 223901: 'VITAL_223901', 220739: 'VITAL_220739', 223900: 'VITAL_223900', 220052: 'VITAL_220052', 220050: 'VITAL_220050', 220051: 'VITAL_220051', 223753: 'VITAL_223753', 8441: 'VITAL_8441', 455: 'VITAL_455', 456: 'VITAL_456'}\n",
      "ðŸ“Š Filtering CHARTEVENTS for top 20 items...\n",
      "ðŸ“Š Calculating aggregates for top 20 vitals...\n",
      "âœ… Created 20 features from top 20 vital signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+-------------+-----------------+-----------------+----------------+----------------+----------------+-----------------+------------------+-----------------+-----------------+-----------------+----------------+-----------------+\n",
      "|ICUSTAY_ID|  VITAL_220045_AVG|VITAL_220277_AVG|  VITAL_220210_AVG| VITAL_220181_AVG|  VITAL_220179_AVG| VITAL_220180_AVG|    VITAL_211_AVG|VITAL_742_AVG|    VITAL_618_AVG|    VITAL_646_AVG|VITAL_223901_AVG|VITAL_220739_AVG|VITAL_223900_AVG| VITAL_220052_AVG|  VITAL_220050_AVG| VITAL_220051_AVG| VITAL_223753_AVG|   VITAL_8441_AVG|   VITAL_455_AVG|    VITAL_456_AVG|\n",
      "+----------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+-------------+-----------------+-----------------+----------------+----------------+----------------+-----------------+------------------+-----------------+-----------------+-----------------+----------------+-----------------+\n",
      "|    255819|              NULL|            NULL|              NULL|             NULL|              NULL|             NULL|74.58823529411765|          1.0|16.41176470588235|97.70588235294117|            NULL|            NULL|            NULL|             NULL|              NULL|             NULL|             NULL|57.11764705882353|94.6470588235294|65.70588235294117|\n",
      "|    279243|101.48936170212765|80.8780487804878|29.729166666666668| 75.2258064516129| 131.0344827586207|60.03448275862069|             NULL|         NULL|             NULL|             NULL|             1.0|             1.0|             1.0|74.54166666666667| 99.58333333333333|64.20833333333333|              1.0|             NULL|            NULL|             NULL|\n",
      "|    298190|              84.5|99.1923076923077|23.615384615384617|84.33333333333333|119.66666666666667|76.66666666666667|             NULL|         NULL|             NULL|             NULL|             5.0|             3.0|             1.8| 74.5925925925926|109.11111111111111|55.73076923076923|3.909090909090909|             NULL|            NULL|             NULL|\n",
      "|    264061|            81.125|            91.0|18.434782608695652|64.26315789473684| 99.81818181818181|56.42857142857143|             NULL|         NULL|             NULL|             NULL|             6.0|             4.0|             5.0|             NULL|              NULL|             NULL|             NULL|             NULL|            NULL|             NULL|\n",
      "|    231977|              NULL|            NULL|              NULL|             NULL|              NULL|             NULL|            89.36|          1.0|            16.64|            100.0|            NULL|            NULL|            NULL|             NULL|              NULL|             NULL|             NULL|             NULL|            NULL|             NULL|\n",
      "+----------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+-------------+-----------------+-----------------+----------------+----------------+----------------+-----------------+------------------+-----------------+-----------------+-----------------+----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Identifying top 20 most frequent tests from CHARTEVENTS...\")\n",
    "\n",
    "\n",
    "# Get frequency count of each ITEMID in CHARTEVENTS\n",
    "itemid_counts = chartevents_df \\\n",
    "    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .groupBy(\"ITEMID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .collect()\n",
    "\n",
    "# Create mapping dictionary for top 20 items\n",
    "top_20_items = {row[\"ITEMID\"]: f\"VITAL_{row['ITEMID']}\" for row in itemid_counts}\n",
    "print(f\"ðŸŽ¯ Top 20 chart items selected: {top_20_items}\")\n",
    "\n",
    "print(\"ðŸ“Š Filtering CHARTEVENTS for top 20 items...\")\n",
    "\n",
    "chartevents_top20 = chartevents_df \\\n",
    "    .filter(col(\"ITEMID\").isin(list(top_20_items.keys()))) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(col(\"CHARTTIME\").between(col(\"ICU_INTIME\"), col(\"ICU_OUTTIME\"))) \\\n",
    "    .select(\"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\")\n",
    "\n",
    "# Process first 24 hours\n",
    "vitals_24h_top20 = chartevents_top20.alias(\"ce\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(\n",
    "        col(\"ce.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\"), \n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"ðŸ“Š Calculating aggregates for top 20 vitals...\")\n",
    "\n",
    "# Initialize with ICUSTAY_ID\n",
    "vitals_features_top20 = base_icu_df.select(\"ICUSTAY_ID\")\n",
    "\n",
    "# Process each vital sign\n",
    "for itemid, name in top_20_items.items():\n",
    "    #print(f\"Processing {name} (ITEMID={itemid})...\")\n",
    "    \n",
    "    vital_stats = vitals_24h_top20 \\\n",
    "        .filter(col(\"ITEMID\") == itemid) \\\n",
    "        .groupBy(\"ICUSTAY_ID\") \\\n",
    "        .agg(avg(\"VALUENUM\").alias(f\"{name}_AVG\"))\n",
    "    \n",
    "    # Left join (without filling NULLs yet)\n",
    "    vitals_features_top20 = vitals_features_top20.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Cleanup\n",
    "chartevents_df.unpersist()\n",
    "vitals_24h_top20.unpersist()\n",
    "\n",
    "# Verify no NULLs remain\n",
    "print(f\"âœ… Created {len(top_20_items)} features from top 20 vital signs\")\n",
    "vitals_features_top20.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08c810-405c-48ca-a141-1a48e125635c",
   "metadata": {},
   "source": [
    "## Extracting Laboratory Events\n",
    "\n",
    "**Purpose**: Extract top 20 most common lab tests as features for ML models.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Find 20 most frequent LABEVENTS (blood tests, chemistry panels)\n",
    "2. **Time Window**: 6 hours before ICU admission + first 24 hours in ICU (30h total)\n",
    "3. **Calculate**: Average value of each lab test within the 30-hour window\n",
    "4. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n",
    "\n",
    "**Time Range**: ICU_INTIME - 6h to ICU_INTIME + 24h\n",
    "\n",
    "**Result**: 20 lab test features with consistent -1 encoding for missing data, capturing pre-ICU and early ICU clinical status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a676e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Creating laboratory features from LABEVENTS...\n",
      "ðŸ“Š Identifying top 20 most frequent lab items...\n",
      "ðŸŽ¯ Top 20 lab items selected: [51221, 50983, 50971, 51265, 51222, 51301, 51250, 51248, 51249, 51279, 51277, 50912, 51006, 50902, 50882, 50868, 50931, 50960, 50893, 51275]\n",
      "ðŸ“Š Filtering LABEVENTS for top 20 items...\n",
      "ðŸ“Š Calculating laboratory statistics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 20 lab features for 16 ICU stays\n",
      "ðŸ“Š Sample features:\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+-------------+-----------------+\n",
      "|ICUSTAY_ID|LAB_51221_AVG     |LAB_50983_AVG     |LAB_50971_AVG     |LAB_51265_AVG     |LAB_51222_AVG     |LAB_51301_AVG    |LAB_51250_AVG    |LAB_51248_AVG     |LAB_51249_AVG     |LAB_51279_AVG     |LAB_51277_AVG     |LAB_50912_AVG     |LAB_51006_AVG     |LAB_50902_AVG     |LAB_50882_AVG     |LAB_50868_AVG|LAB_50931_AVG    |LAB_50960_AVG     |LAB_50893_AVG|LAB_51275_AVG    |\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+-------------+-----------------+\n",
      "|255819    |25.53333333333333 |141.33333333333334|4.9               |33.666666666666664|8.966666666666667 |2.5              |88.33333333333333|31.066666666666663|35.166666666666664|2.9166666666666665|20.566666666666666|0.8666666666666666|24.666666666666668|111.33333333333333|20.666666666666668|14.0         |101.0            |3.35              |7.4          |37.4             |\n",
      "|279243    |42.775000000000006|147.6             |5.833333333333333 |200.0             |15.275            |18.5             |90.0             |32.125            |35.675            |4.755             |14.0              |3.4               |26.8              |103.6             |14.6              |35.6         |111.2            |2.12              |6.2          |69.08333333333333|\n",
      "|298190    |32.349999999999994|140.0             |5.0               |219.66666666666666|11.233333333333334|25.73333333333333|83.33333333333333|28.733333333333334|34.6              |3.9               |17.333333333333332|0.6499999999999999|17.0              |110.0             |20.5              |16.0         |143.0            |1.8               |NULL         |29.9             |\n",
      "|264061    |36.900000000000006|137.0             |4.35              |268.5             |12.15             |11.5             |92.5             |30.299999999999997|32.9              |3.995             |15.649999999999999|3.5               |65.5              |95.0              |25.5              |21.0         |132.0            |2.4               |8.7          |NULL             |\n",
      "|231977    |28.080000000000002|137.25            |3.5800000000000005|119.8             |9.860000000000001 |11.28            |86.8             |30.32             |35.1              |3.25              |17.259999999999998|0.325             |4.0               |106.5             |24.25             |10.0         |85.33333333333333|1.4249999999999998|7.55         |32.48            |\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+-------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ§ª Creating laboratory features from LABEVENTS...\")\n",
    "\n",
    "# Step 1: Identify top 20 most frequent lab items\n",
    "print(\"ðŸ“Š Identifying top 20 most frequent lab items...\")\n",
    "top_20_lab_items = labevents_df \\\n",
    "    .filter(col(\"HADM_ID\").isin([row[\"HADM_ID\"] for row in base_icu_df.select(\"HADM_ID\").collect()])) \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"VALUENUM\") > 0) \\\n",
    "    .groupBy(\"ITEMID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .collect()\n",
    "\n",
    "# Create mapping dictionary with clean LAB_[ITEMID] format\n",
    "lab_items = {row[\"ITEMID\"]: f\"LAB_{row['ITEMID']}\" for row in top_20_lab_items}\n",
    "print(f\"ðŸŽ¯ Top 20 lab items selected: {list(lab_items.keys())}\")\n",
    "\n",
    "# Step 2: Filter lab events within first 24 hours of ICU stay\n",
    "print(\"ðŸ“Š Filtering LABEVENTS for top 20 items...\")\n",
    "labs_24h = labevents_df.alias(\"le\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n",
    "    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n",
    "    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"le.VALUENUM\") > 0) \\\n",
    "    .filter(\n",
    "        col(\"le.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Step 3: Calculate lab statistics with clean column names\n",
    "print(\"ðŸ“Š Calculating laboratory statistics...\")\n",
    "labs_features = base_icu_df.select(\"ICUSTAY_ID\")\n",
    "\n",
    "for itemid, name in lab_items.items():\n",
    "    item_stats = labs_24h \\\n",
    "        .filter(col(\"ITEMID\") == itemid) \\\n",
    "        .groupBy(\"ICUSTAY_ID\") \\\n",
    "        .agg(\n",
    "            avg(\"VALUENUM\").alias(f\"{name}_AVG\")  # Simple alias without coalesce in the name\n",
    "        )\n",
    "    \n",
    "    labs_features = labs_features.join(item_stats, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Cleanup\n",
    "labevents_df.unpersist()\n",
    "labs_24h.unpersist()\n",
    "\n",
    "print(f\"âœ… Created {len(lab_items)} lab features for {labs_features.count():,} ICU stays\")\n",
    "\n",
    "# Show sample of features with clean column names\n",
    "print(\"ðŸ“Š Sample features:\")\n",
    "labs_features.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    *[col for col in labs_features.columns if col != \"ICUSTAY_ID\"]\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cd034-b129-46b0-b33f-cf903ee438ae",
   "metadata": {},
   "source": [
    "## Diagnosis ICD\n",
    "\n",
    "**Purpose**: Extract diagnosis patterns as ML features from ICD-9 codes.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Top 10 most frequent ICD-9 diagnosis codes\n",
    "2. **Count**: Total diagnoses per admission (comorbidity burden)\n",
    "3. **Create**: Binary features for each top 10 diagnosis (HAS_[CODE])\n",
    "\n",
    "**Features Created**:\n",
    "- **TOTAL_DIAGNOSES**: Count of all diagnoses (comorbidity indicator)\n",
    "- **HAS_[ICD9_CODE]**: Binary flags for top 10 most common diagnoses\n",
    "\n",
    "**Result**: 11 diagnosis features (1 count + 10 binary) capturing disease complexity and specific conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94a414c1-7139-4e27-a5e4-767919a3eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¥ Creating diagnosis features from ICD codes...\n",
      "ðŸ“Š Identifying top 10 most frequent diagnoses...\n",
      "ðŸŽ¯ Top 10 ICD9 codes: ['4019', '2724', '42731', '0389', '5849', '25000', '2449', '2851', '486', '496']\n",
      "âœ… Created 10 diagnosis features for 20 admissions\n",
      "ðŸ“Š Sample features:\n",
      "+-------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "|HADM_ID|TOTAL_DIAGNOSES|HAS_4019|HAS_2724|HAS_42731|HAS_0389|HAS_5849|HAS_25000|HAS_2449|HAS_2851|HAS_486|HAS_496|\n",
      "+-------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "|152943 |7              |1       |0       |0        |0       |0       |1        |0       |0       |0      |0      |\n",
      "|163177 |7              |0       |0       |0        |0       |0       |0        |0       |1       |0      |1      |\n",
      "|110159 |12             |0       |0       |0        |1       |1       |0        |0       |0       |0      |0      |\n",
      "|109820 |11             |1       |0       |0        |1       |0       |0        |1       |0       |1      |0      |\n",
      "|150954 |6              |1       |0       |0        |0       |0       |0        |1       |0       |0      |1      |\n",
      "+-------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "â° Clinical features completed at: 2025-06-03 20:28:17\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ¥ Creating diagnosis features from ICD codes...\")\n",
    "\n",
    "# Step 1: Identify top 10 most frequent ICD9 codes\n",
    "print(\"ðŸ“Š Identifying top 10 most frequent diagnoses...\")\n",
    "top_10_diagnoses = diagnoses_df \\\n",
    "    .groupBy(\"ICD9_CODE\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .collect()\n",
    "\n",
    "top_10_codes = [row[\"ICD9_CODE\"] for row in top_10_diagnoses]\n",
    "print(f\"ðŸŽ¯ Top 10 ICD9 codes: {top_10_codes}\")\n",
    "\n",
    "# Step 2: Count total diagnoses per admission (comorbidity burden)\n",
    "diagnosis_features = diagnoses_df.groupBy(\"HADM_ID\") \\\n",
    "    .agg(\n",
    "        spark_count(\"ICD9_CODE\").alias(\"TOTAL_DIAGNOSES\"),\n",
    "        collect_list(\"ICD9_CODE\").alias(\"DIAGNOSIS_CODES\")\n",
    "    )\n",
    "\n",
    "# Step 3: Create binary features for top 10 diagnoses\n",
    "for code in top_10_codes:\n",
    "    diagnosis_features = diagnosis_features.withColumn(\n",
    "        f\"HAS_{code}\",\n",
    "        when(array_contains(col(\"DIAGNOSIS_CODES\"), code), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# Drop the raw codes list\n",
    "diagnosis_features = diagnosis_features.drop(\"DIAGNOSIS_CODES\")\n",
    "\n",
    "print(f\"âœ… Created {len(top_10_codes)} diagnosis features for {diagnosis_features.count():,} admissions\")\n",
    "\n",
    "# Show sample of features\n",
    "print(\"ðŸ“Š Sample features:\")\n",
    "diagnosis_features.show(5, truncate=False)\n",
    "\n",
    "print(f\"\\nâ° Clinical features completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d9140-230d-4bc0-91ae-fd9db043e5a5",
   "metadata": {},
   "source": [
    "## Joining All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1fe76c11-fb64-4b12-9a37-8efccf76d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Joining all features and selecting final features for regression modeling...\n",
      "âœ… Final modeling dataset created with 16 records\n",
      "ðŸ“‹ Sample of final modeling dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:28:37 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+------------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+-------------+------------------+-----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+-------------+------------------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|VITAL_220045_AVG  |VITAL_220277_AVG|VITAL_220210_AVG  |VITAL_220181_AVG |VITAL_220179_AVG |VITAL_220180_AVG |VITAL_211_AVG     |VITAL_742_AVG|VITAL_618_AVG     |VITAL_646_AVG    |VITAL_223901_AVG|VITAL_220739_AVG|VITAL_223900_AVG|VITAL_220052_AVG |VITAL_220050_AVG |VITAL_220051_AVG |VITAL_223753_AVG|VITAL_8441_AVG   |VITAL_455_AVG     |VITAL_456_AVG    |LAB_51221_AVG     |LAB_50983_AVG     |LAB_50971_AVG     |LAB_51265_AVG     |LAB_51222_AVG     |LAB_51301_AVG|LAB_51250_AVG    |LAB_51248_AVG     |LAB_51249_AVG     |LAB_51279_AVG     |LAB_51277_AVG     |LAB_50912_AVG     |LAB_51006_AVG     |LAB_50902_AVG     |LAB_50882_AVG     |LAB_50868_AVG|LAB_50931_AVG    |LAB_50960_AVG     |LAB_50893_AVG|LAB_51275_AVG     |TOTAL_DIAGNOSES|HAS_4019|HAS_2724|HAS_42731|HAS_0389|HAS_5849|HAS_25000|HAS_2449|HAS_2851|HAS_486|HAS_496|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+------------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+-------------+------------------+-----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+-------------+------------------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "|0.9792      |30                  |0            |0           |0            |1                     |4                |1                 |0               |NULL              |NULL            |NULL              |NULL             |NULL             |NULL             |89.36             |1.0          |16.64             |100.0            |NULL            |NULL            |NULL            |NULL             |NULL             |NULL             |NULL            |NULL             |NULL              |NULL             |28.080000000000002|137.25            |3.5799999999999996|119.8             |9.860000000000001 |11.28        |86.8             |30.32             |35.1              |3.25              |17.26             |0.325             |4.0               |106.5             |24.25             |10.0         |85.33333333333333|1.425             |7.55         |32.480000000000004|2              |0       |0       |0        |0       |0       |0        |0       |1       |0      |0      |\n",
      "|4.05        |47                  |0            |1           |1            |1                     |1                |1                 |0               |NULL              |NULL            |NULL              |NULL             |NULL             |NULL             |106.26923076923077|1.0          |17.115384615384617|99.46153846153847|NULL            |NULL            |NULL            |NULL             |NULL             |NULL             |NULL            |82.11538461538461|135.03846153846155|99.75636907724234|34.925            |136.0             |5.15              |251.0             |12.4              |22.1         |85.0             |29.5              |34.6              |4.22              |15.7              |0.9               |24.0              |101.0             |25.0              |15.0         |94.0             |2.3               |9.2          |20.15             |7              |0       |0       |0        |0       |0       |0        |0       |1       |0      |1      |\n",
      "|0.758       |56                  |0            |0           |0            |1                     |1                |1                 |1               |NULL              |NULL            |NULL              |NULL             |NULL             |NULL             |74.58823529411765 |1.0          |16.41176470588235 |97.70588235294117|NULL            |NULL            |NULL            |NULL             |NULL             |NULL             |NULL            |57.11764705882353|94.6470588235294  |65.70588235294117|25.53333333333333 |141.33333333333334|4.9               |33.666666666666664|8.966666666666667 |2.5          |88.33333333333333|31.066666666666663|35.166666666666664|2.9166666666666665|20.566666666666666|0.8666666666666667|24.666666666666668|111.33333333333333|20.666666666666668|14.0         |101.0            |3.3499999999999996|7.4          |37.4              |9              |0       |0       |0        |0       |0       |0        |0       |0       |0      |0      |\n",
      "|1.0576      |51                  |1            |0           |0            |1                     |1                |3                 |0               |81.125            |91.0            |18.434782608695652|64.26315789473684|99.81818181818181|56.42857142857143|NULL              |NULL         |NULL              |NULL             |6.0             |4.0             |5.0             |NULL             |NULL             |NULL             |NULL            |NULL             |NULL              |NULL             |36.900000000000006|137.0             |4.35              |268.5             |12.15             |11.5         |92.5             |30.299999999999997|32.9              |3.995             |15.649999999999999|3.5               |65.5              |95.0              |25.5              |21.0         |132.0            |2.4               |8.7          |NULL              |14             |1       |0       |0        |0       |1       |0        |0       |0       |1      |0      |\n",
      "|1.6863      |29                  |1            |0           |0            |1                     |1                |1                 |0               |101.48936170212765|80.8780487804878|29.729166666666668|75.2258064516129 |131.0344827586207|60.03448275862069|NULL              |NULL         |NULL              |NULL             |1.0             |1.0             |1.0             |74.54166666666667|99.58333333333333|64.20833333333333|1.0             |NULL             |NULL              |NULL             |42.775000000000006|147.6             |5.833333333333333 |200.0             |15.274999999999999|18.5         |90.0             |32.125            |35.675            |4.755             |14.0              |3.4               |26.8              |103.6             |14.6              |35.6         |111.2            |2.12              |6.2          |69.08333333333333 |12             |0       |0       |0        |1       |1       |0        |0       |0       |0      |0      |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+------------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+-------------+------------------+-----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+-----------------+------------------+-------------+------------------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Joining all features and selecting final features for regression modeling...\")\n",
    "\n",
    "# Define feature columns to exclude\n",
    "exclude_columns = {\"ICUSTAY_ID\", \"HADM_ID\", \"SUBJECT_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"}\n",
    "\n",
    "# Join all features and immediately select desired columns\n",
    "modeling_dataset = base_icu_df \\\n",
    "    .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "    .select(*[name for name in base_icu_df \\\n",
    "        .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n",
    "        .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n",
    "        .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "        .columns if name not in exclude_columns])\n",
    "\n",
    "# Cleanup\n",
    "base_icu_df.unpersist()\n",
    "vitals_features_top20.unpersist()\n",
    "labs_features.unpersist()\n",
    "diagnosis_features.unpersist()\n",
    "\n",
    "# Display final info\n",
    "print(f\"âœ… Final modeling dataset created with {modeling_dataset.count()} records\")\n",
    "print(\"ðŸ“‹ Sample of final modeling dataset:\")\n",
    "modeling_dataset.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3dd414",
   "metadata": {},
   "source": [
    "we chose min max std beacause -1 will be corresponding to missing values. We only aplied this to float columns since others are binary or int(in case of age), final results have a max of 3 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2414bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Filling NULL entries with -1...\n",
      "ðŸ“Š Computing min-max scaling in _AVG columns, excluding -1 entries...\n",
      "âœ… Data set ready for Machine Learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:29:27 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|VITAL_220045_AVG|VITAL_220277_AVG|VITAL_220210_AVG|VITAL_220181_AVG|VITAL_220179_AVG|VITAL_220180_AVG|VITAL_211_AVG|VITAL_742_AVG|VITAL_618_AVG|VITAL_646_AVG|VITAL_223901_AVG|VITAL_220739_AVG|VITAL_223900_AVG|VITAL_220052_AVG|VITAL_220050_AVG|VITAL_220051_AVG|VITAL_223753_AVG|VITAL_8441_AVG|VITAL_455_AVG|VITAL_456_AVG|LAB_51221_AVG|LAB_50983_AVG|LAB_50971_AVG|LAB_51265_AVG|LAB_51222_AVG|LAB_51301_AVG|LAB_51250_AVG|LAB_51248_AVG|LAB_51249_AVG|LAB_51279_AVG|LAB_51277_AVG|LAB_50912_AVG|LAB_51006_AVG|LAB_50902_AVG|LAB_50882_AVG|LAB_50868_AVG|LAB_50931_AVG|LAB_50960_AVG|LAB_50893_AVG|LAB_51275_AVG|TOTAL_DIAGNOSES|HAS_4019|HAS_2724|HAS_42731|HAS_0389|HAS_5849|HAS_25000|HAS_2449|HAS_2851|HAS_486|HAS_496|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "|1.2488      |69                  |1            |0           |1            |2                     |1                |3                 |0               |0.36045         |0.92166         |0.16936         |-1.0            |-1.0            |-1.0            |-1.0         |-1.0         |-1.0         |-1.0         |0.8             |0.8             |0.8             |0.21421         |0.29071         |0.49351         |-1.0            |-1.0          |-1.0         |-1.0         |0.4898       |0.44118      |0.90737      |0.24784      |0.40924      |0.44046      |0.22222      |0.11789      |0.30228      |0.66044      |0.50826      |0.32808      |0.27642      |0.88235      |0.57711      |0.10025      |0.10369      |0.55844      |0.87097      |0.12568      |12             |1       |1       |1        |0       |0       |1        |0       |0       |0      |0      |\n",
      "|3.968       |66                  |1            |0           |1            |1                     |1                |2                 |0               |0.73934         |0.81105         |0.39125         |0.5806          |0.28312         |0.72395         |-1.0         |-1.0         |-1.0         |-1.0         |1.0             |1.0             |1.0             |-1.0            |-1.0            |-1.0            |-1.0            |-1.0          |-1.0         |-1.0         |0.38367      |0.44118      |0.25236      |0.68319      |0.39604      |0.19799      |0.85417      |0.87805      |0.58385      |0.28037      |0.06818      |0.25984      |0.17886      |0.26471      |0.96269      |0.24436      |0.33458      |0.55844      |0.67742      |0.34911      |15             |0       |1       |0        |0       |1       |0        |1       |0       |0      |0      |\n",
      "|0.9792      |30                  |0            |0           |0            |1                     |4                |1                 |0               |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |0.46627      |0.0          |0.32437      |1.0          |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0            |-1.0          |-1.0         |-1.0         |0.20027      |0.36765      |0.10548      |0.27845      |0.28515      |0.37791      |0.45         |0.57561      |0.85714      |0.2757       |0.59008      |0.0          |0.0          |0.67647      |0.72015      |0.03759      |0.0          |0.0          |0.43548      |0.25198      |2              |0       |0       |0        |0       |0       |0        |0       |1       |0      |0      |\n",
      "|1.2198      |52                  |1            |0           |0            |2                     |1                |3                 |0               |0.71628         |0.94958         |0.35583         |0.43926         |0.22915         |0.56661         |-1.0         |-1.0         |-1.0         |-1.0         |0.66667         |0.55556         |0.5             |0.26539         |0.11342         |0.72543         |-1.0            |-1.0          |-1.0         |-1.0         |0.36774      |0.41176      |0.33176      |0.55         |0.38614      |0.54448      |0.42708      |0.42683      |0.52174      |0.46963      |0.16426      |0.10236      |0.13008      |0.76471      |0.77612      |0.0          |0.14538      |-1.0         |-1.0         |0.3234       |12             |1       |1       |1        |0       |0       |0        |0       |1       |0      |0      |\n",
      "|2.2913      |78                  |1            |0           |1            |1                     |1                |3                 |0               |0.50532         |1.0             |0.23277         |0.0             |0.0             |0.0             |-1.0         |-1.0         |-1.0         |-1.0         |1.0             |1.0             |0.95            |0.0             |0.22606         |0.0             |0.88312         |-1.0          |-1.0         |-1.0         |0.01814      |0.0          |0.431        |0.6153       |0.07261      |0.13773      |0.64583      |0.41463      |0.0          |0.08411      |0.27273      |0.07087      |0.21951      |0.23529      |0.92537      |0.03759      |0.08765      |-1.0         |-1.0         |0.26873      |13             |0       |0       |1        |0       |0       |0        |0       |0       |0      |0      |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+------------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+-------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+---------------+--------+--------+---------+--------+--------+---------+--------+--------+-------+-------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Filling NULL entries with -1...\")\n",
    "std_columns = [c for c in modeling_dataset.columns if c.endswith('_AVG')]\n",
    "\n",
    "modeling_dataset = modeling_dataset.na.fill(-1)\n",
    "\n",
    "print(\"ðŸ“Š Computing min-max scaling in _AVG columns, excluding -1 entries...\")\n",
    "min_max_values = {}\n",
    "for col_name in std_columns:\n",
    "    stats = modeling_dataset.filter(col(col_name) != -1.0).agg(\n",
    "        spark_min(col(col_name)).alias(\"min\"),\n",
    "        spark_max(col(col_name)).alias(\"max\")\n",
    "    ).first()\n",
    "    min_max_values[col_name] = (stats[\"min\"], stats[\"max\"])\n",
    "\n",
    "for col_name in std_columns:\n",
    "    min_val, max_val = min_max_values[col_name]\n",
    "    range_val = max_val - min_val if max_val != min_val else 1.0\n",
    "    modeling_dataset = modeling_dataset.withColumn(\n",
    "        col_name, \n",
    "        when(col(col_name) == -1.0, -1.0).otherwise(\n",
    "            round((col(col_name) - min_val) / range_val, 5)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"âœ… Data set ready for Machine Learning!\")\n",
    "modeling_dataset.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31c2db",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f975-668b-4ab3-a228-4af82187778e",
   "metadata": {},
   "source": [
    "## Preparing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50fa7d59-2d01-4dcf-868f-900431b44be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 1: Creating train/test split...\n",
      "âœ… Data split completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:29:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸš† Training samples: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:29:54 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ§ª Test samples: 3\n",
      "Feature columns: ['AGE_AT_ICU_ADMISSION', 'GENDER_BINARY', 'CAME_FROM_ER', 'HAS_INSURANCE', 'ADMISSION_TYPE_ENCODED', 'ETHNICITY_ENCODED', 'FIRST_UNIT_ENCODED', 'CHANGED_ICU_UNIT', 'VITAL_220045_AVG', 'VITAL_220277_AVG', 'VITAL_220210_AVG', 'VITAL_220181_AVG', 'VITAL_220179_AVG', 'VITAL_220180_AVG', 'VITAL_211_AVG', 'VITAL_742_AVG', 'VITAL_618_AVG', 'VITAL_646_AVG', 'VITAL_223901_AVG', 'VITAL_220739_AVG', 'VITAL_223900_AVG', 'VITAL_220052_AVG', 'VITAL_220050_AVG', 'VITAL_220051_AVG', 'VITAL_223753_AVG', 'VITAL_8441_AVG', 'VITAL_455_AVG', 'VITAL_456_AVG', 'LAB_51221_AVG', 'LAB_50983_AVG', 'LAB_50971_AVG', 'LAB_51265_AVG', 'LAB_51222_AVG', 'LAB_51301_AVG', 'LAB_51250_AVG', 'LAB_51248_AVG', 'LAB_51249_AVG', 'LAB_51279_AVG', 'LAB_51277_AVG', 'LAB_50912_AVG', 'LAB_51006_AVG', 'LAB_50902_AVG', 'LAB_50882_AVG', 'LAB_50868_AVG', 'LAB_50931_AVG', 'LAB_50960_AVG', 'LAB_50893_AVG', 'LAB_51275_AVG', 'TOTAL_DIAGNOSES', 'HAS_4019', 'HAS_2724', 'HAS_42731', 'HAS_0389', 'HAS_5849', 'HAS_25000', 'HAS_2449', 'HAS_2851', 'HAS_486', 'HAS_496']\n",
      "Target column: ICU_LOS_DAYS\n",
      "ðŸ“Š Step 2: Creating the final vectorized train/test datasets...\n",
      "âœ… Final datasets prepared:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:30:08 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:10 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸš† Training features shape: (12, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:30:23 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:24 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "[Stage 40091:===========================================>         (26 + 6) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ§ª Test features shape: (4, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 1: Creating train/test split...\")\n",
    "train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "print(\"âœ… Data split completed.\")\n",
    "print(f\"   ðŸš† Training samples: {train_data.count()}\")\n",
    "print(f\"   ðŸ§ª Test samples: {test_data.count()}\")\n",
    "\n",
    "\n",
    "feature_columns = [col for col in modeling_dataset.columns if col != 'ICU_LOS_DAYS']\n",
    "print(\"Feature columns:\", feature_columns)\n",
    "target_column = 'ICU_LOS_DAYS'\n",
    "print(\"Target column:\", target_column)\n",
    "\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,  \n",
    "    outputCol=\"features\"     \n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Step 2: Creating the final vectorized train/test datasets...\")\n",
    "train_final = feature_assembler.transform(train_data).select(\n",
    "    \"features\", \n",
    "    target_column\n",
    ").withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "test_final = feature_assembler.transform(test_data).select(\n",
    "    \"features\", \n",
    "    target_column\n",
    ").withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "train_final.cache()\n",
    "test_final.cache()\n",
    "\n",
    "print(\"âœ… Final datasets prepared:\")\n",
    "print(f\"   ðŸš† Training features shape: ({train_final.count()}, {len(feature_columns)})\")\n",
    "print(f\"   ðŸ§ª Test features shape: ({test_final.count()}, {len(feature_columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c98beb-ae1b-445a-aa7f-9002b4d9a012",
   "metadata": {},
   "source": [
    "## Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eda7d1c4-c1ef-4261-a3b5-e05db415626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 1: Setting up evaluation metrics...\n",
      "âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Step 1: Setting up evaluation metrics...\")\n",
    "\n",
    "# Create regression evaluators\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f74323-98d0-44fd-b21f-f2e543449457",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a25b20c-1f60-4782-b27c-6fda0121af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Step 2: Training Linear Regression model...\n",
      "ðŸ• Started at: 20:30:26\n",
      "   ðŸ”„ Training Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:30:27 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:29 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:30 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:31 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ”„ Linear Regression - Making predictions (test data)...\n",
      "   ðŸ”„ Linear Regression - Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:30:32 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:33 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:34 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:35 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:36 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:37 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Linear Regression Results:\n",
      "   ðŸ“‰ RMSE: 4.749 days\n",
      "   ðŸ“Š MAE: 3.294 days\n",
      "   ðŸ“ˆ RÂ²: -1.034\n",
      "ðŸ• Completed at: 20:30:38\n",
      "â±ï¸ Total elapsed time: 11.94 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“ˆ Step 2: Training Linear Regression model...\")\n",
    "print(f\"ðŸ• Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=200,                    # Increased for better convergence\n",
    "    regParam=0.001,                 # Lower regularization for healthcare data\n",
    "    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n",
    "    tol=1e-8,                       # Tighter tolerance for precision\n",
    "    standardization=False,          # We're doing manual scaling\n",
    "    fitIntercept=True,\n",
    "    aggregationDepth=3,             # Better for distributed training\n",
    "    loss=\"squaredError\",\n",
    "    solver=\"normal\"                 # Best for small-medium datasets\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"   ðŸ”„ Training Linear Regression...\")\n",
    "lr_model = lr.fit(train_final)\n",
    "\n",
    "print(\"   ðŸ”„ Linear Regression - Making predictions (test data)...\")\n",
    "lr_predictions = lr_model.transform(test_final)\n",
    "\n",
    "print(\"   ðŸ”„ Linear Regression - Evaluation...\")\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"âœ… Linear Regression Results:\")\n",
    "print(f\"   ðŸ“‰ RMSE: {lr_rmse:.3f} days\")\n",
    "print(f\"   ðŸ“Š MAE: {lr_mae:.3f} days\")\n",
    "print(f\"   ðŸ“ˆ RÂ²: {lr_r2:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"ðŸ• Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"â±ï¸ Total elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847f98b-4da8-41ff-b26f-f2943c8baa38",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52707ea4-eb6a-449d-8160-7013057b2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ² Step 3: Training Random Forest model...\n",
      "ðŸ• Started at: 20:30:38\n",
      "   ðŸ”„ Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:30:39 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:39 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:40 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:41 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 12 (= number of training instances)\n",
      "25/06/03 20:30:42 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:43 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/06/03 20:30:44 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/06/03 20:30:46 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/06/03 20:30:47 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/06/03 20:30:49 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/06/03 20:30:50 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ”„ Random Forest - Making predictions (test data)...\n",
      "   ðŸ”„ Random Forest - Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:30:52 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:53 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:54 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:56 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:57 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/03 20:30:58 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random Forest Results:\n",
      "   ðŸ“‰ RMSE: 3.626 days\n",
      "   ðŸ“Š MAE: 2.322 days\n",
      "   ðŸ“ˆ RÂ²: -0.186\n",
      "ðŸ• Completed at: 20:30:58\n",
      "â±ï¸ Total elapsed time: 20.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nðŸŒ² Step 3: Training Random Forest model...\")\n",
    "print(f\"ðŸ• Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=200,                   # More trees = better accuracy (if enough cores/memory)\n",
    "    maxDepth=12,                    # Deeper trees capture more complexity\n",
    "    minInstancesPerNode=2,          # Allows more granular splits\n",
    "    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n",
    "    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n",
    "    seed=42                         # Reproducibility\n",
    ")\n",
    "\n",
    "print(\"   ðŸ”„ Training Random Forest...\")\n",
    "rf_model = rf.fit(train_final)\n",
    "\n",
    "print(\"   ðŸ”„ Random Forest - Making predictions (test data)...\")\n",
    "rf_predictions = rf_model.transform(test_final)\n",
    "\n",
    "print(\"   ðŸ”„ Random Forest - Evaluation...\")\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = mae_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"âœ… Random Forest Results:\")\n",
    "print(f\"   ðŸ“‰ RMSE: {rf_rmse:.3f} days\")\n",
    "print(f\"   ðŸ“Š MAE: {rf_mae:.3f} days\")\n",
    "print(f\"   ðŸ“ˆ RÂ²: {rf_r2:.3f}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"ðŸ• Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"â±ï¸ Total elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0d610-37c0-4c99-9fc0-289846b54033",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e3ff000d-87eb-4b26-976a-65db81b057f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† Step 5: Model Performance Comparison...\n",
      "ðŸ“Š Model Performance Summary:\n",
      "+-----------------+------------------+------------------+--------------------+\n",
      "|Model            |RMSE              |MAE               |R2                  |\n",
      "+-----------------+------------------+------------------+--------------------+\n",
      "|Linear Regression|4.7491543997079635|3.2943815137610257|-1.0344336738649211 |\n",
      "|Random Forest    |3.62616497875568  |2.3215394202380955|-0.18605840569008691|\n",
      "+-----------------+------------------+------------------+--------------------+\n",
      "\n",
      "\n",
      "ðŸ¥‡ Best Models:\n",
      "   ðŸŽ¯ Lowest RMSE: Random Forest (3.626 days)\n",
      "   ðŸ“ˆ Highest RÂ²: Random Forest (-0.186)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ† Step 5: Model Performance Comparison...\")\n",
    "\n",
    "# Create comparison summary\n",
    "results_data = [\n",
    "    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n",
    "    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n",
    "]\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Summary:\")\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Find best model\n",
    "import operator\n",
    "import builtins\n",
    "best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n",
    "best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n",
    "\n",
    "print(f\"\\nðŸ¥‡ Best Models:\")\n",
    "print(f\"   ðŸŽ¯ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n",
    "print(f\"   ðŸ“ˆ Highest RÂ²: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e166a-1c18-4e22-938c-d6a5569bd296",
   "metadata": {},
   "source": [
    "## Display Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "068fa69e-a7fb-40f2-a287-6300bf41e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Linear Regression Predictions (Sample 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:31:00 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------+-------------+\n",
      "|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+-------------+--------------+-------------+\n",
      "|0.758     |2.869        |2.111         |278.45       |\n",
      "|1.2597    |2.166        |0.907         |71.98        |\n",
      "|8.9163    |-0.246       |9.162         |102.76       |\n",
      "|1.8064    |2.804        |0.998         |55.25        |\n",
      "+----------+-------------+--------------+-------------+\n",
      "\n",
      "\n",
      "ðŸŒ² Random Forest Predictions (Sample 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/03 20:31:02 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------+-------------+\n",
      "|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+-------------+--------------+-------------+\n",
      "|0.758     |1.958        |1.2           |158.26       |\n",
      "|1.2597    |2.004        |0.745         |59.12        |\n",
      "|8.9163    |1.807        |7.11          |79.74        |\n",
      "|1.8064    |2.038        |0.232         |12.85        |\n",
      "+----------+-------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“ˆ Linear Regression Predictions (Sample 20):\")\n",
    "lr_display = lr_predictions.select(\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ")\n",
    "\n",
    "lr_display.show(20, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Predictions\n",
    "print(\"\\nðŸŒ² Random Forest Predictions (Sample 20):\")\n",
    "rf_display = rf_predictions.select(\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ")\n",
    "\n",
    "rf_display.show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
