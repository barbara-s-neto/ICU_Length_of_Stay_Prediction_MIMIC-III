{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c8375f-7f35-415f-8288-2ff2193e6af0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ed6638-09bf-4620-89fe-2bb2c00d86e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports loaded successfully!\n",
      "â° Notebook started at: 2025-05-31 16:46:34\n"
     ]
    }
   ],
   "source": [
    "# Core PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Machine Learning imports\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"âœ… All imports loaded successfully!\")\n",
    "print(f\"â° Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1810-67b2-471e-97d2-b8fda7db9728",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a15f1fca-5bf8-4e10-bdca-a6faa11ca17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session created successfully!\n",
      "ğŸ“Š Spark Version: 3.5.5\n",
      "ğŸ”§ Application Name: Forecast-LOS\n",
      "ğŸ’¾ Available cores: 4\n",
      "\n",
      "â° Spark session initialised at: 2025-05-31 16:52:11\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Forecast-LOS\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark session created successfully!\")\n",
    "print(f\"ğŸ“Š Spark Version: {spark.version}\")\n",
    "print(f\"ğŸ”§ Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"ğŸ’¾ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\nâ° Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13163e-55dc-4046-95b0-4815723d0581",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62936c54-45ab-4c03-a8ec-fc3d87f68721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Loading MIMIC-III CSV files...\n",
      "ğŸ“‚ Loading ICUSTAYS table...\n",
      "ğŸ“‚ Loading PATIENTS table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading ADMISSIONS table...\n",
      "ğŸ“‚ Loading CHARTEVENTS table...\n",
      "ğŸ“‚ Loading INPUTEVENTS_MV table...\n",
      "ğŸ“‚ Loading DIAGNOSES_ICD table...\n",
      "ğŸ“‚ Loading LABEVENTS table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/31 17:24:14 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/31 17:24:14 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/31 17:24:14 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/31 17:24:14 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Tables loaded successfully!\n",
      "ğŸ“Š ICUSTAYS: 20 rows Ã— 12 columns\n",
      "ğŸ“Š PATIENTS: 46,520 rows Ã— 8 columns\n",
      "ğŸ“Š ADMISSIONS: 20 rows Ã— 19 columns\n",
      "\n",
      "â° Data loaded at: 2025-05-31 17:24:15\n"
     ]
    }
   ],
   "source": [
    "MIMIC_PATH = \"./mimic-db-short\"\n",
    "\n",
    "print(\"ğŸ¥ Loading MIMIC-III CSV files...\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading ICUSTAYS table...\")\n",
    "icustays_df = spark.read .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading PATIENTS table...\")\n",
    "patients_df = spark.read .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading CHARTEVENTS table...\")\n",
    "chartevents_df = spark.read .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading INPUTEVENTS_MV table...\")\n",
    "inputevents_df = spark.read .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/INPUTEVENTS_MV.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "\n",
    "print(\"ğŸ“‚ Loading LABEVENTS table...\")\n",
    "labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "\n",
    "\n",
    "# Cache the core tables for better performance\n",
    "icustays_df.cache()\n",
    "patients_df.cache() \n",
    "admissions_df.cache()\n",
    "inputevents_df.cache()\n",
    "labevents_df.cache()\n",
    "# Display basic information about loaded tables\n",
    "print(\"\\nâœ… Tables loaded successfully!\")\n",
    "print(f\"ğŸ“Š ICUSTAYS: {icustays_df.count():,} rows Ã— {len(icustays_df.columns)} columns\")\n",
    "print(f\"ğŸ“Š PATIENTS: {patients_df.count():,} rows Ã— {len(patients_df.columns)} columns\") \n",
    "print(f\"ğŸ“Š ADMISSIONS: {admissions_df.count():,} rows Ã— {len(admissions_df.columns)} columns\")\n",
    "\n",
    "print(f\"\\nâ° Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e7157-1230-41d3-8f41-1117b62fdb55",
   "metadata": {},
   "source": [
    "## Features Engineering\n",
    "\n",
    "Current features for regression:\n",
    "\n",
    "- Demographics (age, gender)\n",
    "- Admission characteristics (emergency vs elective, timing)\n",
    "- ICU unit types and transfers\n",
    "- Time-based features (weekend, night admissions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763e6d7-4ba5-439b-8e2b-ba16cf607a3e",
   "metadata": {},
   "source": [
    "## Extracting Data From ICUSTAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1d7adb-eac0-4071-b252-a04268f0c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 1: Creating base ICU dataset with patient demographics...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 1: Creating base ICU dataset with patient demographics...\")\n",
    "\n",
    "base_icu_df = icustays_df.alias(\"icu\") \\\n",
    "    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n",
    "    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        # ICU stay identifiers\n",
    "        col(\"icu.ICUSTAY_ID\"),\n",
    "        col(\"icu.SUBJECT_ID\"), \n",
    "        col(\"icu.HADM_ID\"),\n",
    "        \n",
    "        # Target variable - Length of Stay in ICU (days)\n",
    "        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n",
    "        \n",
    "        # ICU characteristics\n",
    "        col(\"icu.FIRST_CAREUNIT\"),\n",
    "        col(\"icu.LAST_CAREUNIT\"), \n",
    "        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n",
    "        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n",
    "        \n",
    "        # Patient demographics\n",
    "        col(\"pat.GENDER\"),\n",
    "        col(\"pat.DOB\"),\n",
    "        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n",
    "        \n",
    "        # Admission details\n",
    "        col(\"adm.ADMITTIME\"),\n",
    "        col(\"adm.DISCHTIME\"), \n",
    "        col(\"adm.ADMISSION_TYPE\"),\n",
    "        col(\"adm.ADMISSION_LOCATION\"),\n",
    "        col(\"adm.INSURANCE\"),\n",
    "        col(\"adm.ETHNICITY\"),\n",
    "        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n",
    "        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n",
    "    )\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "base_icu_df = base_icu_df.withColumn(\n",
    "    \"AGE_AT_ICU_ADMISSION\", \n",
    "    floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595638e6-74f7-4f2f-a7e5-1fc692089206",
   "metadata": {},
   "source": [
    "## Extracting Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28d34e6-83c3-40ae-95d0-71f9929397a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 2: Engineering categorical features...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 2: Engineering categorical features...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"IS_EMERGENCY_ADMISSION\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"IS_ELECTIVE_ADMISSION\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CAME_FROM_ER\", \n",
    "                when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_MEDICARE\", \n",
    "                when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"IS_WHITE_ETHNICITY\", \n",
    "                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca22bc8-8dd5-4d30-9cb9-0f366da21d76",
   "metadata": {},
   "source": [
    "## Extracting ICU Unit Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6617c3f2-75b9-4fa0-93a8-7161d0c2dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 3: Creating ICU unit type features...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 3: Creating ICU unit type features...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"FIRST_UNIT_MICU\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"FIRST_UNIT_SICU\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"SICU\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"FIRST_UNIT_CSRU\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"FIRST_UNIT_CCU\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"CCU\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"FIRST_UNIT_TSICU\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CHANGED_ICU_UNIT\", \n",
    "                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74662ab6-f263-402e-913f-dc04804eadff",
   "metadata": {},
   "source": [
    "## Extracting Time-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef431386-85f1-4867-9aa1-f5bb84d7c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 4: Creating time-based features...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 4: Creating time-based features...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"ADMISSION_TO_ICU_HOURS\", \n",
    "                (unix_timestamp(\"ICU_INTIME\") - unix_timestamp(\"ADMITTIME\")) / 3600) \\\n",
    "    .withColumn(\"ICU_LOS_HOURS\", col(\"ICU_LOS_DAYS\") * 24) \\\n",
    "    .withColumn(\"WEEKEND_ADMISSION\", \n",
    "                when(dayofweek(\"ICU_INTIME\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "    .withColumn(\"NIGHT_ADMISSION\", \n",
    "                when(hour(\"ICU_INTIME\").between(20, 7), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ac96b-6e5f-4fdd-84a2-043a0beaa06b",
   "metadata": {},
   "source": [
    "## Remove Outliers (Excessive Length Of Stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6ce0e40-182e-4fbd-bb35-2bb23ed0d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 5: Cleaning target variable...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ICUSTAY_ID: int, SUBJECT_ID: int, HADM_ID: int, ICU_LOS_DAYS: double, FIRST_CAREUNIT: string, LAST_CAREUNIT: string, ICU_INTIME: timestamp, ICU_OUTTIME: timestamp, GENDER: string, DOB: timestamp, PATIENT_DIED: int, ADMITTIME: timestamp, DISCHTIME: timestamp, ADMISSION_TYPE: string, ADMISSION_LOCATION: string, INSURANCE: string, ETHNICITY: string, HOSPITAL_DEATH: int, ADMISSION_DIAGNOSIS: string, AGE_AT_ICU_ADMISSION: bigint, GENDER_BINARY: int, IS_EMERGENCY_ADMISSION: int, IS_ELECTIVE_ADMISSION: int, CAME_FROM_ER: int, HAS_MEDICARE: int, IS_WHITE_ETHNICITY: int, FIRST_UNIT_MICU: int, FIRST_UNIT_SICU: int, FIRST_UNIT_CSRU: int, FIRST_UNIT_CCU: int, FIRST_UNIT_TSICU: int, CHANGED_ICU_UNIT: int, ADMISSION_TO_ICU_HOURS: double, ICU_LOS_HOURS: double, WEEKEND_ADMISSION: int, NIGHT_ADMISSION: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 5: Cleaning target variable...\")\n",
    "\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n",
    "    .filter(col(\"ICU_LOS_DAYS\") < 30)  # Remove extreme outliers (>30 days)\n",
    "\n",
    "base_icu_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1aa14-d750-44e7-9b44-c0f37dc4dfaa",
   "metadata": {},
   "source": [
    "## Show Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d6cadf4-d2ab-4242-808d-73575be75130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Master ICU dataset created!\n",
      "ğŸ“ Dataset size: 20 ICU stays\n",
      "ğŸ“Š Features created: 36 columns\n",
      "\n",
      "ğŸ“‹ Sample of regression features:\n",
      "+----------+--------------------+-------------+------------+--------------+----------------------+----------------------+\n",
      "|ICUSTAY_ID|AGE_AT_ICU_ADMISSION|GENDER_BINARY|ICU_LOS_DAYS|FIRST_CAREUNIT|IS_EMERGENCY_ADMISSION|ADMISSION_TO_ICU_HOURS|\n",
      "+----------+--------------------+-------------+------------+--------------+----------------------+----------------------+\n",
      "|    222139|                   0|            0|      0.0905|          NICU|                     0|   0.09194444444444444|\n",
      "|    255819|                  56|            0|       0.758|          MICU|                     1|     4.977777777777778|\n",
      "|    231977|                  30|            0|      0.9792|          MICU|                     1|     69.51611111111112|\n",
      "|    264061|                  51|            1|      1.0576|          CSRU|                     1|  0.016944444444444446|\n",
      "|    259139|                 299|            0|      4.9672|         TSICU|                     1|  0.015833333333333335|\n",
      "+----------+--------------------+-------------+------------+--------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ğŸ“ˆ ICU Length of Stay Statistics:\n",
      "+-------+------------------+\n",
      "|summary|      ICU_LOS_DAYS|\n",
      "+-------+------------------+\n",
      "|  count|                20|\n",
      "|   mean|2.3272399999999998|\n",
      "| stddev|2.0517961344303086|\n",
      "|    min|            0.0905|\n",
      "|    max|            8.9163|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "â° Feature engineering completed at: 2025-05-31 17:03:46\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… Master ICU dataset created!\")\n",
    "print(f\"ğŸ“ Dataset size: {base_icu_df.count():,} ICU stays\")\n",
    "print(f\"ğŸ“Š Features created: {len(base_icu_df.columns)} columns\")\n",
    "\n",
    "# Display sample of the dataset\n",
    "print(\"\\nğŸ“‹ Sample of regression features:\")\n",
    "base_icu_df.select(\n",
    "    \"ICUSTAY_ID\", \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\", \"ICU_LOS_DAYS\", \n",
    "    \"FIRST_CAREUNIT\", \"IS_EMERGENCY_ADMISSION\", \"ADMISSION_TO_ICU_HOURS\"\n",
    ").show(5)\n",
    "\n",
    "# Show basic statistics of target variable\n",
    "print(\"\\nğŸ“ˆ ICU Length of Stay Statistics:\")\n",
    "base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "print(f\"\\nâ° Feature engineering completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268129f5-72e2-40a5-b676-cbd825ae84c8",
   "metadata": {},
   "source": [
    "## Extracting Clinical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc92782-4f16-4c82-bd8a-44ed78e92965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ«€ Step 1: Creating vital signs features from CHARTEVENTS...\n",
      "   ğŸ“Š Calculating vital signs statistics (first 24h)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Vital signs features created for 14 ICU stays\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ«€ Step 1: Creating vital signs features from CHARTEVENTS...\")\n",
    "\n",
    "# Key vital signs ITEMID mappings (common across MIMIC-III)\n",
    "vital_signs_items = {\n",
    "    220045: \"HEART_RATE\",      # Heart Rate\n",
    "    220050: \"SBP\",             # Systolic BP  \n",
    "    220051: \"DBP\",             # Diastolic BP\n",
    "    220210: \"RESP_RATE\",       # Respiratory Rate\n",
    "    223762: \"TEMPERATURE\",     # Temperature Celsius\n",
    "    220277: \"SPO2\"             # Oxygen Saturation\n",
    "}\n",
    "\n",
    "# Filter chartevents for vital signs within first 24 hours of ICU stay\n",
    "vitals_24h = chartevents_df.alias(\"ce\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n",
    "    .filter(col(\"ce.ITEMID\").isin(list(vital_signs_items.keys()))) \\\n",
    "    .filter(col(\"ce.VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"ce.VALUENUM\") > 0) \\\n",
    "    .filter(\n",
    "        col(\"ce.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\"), \n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create vital signs summary statistics for each ICU stay\n",
    "print(\"   ğŸ“Š Calculating vital signs statistics (first 24h)...\")\n",
    "\n",
    "vitals_stats = vitals_24h.groupBy(\"ICUSTAY_ID\", \"ITEMID\") \\\n",
    "    .agg(\n",
    "        avg(\"VALUENUM\").alias(\"avg_value\"),\n",
    "        min(\"VALUENUM\").alias(\"min_value\"), \n",
    "        max(\"VALUENUM\").alias(\"max_value\"),\n",
    "        stddev(\"VALUENUM\").alias(\"std_value\"),\n",
    "        count(\"VALUENUM\").alias(\"count_measurements\")\n",
    "    )\n",
    "\n",
    "# Pivot to get one column per vital sign statistic\n",
    "vitals_features = vitals_stats.groupBy(\"ICUSTAY_ID\").pivot(\"ITEMID\").agg(\n",
    "    first(\"avg_value\").alias(\"avg\"),\n",
    "    first(\"min_value\").alias(\"min\"),\n",
    "    first(\"max_value\").alias(\"max\"),\n",
    "    first(\"std_value\").alias(\"std\")\n",
    ")\n",
    "\n",
    "# Rename columns to meaningful names\n",
    "for itemid, name in vital_signs_items.items():\n",
    "    vitals_features = vitals_features \\\n",
    "        .withColumnRenamed(f\"{itemid}_avg\", f\"{name}_AVG\") \\\n",
    "        .withColumnRenamed(f\"{itemid}_min\", f\"{name}_MIN\") \\\n",
    "        .withColumnRenamed(f\"{itemid}_max\", f\"{name}_MAX\") \\\n",
    "        .withColumnRenamed(f\"{itemid}_std\", f\"{name}_STD\")\n",
    "\n",
    "print(f\"   âœ… Vital signs features created for {vitals_features.count():,} ICU stays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b685f0e-f499-4adf-9fd9-a6f92e22a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Step 2: Creating laboratory features from LABEVENTS...\n",
      "   ğŸ“Š Calculating laboratory statistics (first 24h)...\n",
      "   âœ… Laboratory features created for 20 ICU stays\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ§ª Step 2: Creating laboratory features from LABEVENTS...\")\n",
    "\n",
    "# Key lab test ITEMID mappings\n",
    "lab_items = {\n",
    "    50912: \"CREATININE\",       # Creatinine\n",
    "    50902: \"CHLORIDE\",         # Chloride\n",
    "    50931: \"GLUCOSE\",          # Glucose\n",
    "    50983: \"SODIUM\",           # Sodium\n",
    "    50971: \"POTASSIUM\",        # Potassium\n",
    "    51222: \"HEMOGLOBIN\",       # Hemoglobin\n",
    "    51265: \"PLATELET\",         # Platelet Count\n",
    "    51301: \"WBC\",              # White Blood Cells\n",
    "    50820: \"PH\"                # pH\n",
    "}\n",
    "\n",
    "# Filter lab events within first 24 hours of ICU stay\n",
    "labs_24h = labevents_df.alias(\"le\") \\\n",
    "    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n",
    "    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n",
    "    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"le.VALUENUM\") > 0) \\\n",
    "    .filter(\n",
    "        col(\"le.CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Calculate lab value statistics\n",
    "print(\"   ğŸ“Š Calculating laboratory statistics (first 24h)...\")\n",
    "\n",
    "labs_stats = labs_24h.groupBy(\"ICUSTAY_ID\", \"ITEMID\") \\\n",
    "    .agg(\n",
    "        avg(\"VALUENUM\").alias(\"avg_value\"),\n",
    "        min(\"VALUENUM\").alias(\"min_value\"),\n",
    "        max(\"VALUENUM\").alias(\"max_value\"),\n",
    "        first(\"VALUENUM\").alias(\"first_value\")  # First available value\n",
    "    )\n",
    "\n",
    "# Pivot lab results\n",
    "labs_features = labs_stats.groupBy(\"ICUSTAY_ID\").pivot(\"ITEMID\").agg(\n",
    "    first(\"avg_value\").alias(\"avg\"),\n",
    "    first(\"first_value\").alias(\"first\")\n",
    ")\n",
    "\n",
    "# Rename lab columns\n",
    "for itemid, name in lab_items.items():\n",
    "    labs_features = labs_features \\\n",
    "        .withColumnRenamed(f\"{itemid}_avg\", f\"{name}_AVG\") \\\n",
    "        .withColumnRenamed(f\"{itemid}_first\", f\"{name}_FIRST\")\n",
    "\n",
    "print(f\"   âœ… Laboratory features created for {labs_features.count():,} ICU stays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94a414c1-7139-4e27-a5e4-767919a3eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¥ Step 3: Creating diagnosis features from ICD codes...\n",
      "   âœ… Diagnosis features created for 40 admissions\n",
      "\n",
      "â° Clinical features completed at: 2025-05-31 17:08:40\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ¥ Step 3: Creating diagnosis features from ICD codes...\")\n",
    "\n",
    "# Count number of diagnoses per admission (comorbidity burden)\n",
    "diagnosis_counts = diagnoses_df.groupBy(\"HADM_ID\") \\\n",
    "    .agg(\n",
    "        count(\"ICD9_CODE\").alias(\"TOTAL_DIAGNOSES\"),\n",
    "        collect_list(\"ICD9_CODE\").alias(\"DIAGNOSIS_CODES\")\n",
    "    )\n",
    "\n",
    "# Create features for common diagnosis categories\n",
    "diagnosis_features = diagnosis_counts \\\n",
    "    .withColumn(\"HAS_SEPSIS\", \n",
    "                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"99591\") | \n",
    "                     array_contains(col(\"DIAGNOSIS_CODES\"), \"99592\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_RESPIRATORY_FAILURE\",\n",
    "                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"51881\") |\n",
    "                     array_contains(col(\"DIAGNOSIS_CODES\"), \"51882\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_CARDIAC_ARREST\",\n",
    "                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"4275\"), 1).otherwise(0)) \\\n",
    "    .drop(\"DIAGNOSIS_CODES\")\n",
    "\n",
    "print(f\"   âœ… Diagnosis features created for {diagnosis_features.count():,} admissions\")\n",
    "\n",
    "print(f\"\\nâ° Clinical features completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d9140-230d-4bc0-91ae-fd9db043e5a5",
   "metadata": {},
   "source": [
    "# Joining All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fe76c11-fb64-4b12-9a37-8efccf76d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 1: Joining base features with clinical data...\n",
      "   ğŸ«€ Adding vital signs features...\n",
      "   ğŸ§ª Adding laboratory features...\n",
      "   ğŸ¥ Adding diagnosis features...\n",
      "âœ… All features joined! Dataset shape: 20 rows Ã— 82 columns\n",
      "\n",
      "ğŸ”§ Step 2: Handling missing values...\n",
      "âœ… Missing values handled\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 1: Joining base features with clinical data...\")\n",
    "\n",
    "# Start with base ICU dataset\n",
    "final_dataset = base_icu_df\n",
    "\n",
    "# Join vital signs features\n",
    "print(\"   ğŸ«€ Adding vital signs features...\")\n",
    "final_dataset = final_dataset.join(vitals_features, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Join laboratory features  \n",
    "print(\"   ğŸ§ª Adding laboratory features...\")\n",
    "final_dataset = final_dataset.join(labs_features, \"ICUSTAY_ID\", \"left\")\n",
    "\n",
    "# Join diagnosis features\n",
    "print(\"   ğŸ¥ Adding diagnosis features...\")\n",
    "final_dataset = final_dataset.join(diagnosis_features, \"HADM_ID\", \"left\")\n",
    "\n",
    "print(f\"âœ… All features joined! Dataset shape: {final_dataset.count():,} rows Ã— {len(final_dataset.columns)} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# HANDLE MISSING VALUES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ”§ Step 2: Handling missing values...\")\n",
    "\n",
    "# Fill missing diagnosis counts with 0\n",
    "final_dataset = final_dataset.fillna({\n",
    "    \"TOTAL_DIAGNOSES\": 0,\n",
    "    \"HAS_SEPSIS\": 0, \n",
    "    \"HAS_RESPIRATORY_FAILURE\": 0,\n",
    "    \"HAS_CARDIAC_ARREST\": 0\n",
    "})\n",
    "\n",
    "# Fill missing vital signs with population medians (approximate values)\n",
    "vital_defaults = {\n",
    "    \"HEART_RATE_AVG\": 80, \"HEART_RATE_MIN\": 65, \"HEART_RATE_MAX\": 100, \"HEART_RATE_STD\": 15,\n",
    "    \"SBP_AVG\": 120, \"SBP_MIN\": 100, \"SBP_MAX\": 140, \"SBP_STD\": 20,\n",
    "    \"DBP_AVG\": 70, \"DBP_MIN\": 55, \"DBP_MAX\": 85, \"DBP_STD\": 15,\n",
    "    \"RESP_RATE_AVG\": 18, \"RESP_RATE_MIN\": 12, \"RESP_RATE_MAX\": 24, \"RESP_RATE_STD\": 6,\n",
    "    \"TEMPERATURE_AVG\": 37.0, \"TEMPERATURE_MIN\": 36.5, \"TEMPERATURE_MAX\": 37.5, \"TEMPERATURE_STD\": 0.5,\n",
    "    \"SPO2_AVG\": 97, \"SPO2_MIN\": 95, \"SPO2_MAX\": 99, \"SPO2_STD\": 2\n",
    "}\n",
    "\n",
    "final_dataset = final_dataset.fillna(vital_defaults)\n",
    "\n",
    "# Fill missing lab values with population medians\n",
    "lab_defaults = {\n",
    "    \"CREATININE_AVG\": 1.0, \"CREATININE_FIRST\": 1.0,\n",
    "    \"CHLORIDE_AVG\": 102, \"CHLORIDE_FIRST\": 102,\n",
    "    \"GLUCOSE_AVG\": 120, \"GLUCOSE_FIRST\": 120,\n",
    "    \"SODIUM_AVG\": 140, \"SODIUM_FIRST\": 140,\n",
    "    \"POTASSIUM_AVG\": 4.0, \"POTASSIUM_FIRST\": 4.0,\n",
    "    \"HEMOGLOBIN_AVG\": 11.0, \"HEMOGLOBIN_FIRST\": 11.0,\n",
    "    \"PLATELET_AVG\": 250, \"PLATELET_FIRST\": 250,\n",
    "    \"WBC_AVG\": 8.5, \"WBC_FIRST\": 8.5,\n",
    "    \"PH_AVG\": 7.4, \"PH_FIRST\": 7.4\n",
    "}\n",
    "\n",
    "final_dataset = final_dataset.fillna(lab_defaults)\n",
    "\n",
    "# Fill remaining missing values with 0\n",
    "final_dataset = final_dataset.fillna(0)\n",
    "\n",
    "print(\"âœ… Missing values handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88f6f328-17d8-40a6-ba45-769f22203346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Step 3: Selecting final features for regression modeling...\n",
      "âœ… Final modeling dataset prepared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Final dataset: 19 ICU stays\n",
      "ğŸ“Š Total features: 39 predictive features\n",
      "ğŸ¯ Target variable: ICU_LOS_DAYS (continuous)\n",
      "\n",
      "ğŸ“‹ Feature categories:\n",
      "   ğŸ‘¤ Demographics: 2 features\n",
      "   ğŸ¥ Admission: 8 features\n",
      "   ğŸ¢ ICU Unit: 6 features\n",
      "   ğŸ«€ Vital Signs: 11 features\n",
      "   ğŸ§ª Laboratory: 8 features\n",
      "   ğŸ©º Diagnoses: 4 features\n",
      "\n",
      "ğŸ“‹ Sample of final modeling dataset:\n",
      "+----------+------------+--------------------+-----------------+----------------+----------+\n",
      "|ICUSTAY_ID|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|   HEART_RATE_AVG|CREATININE_FIRST|HAS_SEPSIS|\n",
      "+----------+------------+--------------------+-----------------+----------------+----------+\n",
      "|    290009|      2.8701|                  67|            69.55|             1.1|         1|\n",
      "|    252713|       0.848|                  62|78.82608695652173|             0.9|         0|\n",
      "|    298190|      1.2597|                  73|             84.5|             0.7|         0|\n",
      "|    259725|      1.8064|                  80|         62.28125|             1.0|         1|\n",
      "|    255819|       0.758|                  56|             80.0|             0.9|         0|\n",
      "+----------+------------+--------------------+-----------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ğŸ“ˆ Final ICU Length of Stay Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 113:==============================================>      (174 + 6) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      ICU_LOS_DAYS|\n",
      "+-------+------------------+\n",
      "|  count|                19|\n",
      "|   mean| 2.444963157894737|\n",
      "| stddev|2.0374433631420765|\n",
      "|    min|            0.7459|\n",
      "|    max|            8.9163|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "â° Dataset preparation completed at: 2025-05-31 17:10:27\n",
      "ğŸš€ Ready for train/test split and model training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“‹ Step 3: Selecting final features for regression modeling...\")\n",
    "\n",
    "# Define feature columns for modeling\n",
    "feature_columns = [\n",
    "    # Demographics\n",
    "    \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\",\n",
    "    \n",
    "    # Admission characteristics\n",
    "    \"IS_EMERGENCY_ADMISSION\", \"IS_ELECTIVE_ADMISSION\", \"CAME_FROM_ER\",\n",
    "    \"HAS_MEDICARE\", \"IS_WHITE_ETHNICITY\", \"ADMISSION_TO_ICU_HOURS\",\n",
    "    \"WEEKEND_ADMISSION\", \"NIGHT_ADMISSION\",\n",
    "    \n",
    "    # ICU unit features\n",
    "    \"FIRST_UNIT_MICU\", \"FIRST_UNIT_SICU\", \"FIRST_UNIT_CSRU\", \n",
    "    \"FIRST_UNIT_CCU\", \"FIRST_UNIT_TSICU\", \"CHANGED_ICU_UNIT\",\n",
    "    \n",
    "    # Vital signs (averages)\n",
    "    \"HEART_RATE_AVG\", \"SBP_AVG\", \"DBP_AVG\", \"RESP_RATE_AVG\", \n",
    "    \"TEMPERATURE_AVG\", \"SPO2_AVG\",\n",
    "    \n",
    "    # Vital signs (variability)\n",
    "    \"HEART_RATE_STD\", \"SBP_STD\", \"DBP_STD\", \"RESP_RATE_STD\", \"SPO2_STD\",\n",
    "    \n",
    "    # Laboratory values\n",
    "    \"CREATININE_FIRST\", \"GLUCOSE_FIRST\", \"SODIUM_FIRST\", \"POTASSIUM_FIRST\",\n",
    "    \"HEMOGLOBIN_FIRST\", \"PLATELET_FIRST\", \"WBC_FIRST\", \"PH_FIRST\",\n",
    "    \n",
    "    # Diagnosis features\n",
    "    \"TOTAL_DIAGNOSES\", \"HAS_SEPSIS\", \"HAS_RESPIRATORY_FAILURE\", \"HAS_CARDIAC_ARREST\"\n",
    "]\n",
    "\n",
    "# Create modeling dataset with selected features\n",
    "modeling_dataset = final_dataset.select(\n",
    "    [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\"] + feature_columns\n",
    ")\n",
    "\n",
    "# Remove any remaining nulls and invalid records\n",
    "modeling_dataset = modeling_dataset.filter(col(\"ICU_LOS_DAYS\").isNotNull()) \\\n",
    "                                 .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n",
    "                                 .filter(col(\"AGE_AT_ICU_ADMISSION\") >= 18)  # Adults only\n",
    "\n",
    "# Cache the final dataset\n",
    "modeling_dataset.cache()\n",
    "\n",
    "print(f\"âœ… Final modeling dataset prepared!\")\n",
    "print(f\"ğŸ“ Final dataset: {modeling_dataset.count():,} ICU stays\")\n",
    "print(f\"ğŸ“Š Total features: {len(feature_columns)} predictive features\")\n",
    "print(f\"ğŸ¯ Target variable: ICU_LOS_DAYS (continuous)\")\n",
    "\n",
    "# Show feature summary\n",
    "print(f\"\\nğŸ“‹ Feature categories:\")\n",
    "print(f\"   ğŸ‘¤ Demographics: 2 features\")\n",
    "print(f\"   ğŸ¥ Admission: 8 features\") \n",
    "print(f\"   ğŸ¢ ICU Unit: 6 features\")\n",
    "print(f\"   ğŸ«€ Vital Signs: 11 features\")\n",
    "print(f\"   ğŸ§ª Laboratory: 8 features\")\n",
    "print(f\"   ğŸ©º Diagnoses: 4 features\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(f\"\\nğŸ“‹ Sample of final modeling dataset:\")\n",
    "modeling_dataset.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"AGE_AT_ICU_ADMISSION\", \n",
    "                       \"HEART_RATE_AVG\", \"CREATININE_FIRST\", \"HAS_SEPSIS\").show(5)\n",
    "\n",
    "# Basic statistics of target variable\n",
    "print(f\"\\nğŸ“ˆ Final ICU Length of Stay Statistics:\")\n",
    "modeling_dataset.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "print(f\"\\nâ° Dataset preparation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸš€ Ready for train/test split and model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f975-668b-4ab3-a228-4af82187778e",
   "metadata": {},
   "source": [
    "## Preparing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50fa7d59-2d01-4dcf-868f-900431b44be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 1: Creating train/test split...\n",
      "âœ… Data split completed:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“ˆ Training set: 15 ICU stays (78.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Test set: 4 ICU stays (21.1%)\n",
      "\n",
      "ğŸ“ˆ Target variable distribution:\n",
      "Training set LOS statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      ICU_LOS_DAYS|\n",
      "+-------+------------------+\n",
      "|  count|                15|\n",
      "|   mean|2.7707333333333337|\n",
      "| stddev| 2.181380632819063|\n",
      "|    min|            0.7459|\n",
      "|    max|            8.9163|\n",
      "+-------+------------------+\n",
      "\n",
      "Test set LOS statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|       ICU_LOS_DAYS|\n",
      "+-------+-------------------+\n",
      "|  count|                  4|\n",
      "|   mean|           1.223325|\n",
      "| stddev|0.42495704390757744|\n",
      "|    min|              0.848|\n",
      "|    max|             1.8064|\n",
      "+-------+-------------------+\n",
      "\n",
      "\n",
      "ğŸ”§ Step 2: Assembling feature vectors...\n",
      "âœ… Feature vectors assembled:\n",
      "   ğŸ“Š Feature vector size: 39 dimensions\n",
      "\n",
      "âš–ï¸ Step 3: Scaling features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature scaling completed:\n",
      "   ğŸ“Š Features standardized (mean=0, std=1)\n",
      "   ğŸ”§ Scaler fitted on training data only\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 1: Creating train/test split...\")\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache both datasets for performance\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"âœ… Data split completed:\")\n",
    "print(f\"   ğŸ“ˆ Training set: {train_data.count():,} ICU stays ({train_data.count()/modeling_dataset.count()*100:.1f}%)\")\n",
    "print(f\"   ğŸ“Š Test set: {test_data.count():,} ICU stays ({test_data.count()/modeling_dataset.count()*100:.1f}%)\")\n",
    "\n",
    "# Show target variable distribution in both sets\n",
    "print(f\"\\nğŸ“ˆ Target variable distribution:\")\n",
    "print(f\"Training set LOS statistics:\")\n",
    "train_data.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "print(f\"Test set LOS statistics:\")\n",
    "test_data.select(\"ICU_LOS_DAYS\").describe().show()\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE VECTOR ASSEMBLY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ”§ Step 2: Assembling feature vectors...\")\n",
    "\n",
    "# Create feature vector assembler\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Apply feature assembler to training data\n",
    "train_assembled = feature_assembler.transform(train_data)\n",
    "test_assembled = feature_assembler.transform(test_data)\n",
    "\n",
    "print(f\"âœ… Feature vectors assembled:\")\n",
    "print(f\"   ğŸ“Š Feature vector size: {len(feature_columns)} dimensions\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nâš–ï¸ Step 3: Scaling features...\")\n",
    "\n",
    "# Create StandardScaler to normalize features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler_model = scaler.fit(train_assembled)\n",
    "\n",
    "# Transform both training and test data\n",
    "train_scaled = scaler_model.transform(train_assembled)\n",
    "test_scaled = scaler_model.transform(test_assembled)\n",
    "\n",
    "# Cache the final processed datasets\n",
    "train_scaled.cache()\n",
    "test_scaled.cache()\n",
    "\n",
    "print(f\"âœ… Feature scaling completed:\")\n",
    "print(f\"   ğŸ“Š Features standardized (mean=0, std=1)\")\n",
    "print(f\"   ğŸ”§ Scaler fitted on training data only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1aa2af-6e65-4688-84ac-724a79f8ba00",
   "metadata": {},
   "source": [
    "## Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2397a40-5d63-475b-a2dc-4b9f2fe4233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Step 4: Preparing final ML datasets...\n",
      "âœ… Final ML datasets prepared:\n",
      "   ğŸ¯ Target variable: 'label' (ICU_LOS_DAYS)\n",
      "   ğŸ“Š Features: 'features' (scaled vector)\n",
      "   ğŸ”‘ Identifier: 'ICUSTAY_ID'\n",
      "\n",
      "ğŸ“‹ Sample of training data structure:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|ICUSTAY_ID| label|\n",
      "+----------+------+\n",
      "|    290009|2.8701|\n",
      "|    255819| 0.758|\n",
      "|    248205|  4.05|\n",
      "|    253828|8.9163|\n",
      "|    271202|1.7742|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ğŸ“‹ Feature vector example (first 10 features):\n",
      "   ğŸ“Š Feature vector sample: [-0.17957607  0.78881064  0.48304589 -0.48304589 -0.58257527  0.90369611\n",
      " -1.93218357 -0.43565383 -0.37893237  0.        ]...\n",
      "   ğŸ“ Total feature dimensions: 39\n",
      "\n",
      "ğŸ” Step 5: Final data quality checks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ” Null values in training set: 0\n",
      "   ğŸ” Null values in test set: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:==================================================>  (191 + 5) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Final training set target statistics:\n",
      "   ğŸ“‰ Min LOS: 0.75 days\n",
      "   ğŸ“ˆ Max LOS: 8.92 days\n",
      "   ğŸ“Š Mean LOS: 2.77 days\n",
      "   ğŸ“ Std LOS: 2.18 days\n",
      "\n",
      "âœ… Data preprocessing completed successfully!\n",
      "ğŸš€ Ready for model training with 39 features\n",
      "â° Preprocessing completed at: 2025-05-31 17:12:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nğŸ“‹ Step 4: Preparing final ML datasets...\")\n",
    "\n",
    "# Select columns needed for modeling\n",
    "ml_columns = [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features\"]\n",
    "\n",
    "train_final = train_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n",
    "test_final = test_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n",
    "\n",
    "# Cache final datasets\n",
    "train_final.cache()\n",
    "test_final.cache()\n",
    "\n",
    "print(f\"âœ… Final ML datasets prepared:\")\n",
    "print(f\"   ğŸ¯ Target variable: 'label' (ICU_LOS_DAYS)\")\n",
    "print(f\"   ğŸ“Š Features: 'features' (scaled vector)\")\n",
    "print(f\"   ğŸ”‘ Identifier: 'ICUSTAY_ID'\")\n",
    "\n",
    "# Show sample of final datasets\n",
    "print(f\"\\nğŸ“‹ Sample of training data structure:\")\n",
    "train_final.select(\"ICUSTAY_ID\", \"label\").show(5)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Feature vector example (first 10 features):\")\n",
    "# Show first few elements of feature vector for one sample\n",
    "sample_features = train_final.select(\"features\").take(1)[0][\"features\"]\n",
    "print(f\"   ğŸ“Š Feature vector sample: {sample_features.toArray()[:10]}...\")\n",
    "print(f\"   ğŸ“ Total feature dimensions: {len(sample_features.toArray())}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nğŸ” Step 5: Final data quality checks...\")\n",
    "\n",
    "# Check for any remaining nulls\n",
    "train_nulls = train_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n",
    "test_nulls = test_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n",
    "\n",
    "print(f\"   ğŸ” Null values in training set: {train_nulls}\")\n",
    "print(f\"   ğŸ” Null values in test set: {test_nulls}\")\n",
    "\n",
    "# Show target variable ranges\n",
    "train_stats = train_final.agg(\n",
    "    min(\"label\").alias(\"min_los\"),\n",
    "    max(\"label\").alias(\"max_los\"), \n",
    "    avg(\"label\").alias(\"mean_los\"),\n",
    "    stddev(\"label\").alias(\"std_los\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nğŸ“Š Final training set target statistics:\")\n",
    "print(f\"   ğŸ“‰ Min LOS: {train_stats['min_los']:.2f} days\")\n",
    "print(f\"   ğŸ“ˆ Max LOS: {train_stats['max_los']:.2f} days\") \n",
    "print(f\"   ğŸ“Š Mean LOS: {train_stats['mean_los']:.2f} days\")\n",
    "print(f\"   ğŸ“ Std LOS: {train_stats['std_los']:.2f} days\")\n",
    "\n",
    "print(f\"\\nâœ… Data preprocessing completed successfully!\")\n",
    "print(f\"ğŸš€ Ready for model training with {len(feature_columns)} features\")\n",
    "print(f\"â° Preprocessing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c98beb-ae1b-445a-aa7f-9002b4d9a012",
   "metadata": {},
   "source": [
    "## Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eda7d1c4-c1ef-4261-a3b5-e05db415626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Step 1: Setting up evaluation metrics...\n",
      "âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Step 1: Setting up evaluation metrics...\")\n",
    "\n",
    "# Create regression evaluators\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f74323-98d0-44fd-b21f-f2e543449457",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a25b20c-1f60-4782-b27c-6fda0121af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Step 2: Training Linear Regression model...\n",
      "   ğŸ”„ Training Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/31 17:14:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/05/31 17:14:10 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "[Stage 214:===================================================> (196 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Linear Regression Results:\n",
      "   ğŸ“‰ RMSE: 2.169 days\n",
      "   ğŸ“Š MAE: 2.042 days\n",
      "   ğŸ“ˆ RÂ²: -33.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“ˆ Step 2: Training Linear Regression model...\")\n",
    "\n",
    "# Create Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,  # L2 regularization\n",
    "    elasticNetParam=0.0  # Ridge regression\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"   ğŸ”„ Training Linear Regression...\")\n",
    "lr_model = lr.fit(train_final)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_final)\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"âœ… Linear Regression Results:\")\n",
    "print(f\"   ğŸ“‰ RMSE: {lr_rmse:.3f} days\")\n",
    "print(f\"   ğŸ“Š MAE: {lr_mae:.3f} days\")\n",
    "print(f\"   ğŸ“ˆ RÂ²: {lr_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847f98b-4da8-41ff-b26f-f2943c8baa38",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52707ea4-eb6a-449d-8160-7013057b2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ² Step 3: Training Random Forest model...\n",
      "   ğŸ”„ Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/31 17:14:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "[Stage 244:============================================>        (168 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random Forest Results:\n",
      "   ğŸ“‰ RMSE: 1.806 days\n",
      "   ğŸ“Š MAE: 1.742 days\n",
      "   ğŸ“ˆ RÂ²: -23.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸŒ² Step 3: Training Random Forest model...\")\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"   ğŸ”„ Training Random Forest...\")\n",
    "rf_model = rf.fit(train_final)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_final)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = mae_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"âœ… Random Forest Results:\")\n",
    "print(f\"   ğŸ“‰ RMSE: {rf_rmse:.3f} days\")\n",
    "print(f\"   ğŸ“Š MAE: {rf_mae:.3f} days\")\n",
    "print(f\"   ğŸ“ˆ RÂ²: {rf_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0d610-37c0-4c99-9fc0-289846b54033",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3ff000d-87eb-4b26-976a-65db81b057f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ† Step 5: Model Performance Comparison...\n",
      "ğŸ“Š Model Performance Summary:\n",
      "+-----------------+------------------+------------------+------------------+\n",
      "|Model            |RMSE              |MAE               |R2                |\n",
      "+-----------------+------------------+------------------+------------------+\n",
      "|Linear Regression|2.1691271877330904|2.0419278696901495|-33.73911158106706|\n",
      "|Random Forest    |1.806201085747213 |1.7418127218253976|-23.0868978025397 |\n",
      "+-----------------+------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "ğŸ¥‡ Best Models:\n",
      "   ğŸ¯ Lowest RMSE: Random Forest (1.806 days)\n",
      "   ğŸ“ˆ Highest RÂ²: Random Forest (-23.087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ† Step 5: Model Performance Comparison...\")\n",
    "\n",
    "# Create comparison summary\n",
    "results_data = [\n",
    "    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n",
    "    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n",
    "]\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n",
    "\n",
    "print(\"ğŸ“Š Model Performance Summary:\")\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Find best model\n",
    "import operator\n",
    "import builtins\n",
    "best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n",
    "best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n",
    "\n",
    "print(f\"\\nğŸ¥‡ Best Models:\")\n",
    "print(f\"   ğŸ¯ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n",
    "print(f\"   ğŸ“ˆ Highest RÂ²: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e166a-1c18-4e22-938c-d6a5569bd296",
   "metadata": {},
   "source": [
    "## Display Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "068fa69e-a7fb-40f2-a287-6300bf41e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Linear Regression Predictions (Sample 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+--------------+-------------+\n",
      "|ICUSTAY_ID|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+----------+-------------+--------------+-------------+\n",
      "|231977    |0.9792    |4.016        |3.036         |310.09       |\n",
      "|252713    |0.848     |2.805        |1.957         |230.8        |\n",
      "|259725    |1.8064    |0.825        |0.982         |54.35        |\n",
      "|298190    |1.2597    |3.452        |2.192         |174.04       |\n",
      "+----------+----------+-------------+--------------+-------------+\n",
      "\n",
      "\n",
      "ğŸŒ² Random Forest Predictions (Sample 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 282:======================================>              (144 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+--------------+-------------+\n",
      "|ICUSTAY_ID|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+----------+-------------+--------------+-------------+\n",
      "|231977    |0.9792    |2.91         |1.931         |197.2        |\n",
      "|252713    |0.848     |3.174        |2.326         |274.32       |\n",
      "|259725    |1.8064    |2.817        |1.011         |55.94        |\n",
      "|298190    |1.2597    |2.959        |1.7           |134.92       |\n",
      "+----------+----------+-------------+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# Linear Regression Predictions\n",
    "print(\"\\nğŸ“ˆ Linear Regression Predictions (Sample 20):\")\n",
    "lr_display = lr_predictions.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ").orderBy(\"ICUSTAY_ID\")\n",
    "\n",
    "lr_display.show(20, truncate=False)\n",
    "\n",
    "# Random Forest Predictions\n",
    "print(\"\\nğŸŒ² Random Forest Predictions (Sample 20):\")\n",
    "rf_display = rf_predictions.select(\n",
    "    \"ICUSTAY_ID\",\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ").orderBy(\"ICUSTAY_ID\")\n",
    "\n",
    "rf_display.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905be484-c471-4032-9b55-207d934e8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
