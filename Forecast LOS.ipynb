{"cells":[{"cell_type":"markdown","id":"da1d1427","metadata":{},"source":["# ICU Length of Stay Prediction - MIMIC-III Pipeline\n","\n","## ğŸ¯ Objective\n","Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n","\n","## ğŸ“Š Data & Constraints\n","- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n","- **Filters**: \n","        - Patient Age 18-80\n","        - LOS 0.1-15 days\n","        - Valid time sequences\n","- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n","\n","\n","## ğŸŒ€ Big Data Processing\n","\n","- **CHARTEVENTS**: Chart Events table has +330 million rows\n","- **Parquet**: Converted to Parquet format for efficient storage and processing\n","- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n","\n","## ğŸ”§ Features (39 total)\n","- **Demographics (2)**: Age, gender\n","- **Admission (8)**: Emergency/elective, timing, insurance\n","- **ICU Units (6)**: Care unit types, transfers\n","- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n","- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n","- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n","\n","## ğŸ¤– Models & Results\n","- **Linear Regression**: \n","- **Random Forest**: \n","\n","## â˜ï¸ Infrastructure\n","- **GCP Dataproc**: 6x e2-highmem-4 workers (28 vCPUs, 224GB RAM)\n","- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n","\n","\n","\n"]},{"cell_type":"markdown","id":"f7c8375f-7f35-415f-8288-2ff2193e6af0","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"id":"89ed6638-09bf-4620-89fe-2bb2c00d86e6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… All imports loaded successfully!\n","â° Notebook started at: 2025-06-02 08:06:26\n"]}],"source":["# Core PySpark imports\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","\n","# Machine Learning imports\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n","from pyspark.ml.regression import RandomForestRegressor, LinearRegression #, GBTRegressor\n","#from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n","from pyspark.ml.evaluation import RegressionEvaluator #, MulticlassClassificationEvaluator\n","#from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","#from pyspark.ml import Pipeline\n","\n","from datetime import datetime, timedelta\n","import time\n","\n","print(\"âœ… All imports loaded successfully!\")\n","print(f\"â° Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"4f9d1810-67b2-471e-97d2-b8fda7db9728","metadata":{},"source":["## Setup Spark Session"]},{"cell_type":"code","execution_count":2,"id":"a15f1fca-5bf8-4e10-bdca-a6faa11ca17a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","25/06/02 08:06:52 INFO SparkEnv: Registering MapOutputTracker\n","25/06/02 08:06:52 INFO SparkEnv: Registering BlockManagerMaster\n","25/06/02 08:06:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/06/02 08:06:52 INFO SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["âœ… Spark session created successfully!\n","ğŸ“Š Spark Version: 3.5.3\n","ğŸ”§ Application Name: Forecast-LOS\n","ğŸ’¾ Available cores: 2\n","\n","â° Spark session initialised at: 2025-06-02 08:07:03\n"]}],"source":["# spark = SparkSession.builder \\\n","#     .appName(\"Forecast-LOS\") \\\n","#     .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n","#     .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n","#     .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n","#     .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n","#     .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n","#     .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n","#     .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\") \\\n","#     .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n","#     .config(\"spark.sql.files.maxPartitionBytes\", \"64MB\") \\\n","#     .config(\"spark.network.timeout\", \"1200s\") \\\n","#     .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n","#     .config(\"spark.executor.memory\", \"12g\") \\\n","#     .config(\"spark.executor.cores\", \"2\") \\\n","#     .config(\"spark.executor.instances\", \"8\") \\\n","#     .config(\"spark.driver.memory\", \"4g\") \\\n","#     .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","#     .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n","#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n","#     .config(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\") \\\n","#     .config(\"spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin\", \"0.2\") \\\n","#     .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n","#     .config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n","#     .config(\"spark.rpc.askTimeout\", \"600s\") \\\n","#     .config(\"spark.network.timeoutInterval\", \"120s\") \\\n","#     .config(\"spark.storage.blockManagerSlaveTimeoutMs\", \"120000\") \\\n","#     .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n","#     .config(\"spark.network.timeout\", \"120s\") \\\n","#     .config(\"spark.rpc.lookupTimeout\", \"120s\") \\\n","#     .config(\"spark.shuffle.registration.timeout\", \"120000\") \\\n","#     .config(\"spark.shuffle.registration.maxAttempts\", \"5\") \\\n","#     .getOrCreate()\n","\n","spark = SparkSession.builder \\\n","    .appName(\"Forecast-LOS\") \\\n","    .getOrCreate()\n","\n","print(\"âœ… Spark session created successfully!\")\n","print(f\"ğŸ“Š Spark Version: {spark.version}\")\n","print(f\"ğŸ”§ Application Name: {spark.sparkContext.appName}\")\n","print(f\"ğŸ’¾ Available cores: {spark.sparkContext.defaultParallelism}\")\n","print(f\"\\nâ° Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"7e13163e-55dc-4046-95b0-4815723d0581","metadata":{},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"id":"62936c54-45ab-4c03-a8ec-fc3d87f68721","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ¥ Loading MIMIC-III CSV files...\n","ğŸ“‚ Loading ICUSTAYS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ¯ Sampling 100 ICU stays...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["âœ… Sampled 100 ICU stays\n","ğŸ“‹ Extracting required IDs...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“Š IDs to filter: 100 ICUSTAY_IDs, 100 HADM_IDs, 100 SUBJECT_IDs\n","ğŸ“‚ Loading PATIENTS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading ADMISSIONS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading DIAGNOSES_ICD table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\n"," Converting CHARTEVENTS.csv.gz to parquet...\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 14:>                                                         (0 + 1) / 1]\r"]}],"source":["# Configuration flags\n","SAMPLE_ENABLE = True\n","SAMPLE_SIZE = 100\n","\n","MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n","#MIMIC_PATH = \"./mimic-data\"\n","\n","print(\"ğŸ¥ Loading MIMIC-III CSV files...\")\n","\n","# Step 1: Load and sample ICUSTAYS first\n","print(\"ğŸ“‚ Loading ICUSTAYS table...\")\n","icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n","\n","if SAMPLE_ENABLE:\n","    print(f\"ğŸ¯ Sampling {SAMPLE_SIZE} ICU stays...\")\n","    icustays_df = icustays_df.limit(SAMPLE_SIZE) \n","    icustays_df.cache()\n","    actual_sample_size = icustays_df.count()\n","    print(f\"âœ… Sampled {actual_sample_size} ICU stays\")\n","\n","# Step 2: Get required IDs for filtering other tables\n","print(\"ğŸ“‹ Extracting required IDs...\")\n","icu_ids = icustays_df.select(\"ICUSTAY_ID\").rdd.map(lambda row: row[0]).collect()\n","hadm_ids = icustays_df.select(\"HADM_ID\").rdd.map(lambda row: row[0]).collect()\n","subject_ids = icustays_df.select(\"SUBJECT_ID\").rdd.map(lambda row: row[0]).collect()\n","\n","print(f\"ğŸ“Š IDs to filter: {len(icu_ids)} ICUSTAY_IDs, {len(hadm_ids)} HADM_IDs, {len(subject_ids)} SUBJECT_IDs\")\n","\n","# Step 3: Load other tables with pre-filtering\n","print(\"ğŸ“‚ Loading PATIENTS table...\")\n","patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n","patients_df = patients_df.filter(col(\"SUBJECT_ID\").isin(subject_ids))\n","\n","print(\"ğŸ“‚ Loading ADMISSIONS table...\")\n","admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n","admissions_df = admissions_df.filter(col(\"HADM_ID\").isin(hadm_ids))\n","\n","print(\"ğŸ“‚ Loading DIAGNOSES_ICD table...\")\n","diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n","diagnoses_df = diagnoses_df.filter(col(\"HADM_ID\").isin(hadm_ids))\n","\n","\n","\n","\n","# Step 4: Load large tables (CHARTEVENTS, LABEVENTS) with aggressive filtering\n","print(\"ğŸ“‚ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n","\n","try:\n","    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","except:\n","    print(f\" Converting CHARTEVENTS.csv.gz to parquet...\")\n","    chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n","    chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","    \n","icu_ids_df = spark.createDataFrame([(id,) for id in icu_ids], [\"ICUSTAY_ID\"])\n","chartevents_df = chartevents_df \\\n","    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n","    .join(broadcast(icu_ids_df), \"ICUSTAY_ID\", \"inner\")\n","\n","\n","\n","\n","print(\"ğŸ“‚ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n","\n","try:\n","    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","except:\n","    print(f\" Converting LABEVENTS.csv.gz to parquet...\")\n","    labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n","    labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","\n","    \n","labevents_df = labevents_df \\\n","                .filter(col(\"HADM_ID\").isin(hadm_ids))\n","\n","\n","\n","\n","# Display final information\n","print(\"\\nâœ… Tables loaded and filtered successfully!\")\n","if SAMPLE_ENABLE:\n","    print(f\"ğŸ¯ Using sample of {actual_sample_size} ICU stays\")\n","    print(f\"ğŸ“Š ICUSTAYS: {icustays_df.count():,} rows\")\n","    print(f\"ğŸ“Š PATIENTS: {patients_df.count():,} rows\") \n","    print(f\"ğŸ“Š ADMISSIONS: {admissions_df.count():,} rows\")\n","    print(f\"ğŸ“Š DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n","    print(f\"ğŸ“Š CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n","    print(f\"ğŸ“Š LABEVENTS (filtered): {labevents_df.count():,} rows\")\n","\n","print(f\"\\nâ° Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"9e8e7157-1230-41d3-8f41-1117b62fdb55","metadata":{},"source":["## Features Engineering\n","\n","Current features for regression:\n","\n","- Demographics (age, gender)\n","- Admission characteristics (emergency vs elective, timing)\n","- ICU unit types and transfers\n","- Time-based features (weekend, night admissions)\n","- Medical data\n"]},{"cell_type":"markdown","id":"8763e6d7-4ba5-439b-8e2b-ba16cf607a3e","metadata":{},"source":["## Extracting Data From ICUSTAYS"]},{"cell_type":"code","execution_count":null,"id":"9f1d7adb-eac0-4071-b252-a04268f0c767","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Creating base ICU dataset with patient demographics...\")\n","\n","base_icu_df = icustays_df.alias(\"icu\") \\\n","    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n","    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n","    .select(\n","        # ICU stay identifiers\n","        col(\"icu.ICUSTAY_ID\"),\n","        col(\"icu.SUBJECT_ID\"), \n","        col(\"icu.HADM_ID\"),\n","        \n","        # Target variable - Length of Stay in ICU (days)\n","        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n","        \n","        # ICU characteristics\n","        col(\"icu.FIRST_CAREUNIT\"),\n","        col(\"icu.LAST_CAREUNIT\"), \n","        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n","        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n","        \n","        # Patient demographics\n","        col(\"pat.GENDER\"),\n","        col(\"pat.DOB\"),\n","        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n","        \n","        # Admission details\n","        col(\"adm.ADMITTIME\"),\n","        col(\"adm.DISCHTIME\"), \n","        col(\"adm.ADMISSION_TYPE\"),\n","        col(\"adm.ADMISSION_LOCATION\"),\n","        col(\"adm.INSURANCE\"),\n","        col(\"adm.ETHNICITY\"),\n","        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n","        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n","    )\n","\n","# Calculate age at ICU admission\n","base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n","                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n","                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n","\n","\n","print(\"âœ… Created base ICU dataset!\")\n"]},{"cell_type":"markdown","id":"595638e6-74f7-4f2f-a7e5-1fc692089206","metadata":{},"source":["## Extracting Categorical Features"]},{"cell_type":"code","execution_count":null,"id":"d28d34e6-83c3-40ae-95d0-71f9929397a1","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 2: Engineering categorical features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n","    .withColumn(\"IS_EMERGENCY_ADMISSION\", \n","                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1).otherwise(0)) \\\n","    .withColumn(\"IS_ELECTIVE_ADMISSION\", \n","                when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 1).otherwise(0)) \\\n","    .withColumn(\"CAME_FROM_ER\", \n","                when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_MEDICARE\", \n","                when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n","    .withColumn(\"IS_WHITE_ETHNICITY\", \n","                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1).otherwise(0))\n","\n","print(\"âœ… Base ICU Dataset - Categorical Features\")\n"]},{"cell_type":"markdown","id":"9ca22bc8-8dd5-4d30-9cb9-0f366da21d76","metadata":{},"source":["## Extracting ICU Unit Types"]},{"cell_type":"code","execution_count":null,"id":"6617c3f2-75b9-4fa0-93a8-7161d0c2dd57","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 3: Creating ICU unit type features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"FIRST_UNIT_MICU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_SICU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"SICU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_CSRU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_CCU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"CCU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_TSICU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 1).otherwise(0)) \\\n","    .withColumn(\"CHANGED_ICU_UNIT\", \n","                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n","\n","\n","print(\"âœ… Base ICU Dataset - Unit Type Features\")"]},{"cell_type":"markdown","id":"74662ab6-f263-402e-913f-dc04804eadff","metadata":{},"source":["## Extracting Time-based Features"]},{"cell_type":"code","execution_count":null,"id":"ef431386-85f1-4867-9aa1-f5bb84d7c8ae","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 4: Creating time-based features...\")\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"ADMISSION_TO_ICU_HOURS\", \n","                (unix_timestamp(\"ICU_INTIME\") - unix_timestamp(\"ADMITTIME\")) / 3600) \\\n","    .withColumn(\"ICU_LOS_HOURS\", col(\"ICU_LOS_DAYS\") * 24) \\\n","    .withColumn(\"WEEKEND_ADMISSION\", \n","                when(dayofweek(\"ICU_INTIME\").isin([1, 7]), 1).otherwise(0)) \\\n","    .withColumn(\"NIGHT_ADMISSION\", \n","                when(hour(\"ICU_INTIME\").between(20, 7), 1).otherwise(0)) \\\n","    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\")) \\\n","    .filter(col(\"ADMITTIME\") <= col(\"ICU_INTIME\")) \\\n","    .filter(col(\"ICU_LOS_DAYS\") > 0.04)"]},{"cell_type":"markdown","id":"779ac96b-6e5f-4fdd-84a2-043a0beaa06b","metadata":{},"source":["## Remove Outliers (Excessive Length Of Stay)"]},{"cell_type":"code","execution_count":null,"id":"b6ce0e40-182e-4fbd-bb35-2bb23ed0d0de","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 5: Cleaning target variable...\")\n","\n","base_icu_df = base_icu_df.filter(col(\"ICU_LOS_DAYS\").between(0.1, 15))\n","base_icu_df.cache()\n","\n","print(\"âœ… Base ICU Dataset - Remove Outliers\")"]},{"cell_type":"markdown","id":"d4a1aa14-d750-44e7-9b44-c0f37dc4dfaa","metadata":{},"source":["## Show Dataset Info"]},{"cell_type":"code","execution_count":null,"id":"8d6cadf4-d2ab-4242-808d-73575be75130","metadata":{},"outputs":[],"source":["print(\"âœ… Master ICU dataset created!\")\n","print(f\"ğŸ“ Dataset size: {base_icu_df.count():,} ICU stays\")\n","print(f\"ğŸ“Š Features created: {len(base_icu_df.columns)} columns\")\n","\n","# Display sample of the dataset\n","print(\"\\nğŸ“‹ Sample of regression features:\")\n","base_icu_df.select(\n","    \"ICUSTAY_ID\", \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\", \"ICU_LOS_DAYS\", \n","    \"FIRST_CAREUNIT\", \"IS_EMERGENCY_ADMISSION\", \"ADMISSION_TO_ICU_HOURS\"\n",").show(5)\n","\n","# Show basic statistics of target variable\n","print(\"\\nğŸ“ˆ ICU Length of Stay Statistics:\")\n","base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","print(f\"\\nâ° Feature engineering completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"268129f5-72e2-40a5-b676-cbd825ae84c8","metadata":{},"source":["## Extracting Clinical Features"]},{"cell_type":"code","execution_count":null,"id":"9dc92782-4f16-4c82-bd8a-44ed78e92965","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š  Extrcting Vital Signs...\")\n","\n","# Key vital signs ITEMID mappings (common across MIMIC-III)\n","vital_signs_items = {\n","    220045: \"HEART_RATE\",      # Heart Rate\n","    220050: \"SBP\",             # Systolic BP  \n","    220051: \"DBP\",             # Diastolic BP\n","    220210: \"RESP_RATE\",       # Respiratory Rate\n","    223762: \"TEMPERATURE\",     # Temperature Celsius\n","    220277: \"SPO2\"             # Oxygen Saturation\n","}\n","\n","# Get ICU IDs (using existing approach)\n","icu_ids_list = [row[\"ICUSTAY_ID\"] for row in base_icu_df.select(\"ICUSTAY_ID\").collect()]\n","print(f\"ğŸ¯ Processing {len(icu_ids_list)} ICU stays for {len(vital_signs_items)} vital signs\")\n","\n","vital_items_list = list(vital_signs_items.keys())\n","\n","\n","print(\"ğŸ“Š Filtering CHARTEVENTS...\")\n","chartevents_prefiltered = chartevents_df \\\n","    .filter(col(\"ITEMID\").isin(vital_items_list)) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\").between(1, 500)) \\\n","    .filter(col(\"ICUSTAY_ID\").isin(icu_ids_list)) \\\n","    .filter(col(\"CHARTTIME\").isNotNull()) \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(col(\"CHARTTIME\").between(col(\"ICU_INTIME\"), col(\"ICU_OUTTIME\"))) \\\n","    .select(\"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\")\n","\n","\n","\n","print(\"ğŸ“Š Processing vital signs within first 24 hours...\")\n","\n","vitals_24h = chartevents_prefiltered.alias(\"ce\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(\n","        col(\"ce.CHARTTIME\").between(\n","            col(\"ICU_INTIME\"), \n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","print(\"âœ… 24-hour vital signs data ready\")\n","\n","\n","\n","vital_signs_items = {\n","    220045: \"HEART_RATE\",\n","    220050: \"SBP\", \n","    220051: \"DBP\",\n","    220210: \"RESP_RATE\",\n","    223762: \"TEMPERATURE\",\n","    220277: \"SPO2\"\n","}\n","\n","# Start with base ICU dataframe\n","vitals_features = base_icu_df.select(\"ICUSTAY_ID\")\n","\n","# Add each vital sign as separate joins (faster than pivot)\n","for itemid, name in vital_signs_items.items():\n","    print(f\"   ğŸ“Š Processing {name}...\")\n","    \n","    vital_stats = vitals_24h \\\n","        .filter(col(\"ITEMID\") == itemid) \\\n","        .groupBy(\"ICUSTAY_ID\") \\\n","        .agg(\n","            avg(\"VALUENUM\").alias(f\"{name}_AVG\"),\n","            min(\"VALUENUM\").alias(f\"{name}_MIN\"),\n","            max(\"VALUENUM\").alias(f\"{name}_MAX\"),\n","            stddev(\"VALUENUM\").alias(f\"{name}_STD\"),\n","            count(\"VALUENUM\").alias(f\"{name}_COUNT\")\n","        )\n","    \n","    # Left join to maintain all ICU stays\n","    vitals_features = vitals_features.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n","\n","    \n","chartevents_df.unpersist()\n","vitals_24h.unpersist()\n","\n","print(\"ğŸ“Š Counting final features...\")\n","feature_count = vitals_features.count()\n","print(f\"âœ… Vital signs features created for vital_features_count ICU stays\")\n","\n","# Show sample of features\n","print(\"ğŸ“Š Sample features:\")\n","vitals_features.show(5)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5b685f0e-f499-4adf-9fd9-a6f92e22a1cf","metadata":{},"outputs":[],"source":["print(\"\\nğŸ§ª Creating laboratory features from LABEVENTS...\")\n","\n","# Key lab test ITEMID mappings\n","lab_items = {\n","    50912: \"CREATININE\",       # Creatinine\n","    50902: \"CHLORIDE\",         # Chloride\n","    50931: \"GLUCOSE\",          # Glucose\n","    50983: \"SODIUM\",           # Sodium\n","    50971: \"POTASSIUM\",        # Potassium\n","    51222: \"HEMOGLOBIN\",       # Hemoglobin\n","    51265: \"PLATELET\",         # Platelet Count\n","    51301: \"WBC\",              # White Blood Cells\n","    50820: \"PH\"                # pH\n","}\n","\n","# Filter lab events within first 24 hours of ICU stay\n","labs_24h = labevents_df.alias(\"le\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n","    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n","    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n","    .filter(col(\"le.VALUENUM\") > 0) \\\n","    .filter(\n","        col(\"le.CHARTTIME\").between(\n","            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","# Calculate lab value statistics\n","print(\"   ğŸ“Š Calculating laboratory statistics (first 24h)...\")\n","\n","labs_stats = labs_24h.groupBy(\"ICUSTAY_ID\", \"ITEMID\") \\\n","    .agg(\n","        avg(\"VALUENUM\").alias(\"avg_value\"),\n","        min(\"VALUENUM\").alias(\"min_value\"),\n","        max(\"VALUENUM\").alias(\"max_value\"),\n","        first(\"VALUENUM\").alias(\"first_value\")  # First available value\n","    )\n","\n","\n","\n","# Pivot lab results\n","labs_features = labs_stats.groupBy(\"ICUSTAY_ID\").pivot(\"ITEMID\").agg(\n","    first(\"avg_value\").alias(\"avg\"),\n","    first(\"first_value\").alias(\"first\")\n",")\n","\n","\n","# Rename lab columns\n","for itemid, name in lab_items.items():\n","    labs_features = labs_features \\\n","        .withColumnRenamed(f\"{itemid}_avg\", f\"{name}_AVG\") \\\n","        .withColumnRenamed(f\"{itemid}_first\", f\"{name}_FIRST\")\n","\n","print(f\"   âœ… Laboratory features created for {labs_features.count():,} ICU stays\")\n","\n","\n","# Show sample of features\n","print(\"ğŸ“Š Sample features:\")\n","labs_features.show(5)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"94a414c1-7139-4e27-a5e4-767919a3eead","metadata":{},"outputs":[],"source":["print(\"\\nğŸ¥ Creating diagnosis features from ICD codes...\")\n","\n","# Count number of diagnoses per admission (comorbidity burden)\n","diagnosis_counts = diagnoses_df.groupBy(\"HADM_ID\") \\\n","    .agg(\n","        count(\"ICD9_CODE\").alias(\"TOTAL_DIAGNOSES\"),\n","        collect_list(\"ICD9_CODE\").alias(\"DIAGNOSIS_CODES\")\n","    )\n","\n","# Create features for common diagnosis categories\n","diagnosis_features = diagnosis_counts \\\n","    .withColumn(\"HAS_SEPSIS\", \n","                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"99591\") | \n","                     array_contains(col(\"DIAGNOSIS_CODES\"), \"99592\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_RESPIRATORY_FAILURE\",\n","                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"51881\") |\n","                     array_contains(col(\"DIAGNOSIS_CODES\"), \"51882\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_CARDIAC_ARREST\",\n","                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"4275\"), 1).otherwise(0)) \\\n","    .drop(\"DIAGNOSIS_CODES\")\n","\n","\n","print(f\"   âœ… Diagnosis features created for {diagnosis_features.count():,} admissions\")\n","\n","# Show sample of features\n","print(\"ğŸ“Š Sample features:\")\n","diagnosis_features.show(5)\n","\n","\n","print(f\"\\nâ° Clinical features completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"b27d9140-230d-4bc0-91ae-fd9db043e5a5","metadata":{},"source":["# Joining All Features"]},{"cell_type":"code","execution_count":null,"id":"1fe76c11-fb64-4b12-9a37-8efccf76d55d","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Joining base features with clinical data...\")\n","\n","# Start with base ICU dataset\n","final_dataset = base_icu_df\n","\n","\n","\n","print(\"   ğŸ«€ Adding vital signs features...\")\n","final_dataset = final_dataset.join(vitals_features, \"ICUSTAY_ID\", \"left\")\n","\n","print(\"   ğŸ§ª Adding laboratory features...\")\n","final_dataset = final_dataset.join(labs_features, \"ICUSTAY_ID\", \"left\")\n","\n","print(\"   ğŸ¥ Adding diagnosis features...\")\n","final_dataset = final_dataset.join(diagnosis_features, \"HADM_ID\", \"left\")\n","\n","print(f\"âœ… All features joined! Final Dataset\")\n","\n","base_icu_df.unpersist()\n","vitals_features.unpersist()\n","labs_features.unpersist()\n","diagnosis_features.unpersist()\n","\n","\n","print(f\"âœ… Cleared from memory and disk\")\n","\n","# ============================================================================\n","# HANDLE MISSING VALUES\n","# ============================================================================\n","\n","print(\"\\nğŸ”§ Step 2: Handling missing values...\")\n","\n","# Fill missing diagnosis counts with 0\n","final_dataset = final_dataset.fillna({\n","    \"TOTAL_DIAGNOSES\": 0,\n","    \"HAS_SEPSIS\": 0, \n","    \"HAS_RESPIRATORY_FAILURE\": 0,\n","    \"HAS_CARDIAC_ARREST\": 0\n","})\n","\n","# Fill missing vital signs with population medians (approximate values)\n","# MTS REGISTOS VAO FICAR COM OS DEFAULTS POR CAUSA DOS NULLS NAS FEATURES DE CHARTEVENTS\n","vital_defaults = {\n","    \"HEART_RATE_AVG\": 80, \"HEART_RATE_MIN\": 65, \"HEART_RATE_MAX\": 100, \"HEART_RATE_STD\": 15,\n","    \"SBP_AVG\": 120, \"SBP_MIN\": 100, \"SBP_MAX\": 140, \"SBP_STD\": 20,\n","    \"DBP_AVG\": 70, \"DBP_MIN\": 55, \"DBP_MAX\": 85, \"DBP_STD\": 15,\n","    \"RESP_RATE_AVG\": 18, \"RESP_RATE_MIN\": 12, \"RESP_RATE_MAX\": 24, \"RESP_RATE_STD\": 6,\n","    \"TEMPERATURE_AVG\": 37.0, \"TEMPERATURE_MIN\": 36.5, \"TEMPERATURE_MAX\": 37.5, \"TEMPERATURE_STD\": 0.5,\n","    \"SPO2_AVG\": 97, \"SPO2_MIN\": 95, \"SPO2_MAX\": 99, \"SPO2_STD\": 2\n","}\n","\n","final_dataset = final_dataset.fillna(vital_defaults)\n","\n","# Fill missing lab values with population medians\n","lab_defaults = {\n","    \"CREATININE_AVG\": 1.0, \"CREATININE_FIRST\": 1.0,\n","    \"CHLORIDE_AVG\": 102, \"CHLORIDE_FIRST\": 102,\n","    \"GLUCOSE_AVG\": 120, \"GLUCOSE_FIRST\": 120,\n","    \"SODIUM_AVG\": 140, \"SODIUM_FIRST\": 140,\n","    \"POTASSIUM_AVG\": 4.0, \"POTASSIUM_FIRST\": 4.0,\n","    \"HEMOGLOBIN_AVG\": 11.0, \"HEMOGLOBIN_FIRST\": 11.0,\n","    \"PLATELET_AVG\": 250, \"PLATELET_FIRST\": 250,\n","    \"WBC_AVG\": 8.5, \"WBC_FIRST\": 8.5,\n","    \"PH_AVG\": 7.4, \"PH_FIRST\": 7.4\n","}\n","\n","final_dataset = final_dataset.fillna(lab_defaults)\n","\n","# Fill remaining missing values with 0\n","final_dataset = final_dataset.fillna(0)\n","\n","\n","print(\"âœ… Missing values handled\")\n","\n","print(\"ğŸ”§ Fixing data types for ML...\")\n","\n","# Cast problematic string columns to double\n","from pyspark.sql.functions import col\n","\n","string_columns = [\n","    \"CREATININE_FIRST\", \"GLUCOSE_FIRST\", \"SODIUM_FIRST\", \"POTASSIUM_FIRST\",\n","    \"HEMOGLOBIN_FIRST\", \"PLATELET_FIRST\", \"WBC_FIRST\", \"PH_FIRST\"\n","]\n","\n","for col_name in string_columns:\n","    if col_name in final_dataset.columns:\n","        final_dataset = final_dataset.withColumn(\n","            col_name, \n","            col(col_name).cast(\"double\")\n","        )\n","\n","# Fill any nulls created during conversion\n","final_dataset = final_dataset.fillna({\n","    \"CREATININE_FIRST\": 1.0,\n","    \"GLUCOSE_FIRST\": 120.0,\n","    \"SODIUM_FIRST\": 140.0,\n","    \"POTASSIUM_FIRST\": 4.0,\n","    \"HEMOGLOBIN_FIRST\": 11.0,\n","    \"PLATELET_FIRST\": 250.0,\n","    \"WBC_FIRST\": 8.5,\n","    \"PH_FIRST\": 7.4\n","})\n","\n","print(\"âœ… Data types fixed!\")\n"]},{"cell_type":"code","execution_count":null,"id":"88f6f328-17d8-40a6-ba45-769f22203346","metadata":{},"outputs":[],"source":["print(\"\\nğŸ“‹ Step 3: Selecting final features for regression modeling...\")\n","\n","# Define feature columns for modeling\n","feature_columns = [\n","    # Demographics\n","    \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\",\n","    \n","    # Admission characteristics\n","    \"IS_EMERGENCY_ADMISSION\", \"IS_ELECTIVE_ADMISSION\", \"CAME_FROM_ER\",\n","    \"HAS_MEDICARE\", \"IS_WHITE_ETHNICITY\", \"ADMISSION_TO_ICU_HOURS\",\n","    \"WEEKEND_ADMISSION\", \"NIGHT_ADMISSION\",\n","    \n","    # ICU unit features\n","    \"FIRST_UNIT_MICU\", \"FIRST_UNIT_SICU\", \"FIRST_UNIT_CSRU\", \n","    \"FIRST_UNIT_CCU\", \"FIRST_UNIT_TSICU\", \"CHANGED_ICU_UNIT\",\n","    \n","    # Vital signs (averages)\n","    \"HEART_RATE_AVG\", \"SBP_AVG\", \"DBP_AVG\", \"RESP_RATE_AVG\", \n","    \"TEMPERATURE_AVG\", \"SPO2_AVG\",\n","    \n","    # Vital signs (variability)\n","    \"HEART_RATE_STD\", \"SBP_STD\", \"DBP_STD\", \"RESP_RATE_STD\", \"SPO2_STD\",\n","    \n","    # Laboratory values\n","    \"CREATININE_FIRST\", \"GLUCOSE_FIRST\", \"SODIUM_FIRST\", \"POTASSIUM_FIRST\",\n","    \"HEMOGLOBIN_FIRST\", \"PLATELET_FIRST\", \"WBC_FIRST\", \"PH_FIRST\",\n","    \n","    # Diagnosis features\n","    \"TOTAL_DIAGNOSES\", \"HAS_SEPSIS\", \"HAS_RESPIRATORY_FAILURE\", \"HAS_CARDIAC_ARREST\"\n","]\n","\n","# Create modeling dataset with selected features\n","modeling_dataset = final_dataset.select(\n","    [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\"] + feature_columns\n",")\n","\n","# Remove any remaining nulls and invalid records\n","modeling_dataset = modeling_dataset.filter(col(\"ICU_LOS_DAYS\").isNotNull()) \\\n","    .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n","    .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n","\n","# Cache the final dataset\n","#modeling_dataset = modeling_dataset.repartition()\n","#modeling_dataset.cache()\n","\n","\n","print(f\"âœ… Final modeling dataset prepared!\")\n","print(f\"ğŸ“ Final dataset: {modeling_dataset.count():,} ICU stays\")\n","print(f\"ğŸ“Š Total features: {len(feature_columns)} predictive features\")\n","print(f\"ğŸ¯ Target variable: ICU_LOS_DAYS (continuous)\")\n","\n","# Show feature summary\n","print(f\"\\nğŸ“‹ Feature categories:\")\n","print(f\"   ğŸ‘¤ Demographics: 2 features\")\n","print(f\"   ğŸ¥ Admission: 8 features\") \n","print(f\"   ğŸ¢ ICU Unit: 6 features\")\n","print(f\"   ğŸ«€ Vital Signs: 11 features\")\n","print(f\"   ğŸ§ª Laboratory: 8 features\")\n","print(f\"   ğŸ©º Diagnoses: 4 features\")\n","\n","# Display sample of final dataset\n","#print(f\"\\nğŸ“‹ Sample of final modeling dataset:\")\n","#modeling_dataset.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"AGE_AT_ICU_ADMISSION\", \n","#                        \"HEART_RATE_AVG\", \"CREATININE_FIRST\", \"HAS_SEPSIS\").show(5)\n","\n","# Basic statistics of target variable\n","#print(f\"\\nğŸ“ˆ Final ICU Length of Stay Statistics:\")\n","#modeling_dataset.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","print(f\"\\nâ° Dataset preparation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(f\"ğŸš€ Ready for train/test split and model training!\")"]},{"cell_type":"markdown","id":"5fa7f975-668b-4ab3-a228-4af82187778e","metadata":{},"source":["## Preparing for Machine Learning"]},{"cell_type":"code","execution_count":null,"id":"50fa7d59-2d01-4dcf-868f-900431b44be9","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Creating train/test split...\")\n","\n","# Split the data (80% train, 20% test)\n","train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=42)\n","\n","# Cache both datasets for performance\n","train_data.cache()\n","test_data.cache()\n","\n","\n","print(f\"âœ… Data split completed:\")\n","#print(f\"   ğŸ“ˆ Training set: {train_data.count():,} ICU stays ({train_data.count()/modeling_dataset.count()*100:.1f}%)\")\n","#print(f\"   ğŸ“Š Test set: {test_data.count():,} ICU stays ({test_data.count()/modeling_dataset.count()*100:.1f}%)\")\n","\n","# Show target variable distribution in both sets\n","print(f\"\\nğŸ“ˆ Target variable distribution:\")\n","print(f\"Training set LOS statistics:\")\n","train_data.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","print(f\"Test set LOS statistics:\")\n","test_data.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","# ============================================================================\n","# FEATURE VECTOR ASSEMBLY\n","# ============================================================================\n","\n","print(\"\\nğŸ”§ Step 2: Assembling feature vectors...\")\n","\n","# Create feature vector assembler\n","feature_assembler = VectorAssembler(\n","    inputCols=feature_columns,\n","    outputCol=\"features_raw\"\n",")\n","\n","# Apply feature assembler to training data\n","train_assembled = feature_assembler.transform(train_data)\n","test_assembled = feature_assembler.transform(test_data)\n","\n","\n","train_data.unpersist()\n","test_data.unpersist()\n","\n","\n","print(f\"âœ… Feature vectors assembled:\")\n","print(f\"   ğŸ“Š Feature vector size: {len(feature_columns)} dimensions\")\n","\n","# ============================================================================\n","# FEATURE SCALING\n","# ============================================================================\n","\n","print(\"\\nâš–ï¸ Step 3: Scaling features...\")\n","\n","# Create StandardScaler to normalize features\n","scaler = StandardScaler(\n","    inputCol=\"features_raw\",\n","    outputCol=\"features\",\n","    withStd=True,\n","    withMean=True\n",")\n","\n","# Fit scaler on training data\n","scaler_model = scaler.fit(train_assembled)\n","train_scaled = scaler_model.transform(train_assembled)\n","test_scaled = scaler_model.transform(test_assembled)\n","\n","# Cache the final processed datasets\n","train_scaled.cache()\n","test_scaled.cache()\n","\n","\n","print(f\"âœ… Feature scaling completed:\")\n","print(f\"   ğŸ“Š Features standardized (mean=0, std=1)\")\n","print(f\"   ğŸ”§ Scaler fitted on training data only\")\n"]},{"cell_type":"markdown","id":"8b1aa2af-6e65-4688-84ac-724a79f8ba00","metadata":{},"source":["## Final Dataset Preparation"]},{"cell_type":"code","execution_count":null,"id":"c2397a40-5d63-475b-a2dc-4b9f2fe4233b","metadata":{},"outputs":[],"source":["\n","print(\"\\nğŸ“‹ Step 4: Preparing final ML datasets...\")\n","\n","# Select columns needed for modeling\n","ml_columns = [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features\"]\n","\n","train_final = train_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","test_final = test_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","\n","#train_final = train_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","#test_final = test_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","\n","\n","# train_final = train_assembled.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features_raw\") \\\n","#     .withColumnRenamed(\"ICU_LOS_DAYS\", \"label\") \\\n","#     .withColumnRenamed(\"features_raw\", \"features\")\n","\n","# test_final = test_assembled.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features_raw\") \\\n","#     .withColumnRenamed(\"ICU_LOS_DAYS\", \"label\") \\\n","#     .withColumnRenamed(\"features_raw\", \"features\")\n","\n","\n","\n","\n","print(\"\\nğŸ“‹ Caching...\")\n","\n","# Cache final datasets\n","train_final.cache()\n","test_final.cache()\n","\n","print(f\"âœ… Final ML datasets prepared:\")\n","print(f\"   ğŸ¯ Target variable: 'label' (ICU_LOS_DAYS)\")\n","print(f\"   ğŸ“Š Features: 'features' (scaled vector)\")\n","print(f\"   ğŸ”‘ Identifier: 'ICUSTAY_ID'\")\n","\n","# Show sample of final datasets\n","print(f\"\\nğŸ“‹ Sample of training data structure:\")\n","train_final.select(\"ICUSTAY_ID\", \"label\").show(3)\n","\n","print(f\"\\nğŸ“‹ Feature vector example (first 10 features):\")\n","# Show first few elements of feature vector for one sample\n","sample_features = train_final.select(\"features\").take(1)[0][\"features\"]\n","print(f\"   ğŸ“Š Feature vector sample: {sample_features.toArray()[:10]}...\")\n","print(f\"   ğŸ“ Total feature dimensions: {len(sample_features.toArray())}\")\n","\n","# ============================================================================\n","# DATA QUALITY CHECKS\n","# ============================================================================\n","\n","print(f\"\\nğŸ” Step 5: Final data quality checks...\")\n","\n","# Check for any remaining nulls\n","train_nulls = train_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n","test_nulls = test_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n","\n","print(f\"   ğŸ” Null values in training set: {train_nulls}\")\n","print(f\"   ğŸ” Null values in test set: {test_nulls}\")\n","\n","# Show target variable ranges\n","# train_stats = train_final.agg(\n","#     min(\"label\").alias(\"min_los\"),\n","#     max(\"label\").alias(\"max_los\"), \n","#     avg(\"label\").alias(\"mean_los\"),\n","#     stddev(\"label\").alias(\"std_los\")\n","# ).collect()[0]\n","\n","# print(f\"\\nğŸ“Š Final training set target statistics:\")\n","# print(f\"   ğŸ“‰ Min LOS: {train_stats['min_los']:.2f} days\")\n","# print(f\"   ğŸ“ˆ Max LOS: {train_stats['max_los']:.2f} days\") \n","# print(f\"   ğŸ“Š Mean LOS: {train_stats['mean_los']:.2f} days\")\n","# print(f\"   ğŸ“ Std LOS: {train_stats['std_los']:.2f} days\")\n","\n","print(f\"\\nâœ… Data preprocessing completed successfully!\")\n","print(f\"ğŸš€ Ready for model training with {len(feature_columns)} features\")\n","\n","\n","print(f\"â° Preprocessing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"f0c98beb-ae1b-445a-aa7f-9002b4d9a012","metadata":{},"source":["## Training Multiple Models"]},{"cell_type":"code","execution_count":null,"id":"eda7d1c4-c1ef-4261-a3b5-e05db415626d","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Setting up evaluation metrics...\")\n","\n","# Create regression evaluators\n","rmse_evaluator = RegressionEvaluator(\n","    labelCol=\"label\", \n","    predictionCol=\"prediction\", \n","    metricName=\"rmse\"\n",")\n","\n","mae_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\", \n","    metricName=\"mae\"\n",")\n","\n","r2_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\",\n","    metricName=\"r2\"\n",")\n","\n","print(\"âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\")"]},{"cell_type":"markdown","id":"b0f74323-98d0-44fd-b21f-f2e543449457","metadata":{},"source":["### Linear Regression"]},{"cell_type":"code","execution_count":null,"id":"4a25b20c-1f60-4782-b27c-6fda0121af85","metadata":{},"outputs":[],"source":["print(\"\\nğŸ“ˆ Step 2: Training Linear Regression model...\")\n","print(f\"ğŸ• Started at: {datetime.now().strftime('%H:%M:%S')}\")\n","start_time = time.time()\n","\n","# Create Linear Regression model\n","lr = LinearRegression(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    maxIter=200,                    # Increased for better convergence\n","    regParam=0.001,                 # Lower regularization for healthcare data\n","    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n","    tol=1e-8,                       # Tighter tolerance for precision\n","    standardization=False,          # We're doing manual scaling\n","    fitIntercept=True,\n","    aggregationDepth=3,             # Better for distributed training\n","    loss=\"squaredError\",\n","    solver=\"normal\"                 # Best for small-medium datasets\n",")\n","\n","\n","# Train the model\n","print(\"   ğŸ”„ Training Linear Regression...\")\n","lr_model = lr.fit(train_final)\n","\n","print(\"   ğŸ”„ Linear Regression - Making predictions (test data)...\")\n","lr_predictions = lr_model.transform(test_final)\n","\n","print(\"   ğŸ”„ Linear Regression - Evaluation...\")\n","lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n","lr_mae = mae_evaluator.evaluate(lr_predictions)\n","lr_r2 = r2_evaluator.evaluate(lr_predictions)\n","\n","print(f\"âœ… Linear Regression Results:\")\n","print(f\"   ğŸ“‰ RMSE: {lr_rmse:.3f} days\")\n","print(f\"   ğŸ“Š MAE: {lr_mae:.3f} days\")\n","print(f\"   ğŸ“ˆ RÂ²: {lr_r2:.3f}\")\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"ğŸ• Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n","print(f\"â±ï¸ Total elapsed time: {elapsed_time:.2f} seconds\")\n","\n","\n","# Linear Regression Predictions\n","\n","print(\"\\nğŸ“ˆ Linear Regression Predictions (Sample 20):\")\n","lr_display = lr_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","lr_display.show(20, truncate=False)"]},{"cell_type":"markdown","id":"6847f98b-4da8-41ff-b26f-f2943c8baa38","metadata":{},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"id":"52707ea4-eb6a-449d-8160-7013057b2c3f","metadata":{},"outputs":[],"source":["\n","print(\"\\nğŸŒ² Step 3: Training Random Forest model...\")\n","print(f\"ğŸ• Started at: {datetime.now().strftime('%H:%M:%S')}\")\n","start_time = time.time()\n","\n","# Create Random Forest model\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    numTrees=300,                   # More trees = better accuracy (if enough cores/memory)\n","    maxDepth=12,                    # Deeper trees capture more complexity\n","    minInstancesPerNode=2,          # Allows more granular splits\n","    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n","    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n","    maxBins=64,                     # More bins = better numeric split precision\n","    impurity=\"variance\",            # Required for regression\n","    maxMemoryInMB=512,              # Give more memory per node for splits\n","    cacheNodeIds=True,              # Improves tree building performance\n","    checkpointInterval=5,           # Frequent checkpoints = safer on big jobs\n","    seed=42                         # Reproducibility\n",")\n","\n","print(\"   ğŸ”„ Training Random Forest...\")\n","rf_model = rf.fit(train_final)\n","\n","print(\"   ğŸ”„ Random Forest - Making predictions (test data)...\")\n","rf_predictions = rf_model.transform(test_final)\n","\n","print(\"   ğŸ”„ Random Forest - Evaluation...\")\n","rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n","rf_mae = mae_evaluator.evaluate(rf_predictions)\n","rf_r2 = r2_evaluator.evaluate(rf_predictions)\n","\n","print(f\"âœ… Random Forest Results:\")\n","print(f\"   ğŸ“‰ RMSE: {rf_rmse:.3f} days\")\n","print(f\"   ğŸ“Š MAE: {rf_mae:.3f} days\")\n","print(f\"   ğŸ“ˆ RÂ²: {rf_r2:.3f}\")\n","\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"ğŸ• Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n","print(f\"â±ï¸ Total elapsed time: {elapsed_time:.2f} seconds\")\n","\n","\n","# Random Forest Predictions\n","print(\"\\nğŸŒ² Random Forest Predictions (Sample 20):\")\n","rf_display = rf_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","rf_display.show(20, truncate=False)\n"]},{"cell_type":"markdown","id":"5cc0d610-37c0-4c99-9fc0-289846b54033","metadata":{},"source":["## Model Comparison"]},{"cell_type":"code","execution_count":null,"id":"e3ff000d-87eb-4b26-976a-65db81b057f0","metadata":{},"outputs":[],"source":["print(\"\\nğŸ† Step 5: Model Performance Comparison...\")\n","\n","# Create comparison summary\n","results_data = [\n","    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n","    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n","]\n","\n","results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n","\n","print(\"ğŸ“Š Model Performance Summary:\")\n","results_df.show(truncate=False)\n","\n","# Find best model\n","import operator\n","import builtins\n","best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n","best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n","\n","print(f\"\\nğŸ¥‡ Best Models:\")\n","print(f\"   ğŸ¯ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n","print(f\"   ğŸ“ˆ Highest RÂ²: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"]},{"cell_type":"markdown","id":"400e166a-1c18-4e22-938c-d6a5569bd296","metadata":{},"source":["## Display Predictions"]},{"cell_type":"code","execution_count":null,"id":"068fa69e-a7fb-40f2-a287-6300bf41e03d","metadata":{},"outputs":[],"source":["\n","# Linear Regression Predictions\n","\n","print(\"\\nğŸ“ˆ Linear Regression Predictions (Sample 20):\")\n","lr_display = lr_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","lr_display.show(20, truncate=False)\n","\n","\n","\n","# Random Forest Predictions\n","print(\"\\nğŸŒ² Random Forest Predictions (Sample 20):\")\n","rf_display = rf_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","rf_display.show(20, truncate=False)"]},{"cell_type":"code","execution_count":null,"id":"343e733a","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}