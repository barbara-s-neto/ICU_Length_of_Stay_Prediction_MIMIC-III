{"cells":[{"cell_type":"markdown","id":"da1d1427","metadata":{},"source":["# ICU Length of Stay Prediction - MIMIC-III Pipeline\n","\n","## ğŸ¯ Objective\n","Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n","\n","## ğŸ“Š Data & Constraints\n","- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n","- **Filters**: \n","        - Patient Age 18-80\n","        - LOS 0.1-15 days\n","        - Valid time sequences\n","- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n","\n","\n","## ğŸŒ€ Big Data Processing\n","\n","- **Storage**: We used Google Cloud Dataproc and Google Storage Buckets for MIMIC-III storage \n","- **CHARTEVENTS**: Chart Events table has +330 million rows\n","- **Parquet**: Converted \"CHARTEVENTS\" and \"LABEVENTS\" tables to Parquet format for efficient storage and processing\n","- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n","\n","## ğŸ”§ Features (39 total)\n","- **Demographics (2)**: Age, gender\n","- **Admission (8)**: Emergency/elective, timing, insurance\n","- **ICU Units (6)**: Care unit types, transfers\n","- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n","- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n","- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n","\n","## ğŸ¤– Models & Results\n","- **Linear Regression**: \n","- **Random Forest**: \n","\n","## â˜ï¸ Infrastructure\n","- **GCP Dataproc**: 1x Master and 2x Workers, n2-standard-4  (12 vCPUs, 48GB RAM, 400GB Disk Storage)\n","- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n","\n","\n","\n"]},{"cell_type":"markdown","id":"f7c8375f-7f35-415f-8288-2ff2193e6af0","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"id":"89ed6638-09bf-4620-89fe-2bb2c00d86e6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… All imports loaded successfully!\n","â° Notebook started at: 2025-06-02 15:27:31\n"]}],"source":["# Core PySpark imports\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","\n","# Machine Learning imports\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n","from pyspark.ml.regression import RandomForestRegressor, LinearRegression #, GBTRegressor\n","#from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n","from pyspark.ml.evaluation import RegressionEvaluator #, MulticlassClassificationEvaluator\n","#from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","#from pyspark.ml import Pipeline\n","\n","from datetime import datetime, timedelta\n","import time\n","\n","print(\"âœ… All imports loaded successfully!\")\n","print(f\"â° Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"4f9d1810-67b2-471e-97d2-b8fda7db9728","metadata":{},"source":["## Setup Spark Session"]},{"cell_type":"code","execution_count":2,"id":"a15f1fca-5bf8-4e10-bdca-a6faa11ca17a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","25/06/02 15:27:36 INFO SparkEnv: Registering MapOutputTracker\n","25/06/02 15:27:36 INFO SparkEnv: Registering BlockManagerMaster\n","25/06/02 15:27:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/06/02 15:27:36 INFO SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["âœ… Spark session created successfully!\n","ğŸ“Š Spark Version: 3.5.3\n","ğŸ”§ Application Name: Forecast-LOS\n","ğŸ’¾ Available cores: 2\n","\n","â° Spark session initialised at: 2025-06-02 15:27:43\n"]}],"source":["spark = SparkSession.builder \\\n","    .appName(\"Forecast-LOS\") \\\n","    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n","    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n","    \\\n","    .config(\"spark.executor.memory\", \"5g\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .config(\"spark.executor.instances\", \"2\") \\\n","    \\\n","    .config(\"spark.driver.memory\", \"10g\") \\\n","    .config(\"spark.driver.cores\", \"3\") \\\n","    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","    \\\n","    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n","    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n","    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n","    .config(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"32MB\") \\\n","    \\\n","    .config(\"spark.network.timeout\", \"600s\") \\\n","    .config(\"spark.sql.broadcastTimeout\", \"300s\") \\\n","    .config(\"spark.rpc.askTimeout\", \"300s\") \\\n","    \\\n","    .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n","    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n","    \\\n","    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n","    \\\n","    .config(\"spark.executor.memoryOffHeap.enabled\", \"true\") \\\n","    .config(\"spark.executor.memoryOffHeap.size\", \"1g\") \\\n","    \\\n","    .getOrCreate()\n","print(\"âœ… Spark session created successfully!\")\n","print(f\"ğŸ“Š Spark Version: {spark.version}\")\n","print(f\"ğŸ”§ Application Name: {spark.sparkContext.appName}\")\n","print(f\"ğŸ’¾ Available cores: {spark.sparkContext.defaultParallelism}\")\n","print(f\"\\nâ° Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"7e13163e-55dc-4046-95b0-4815723d0581","metadata":{},"source":["# Load Data\n","\n","Strategy: Pre-filter CHARTEVENTS to find ICU stays with required vital signs, then efficiently load all tables using broadcast joins and lookup tables.\n","Key Steps:\n","\n","- Filter for ICU stays with â‰¥1 of 6 vital signs (HR, BP, RR, Temp, SpO2)\n","- Create lookup tables for ICUSTAY_ID, HADM_ID, SUBJECT_ID\n","- Load all tables with pre-filtering using broadcast joins\n","- Convert large files to \"Parquet\" for performance\n","\n","Result: Memory-efficient loading of only relevant data with quality assurance that all ICU stays have vital signs measurements."]},{"cell_type":"code","execution_count":3,"id":"62936c54-45ab-4c03-a8ec-fc3d87f68721","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ¥ Loading MIMIC-III data...\n","ğŸ“‚ Loading CHARTEVENTS...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/02 15:28:00 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:28:15 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:28:30 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:28:45 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:29:00 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:29:15 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:29:30 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:29:45 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:30:00 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:30:15 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","25/06/02 15:30:30 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","ERROR:root:KeyboardInterrupt while sending command.\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n","    answer = smart_decode(self.stream.readline()[:-1])\n","                          ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n","    return self._sock.recv_into(b)\n","           ^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","ERROR:root:KeyboardInterrupt while sending command.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_10376/2609999272.py\", line 14, in <module>\n","    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 544, in parquet\n","    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n","    answer = self.gateway_client.send_command(command)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n","    answer = smart_decode(self.stream.readline()[:-1])\n","                          ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n","    return self._sock.recv_into(b)\n","           ^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n","    answer = smart_decode(self.stream.readline()[:-1])\n","                          ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n","    return self._sock.recv_into(b)\n","           ^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n"]},{"name":"stdout","output_type":"stream","text":["ğŸ“„ Converting CHARTEVENTS.csv.gz to parquet...\n"]},{"name":"stderr","output_type":"stream","text":["Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe287c46690>>\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n","    def _clean_thread_parent_frames(\n","\n","KeyboardInterrupt: \n","Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe287c46690>>\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n","    def _clean_thread_parent_frames(\n","\n","KeyboardInterrupt: \n","Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe287c46690>>\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n","    def _clean_thread_parent_frames(\n","\n","KeyboardInterrupt: \n","\n","KeyboardInterrupt\n","\n"]}],"source":["# Configuration flags\n","SAMPLE_ENABLE = False\n","SAMPLE_SIZE = 20000\n","MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n","\n","\n","\n","print(\"ğŸ¥ Loading MIMIC-III data...\")\n","\n","# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n","print(\"ğŸ“‚ Loading CHARTEVENTS...\")\n","\n","try:\n","    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","    print(\"âœ… Loaded CHARTEVENTS from parquet\")\n","except:\n","    print(\"ğŸ“„ Converting CHARTEVENTS.csv.gz to parquet...\")\n","    chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n","    chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","    chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","    print(\"âœ… Converted and loaded CHARTEVENTS\")\n","\n","\n","\n","# Step 2: Load ICUSTAYS \n","print(\"\\nğŸ“‚ Loading and filtering ICUSTAYS...\")\n","icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n","\n","\n","\n","# Step 3: Apply sampling if enabled\n","if SAMPLE_ENABLE:\n","    print(f\"ğŸ¯ Sampling {SAMPLE_SIZE} ICU stays...\")\n","    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n","    icustays_df.cache()\n","    actual_sample_size = icustays_df.count()\n","    print(f\"âœ… Final sample: {actual_sample_size} ICU stays\")\n","else:\n","    icustays_df.cache()\n","    actual_sample_size = icustays_df.count()\n","\n","    \n","    \n","# Step 4: Create efficient lookup tables\n","print(\"ğŸ“‹ Creating ID lookup tables...\")\n","icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n","hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n","subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n","\n","icu_lookup.count()  # Trigger caching\n","hadm_lookup.count()\n","subject_lookup.count()\n","\n","# Step 5: Load other tables with optimized joins\n","print(\"ğŸ“‚ Loading PATIENTS table...\")\n","patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n","patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n","\n","print(\"ğŸ“‚ Loading ADMISSIONS table...\")\n","admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n","admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","print(\"ğŸ“‚ Loading DIAGNOSES_ICD table...\")\n","diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n","diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# Step 6: Load and filter CHARTEVENTS efficiently\n","print(\"ğŸ“‚ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n","chartevents_df = chartevents_df \\\n","    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n","    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n","\n","# Step 7: Load LABEVENTS\n","print(\"ğŸ“‚ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n","try:\n","    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","except:\n","    print(\"ğŸ“„ Converting LABEVENTS.csv.gz to parquet...\")\n","    labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n","    labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","    labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","\n","labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# Final summary\n","print(\"\\nâœ… Data loading complete!\")\n","print(f\"ğŸ“Š ICUSTAYS: {icustays_df.count():,} rows\")\n","print(f\"ğŸ“Š PATIENTS: {patients_df.count():,} rows\") \n","print(f\"ğŸ“Š ADMISSIONS: {admissions_df.count():,} rows\")\n","print(f\"ğŸ“Š DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n","print(f\"ğŸ“Š CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n","print(f\"ğŸ“Š LABEVENTS (filtered): {labevents_df.count():,} rows\")\n","print(f\"\\nâ° Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"9e8e7157-1230-41d3-8f41-1117b62fdb55","metadata":{},"source":["# Features Engineering\n","\n"]},{"cell_type":"markdown","id":"8763e6d7-4ba5-439b-8e2b-ba16cf607a3e","metadata":{},"source":["## Extracting Data From ICUSTAYS\n","\n","**Purpose**: Create comprehensive ICU dataset by joining ICU stays with patient demographics and admission details.\n","\n","**Key Features**:\n","- **Target Variable**: ICU_LOS_DAYS (length of stay)\n","- **Demographics**: Age (18-80), gender, ethnicity\n","- **Clinical**: Care units, admission type/location, insurance\n","- **Outcomes**: Hospital/patient death flags\n","- **Identifiers**: ICUSTAY_ID, SUBJECT_ID, HADM_ID\n","\n","**Age Filter**: Adults only (18-80 years) to exclude pediatric/very elderly edge cases.\n","\n","**Result**: Clean base dataset ready for vital signs feature engineering."]},{"cell_type":"code","execution_count":null,"id":"9f1d7adb-eac0-4071-b252-a04268f0c767","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Creating base ICU dataset with patient demographics...\")\n","\n","base_icu_df = icustays_df.alias(\"icu\") \\\n","    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n","    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n","    .select(\n","        # ICU stay identifiers\n","        col(\"icu.ICUSTAY_ID\"),\n","        col(\"icu.SUBJECT_ID\"), \n","        col(\"icu.HADM_ID\"),\n","        \n","        # Target variable - Length of Stay in ICU (days)\n","        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n","        \n","        # ICU characteristics\n","        col(\"icu.FIRST_CAREUNIT\"),\n","        col(\"icu.LAST_CAREUNIT\"), \n","        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n","        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n","        \n","        # Patient demographics\n","        col(\"pat.GENDER\"),\n","        col(\"pat.DOB\"),\n","        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n","        \n","        # Admission details\n","        col(\"adm.ADMITTIME\"),\n","        col(\"adm.DISCHTIME\"), \n","        col(\"adm.ADMISSION_TYPE\"),\n","        col(\"adm.ADMISSION_LOCATION\"),\n","        col(\"adm.INSURANCE\"),\n","        col(\"adm.ETHNICITY\"),\n","        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n","        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n","    )\n","\n","# Calculate age at ICU admission\n","base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n","                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n","                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n","\n","\n","print(\"âœ… Created base ICU dataset!\")\n"]},{"cell_type":"markdown","id":"595638e6-74f7-4f2f-a7e5-1fc692089206","metadata":{},"source":["## Extracting Categorical Features\n","\n","**Features Created**:\n","- **GENDER_BINARY**: Male = 1, Female = 0\n","- **CAME_FROM_ER**: Emergency admission = 1\n","- **HAS_INSURANCE**: Medicare = 1, other = 0\n","- **ADMISSION_TYPE_ENCODED**: Emergency=1, Elective=2, Urgent=3, Other=0\n","- **ETHNICITY_ENCODED**: White=1, Black=2, Hispanic=3, Asian=4, Other=5\n","\n","**Result**: Categorical variables converted to numerical format for ML models."]},{"cell_type":"code","execution_count":null,"id":"d28d34e6-83c3-40ae-95d0-71f9929397a1","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 2: Engineering categorical features...\")\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n","    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n","    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n","                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n","                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n","                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n","                .otherwise(0)) \\\n","    .withColumn(\"ETHNICITY_ENCODED\",\n","                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1) \\\n","                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2) \\\n","                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3) \\\n","                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4) \\\n","                .otherwise(5))\n","\n","print(\"âœ… Base ICU Dataset - Categorical Features\")\n"]},{"cell_type":"markdown","id":"9ca22bc8-8dd5-4d30-9cb9-0f366da21d76","metadata":{},"source":["## Extracting ICU Unit Types\n","\n","**Purpose**: Create categorical features for ICU unit types and transfers.\n","\n","**Features Created**:\n","- **FIRST_UNIT_ENCODED**: Numerical encoding of ICU units\n"," - MICU (Medical) = 1\n"," - SICU (Surgical) = 2  \n"," - CSRU (Cardiac Surgery) = 3\n"," - CCU (Coronary Care) = 4\n"," - TSICU (Trauma Surgical) = 5\n"," - Other = 0\n","- **CHANGED_ICU_UNIT**: Binary flag (1 if patient transferred between units)\n","\n","**Clinical Significance**: Different ICU types have varying complexity and typical LOS patterns. Unit transfers often indicate complications.\n","\n","**Result**: Enhanced dataset with ICU unit complexity and transfer indicators."]},{"cell_type":"code","execution_count":null,"id":"6617c3f2-75b9-4fa0-93a8-7161d0c2dd57","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 3: Creating ICU unit type features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"FIRST_UNIT_ENCODED\", \n","                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n","                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n","                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n","                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n","                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n","                .otherwise(0)) \\\n","    .withColumn(\"CHANGED_ICU_UNIT\", \n","                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n","\n","\n","print(\"âœ… Base ICU Dataset - Unit Type Features\")"]},{"cell_type":"markdown","id":"74662ab6-f263-402e-913f-dc04804eadff","metadata":{},"source":["## Extracting Time-based Features\n","\n","**Action**: Filter out invalid records where INTIME >= OUTTIME.\n"]},{"cell_type":"code","execution_count":null,"id":"ef431386-85f1-4867-9aa1-f5bb84d7c8ae","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 4: Creating time-based features...\")\n","base_icu_df = base_icu_df \\\n","    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n","print(\"âœ… Base ICU Dataset - Time Based Features\")"]},{"cell_type":"markdown","id":"779ac96b-6e5f-4fdd-84a2-043a0beaa06b","metadata":{},"source":["## Remove Outliers (Excessive Length Of Stay)"]},{"cell_type":"code","execution_count":null,"id":"b6ce0e40-182e-4fbd-bb35-2bb23ed0d0de","metadata":{},"outputs":[],"source":["print(\"\\nğŸ“ˆ ICU Length of Stay Statistics (Days):\")\n","base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()"]},{"cell_type":"markdown","id":"bf4867ef-642f-4ebe-b1f4-f5a85ddf3e64","metadata":{},"source":["We kept every ICU STAY that had duration (LOS) between 0.0 and 9.1 days, considered normal legnths since:\n","\n","| Statistic                | Value (days)                                    |\n","| ------------------------ | ----------------------------------------------- |\n","| **Minimum**              | 0.0 (can be admission + discharge on same day)  |\n","| **25th percentile (Q1)** | \\~1.1                                           |\n","| **Median (Q2)**          | \\~2.1                                           |\n","| **75th percentile (Q3)** | \\~4.3                                           |\n","| **Maximum**              | \\~88 (but can go slightly higher in edge cases) |\n","| **Mean**                 | \\~3.3â€“3.5                                       |\n","\n","Using interquartile range (IQR) method:\n","\n","* IQR = Q3 - Q1 = 4.3 - 1.1 = ~3.2\n","\n","* Upper Bound for outliers = Q3 + 1.5 Ã— IQR â‰ˆ 4.3 + 4.8 = ~9.1 days\n","\n","* Lower Bound = Q1 - 1.5 Ã— IQR â‰ˆ 1.1 - 4.8 = < 0, which is ignored since LOS canâ€™t be negative\n","\n","So:\n","\n","* Typical ICU LOS: 1.1 to 4.3 days\n","\n","* Outliers: ICU stays longer than ~9.1 days"]},{"cell_type":"code","execution_count":null,"id":"534ab904-5a6a-4f71-899e-e6b49620b0d2","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 5: Cleaning target variable...\")\n","\n","# List of desired columns\n","selected_columns = [\n","    \"ICUSTAY_ID\", \"SUBJECT_ID\", \"HADM_ID\", \"ICU_LOS_DAYS\", \"ICU_INTIME\", \"ICU_OUTTIME\", \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\", \"CAME_FROM_ER\",\n","    \"HAS_INSURANCE\", \"ADMISSION_TYPE_ENCODED\", \"ETHNICITY_ENCODED\",\n","    \"FIRST_UNIT_ENCODED\", \"CHANGED_ICU_UNIT\"\n","]\n","\n","# Apply filter and select columns\n","base_icu_df = base_icu_df \\\n","    .filter(col(\"ICU_LOS_DAYS\").between(0.0, 9.1)) \\\n","    .select(*selected_columns) \\\n","    .cache()\n","\n","print(\"âœ… Base ICU Dataset - Remove Outliers\")\n","\n","print(\"\\nğŸ“‹ Sample of ICU stay records:\")\n","base_icu_df.show(5)"]},{"cell_type":"markdown","id":"268129f5-72e2-40a5-b676-cbd825ae84c8","metadata":{},"source":["## Extracting Clinical Features\n","\n","**Purpose**: Extract top 20 most common CHARTEVENTS as features for ML models.\n","\n","**Process**:\n","1. **Identify**: Find 20 most frequent CHARTEVENTS (typically vital signs)\n","2. **Calculate**: Average value of each test in first 24 hours of ICU stay\n","3. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n","\n","**Time Window**: First 24 hours after ICU admission (INTIME + 24h)\n","\n","**Result**: 20 vital signs features with consistent **-1** encoding for missing data, ensuring ML algorithm compatibility.\n"]},{"cell_type":"code","execution_count":null,"id":"9dc92782-4f16-4c82-bd8a-44ed78e92965","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Identifying top 20 most frequent tests from CHARTEVENTS...\")\n","\n","\n","# Get frequency count of each ITEMID in CHARTEVENTS\n","itemid_counts = chartevents_df \\\n","    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"CHARTTIME\").isNotNull()) \\\n","    .groupBy(\"ITEMID\") \\\n","    .count() \\\n","    .orderBy(col(\"count\").desc()) \\\n","    .limit(20) \\\n","    .collect()\n","\n","# Create mapping dictionary for top 20 items\n","top_20_items = {row[\"ITEMID\"]: f\"VITAL_{row['ITEMID']}\" for row in itemid_counts}\n","print(f\"ğŸ¯ Top 20 chart items selected: {top_20_items}\")\n","\n","print(\"ğŸ“Š Filtering CHARTEVENTS for top 20 items...\")\n","\n","chartevents_top20 = chartevents_df \\\n","    .filter(col(\"ITEMID\").isin(list(top_20_items.keys()))) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n","    .filter(col(\"CHARTTIME\").isNotNull()) \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(col(\"CHARTTIME\").between(col(\"ICU_INTIME\"), col(\"ICU_OUTTIME\"))) \\\n","    .select(\"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\")\n","\n","# Process first 24 hours\n","vitals_24h_top20 = chartevents_top20.alias(\"ce\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(\n","        col(\"ce.CHARTTIME\").between(\n","            col(\"ICU_INTIME\"), \n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","print(\"ğŸ“Š Calculating aggregates for top 20 vitals...\")\n","\n","# Initialize with ICUSTAY_ID\n","vitals_features_top20 = base_icu_df.select(\"ICUSTAY_ID\")\n","\n","# Process each vital sign\n","for itemid, name in top_20_items.items():\n","    #print(f\"Processing {name} (ITEMID={itemid})...\")\n","    \n","    vital_stats = vitals_24h_top20 \\\n","        .filter(col(\"ITEMID\") == itemid) \\\n","        .groupBy(\"ICUSTAY_ID\") \\\n","        .agg(avg(\"VALUENUM\").alias(f\"{name}_AVG\"))\n","    \n","    # Left join (without filling NULLs yet)\n","    vitals_features_top20 = vitals_features_top20.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n","\n","# Fill ALL NULL values with -1 AFTER all joins are done\n","vitals_features_top20 = vitals_features_top20.na.fill(-1)\n","\n","# Cleanup\n","chartevents_df.unpersist()\n","vitals_24h_top20.unpersist()\n","\n","# Verify no NULLs remain\n","print(f\"âœ… Created {len(top_20_items)} features from top 20 vital signs (NULLs replaced with -1)\")\n","vitals_features_top20.show(5)"]},{"cell_type":"markdown","id":"7c08c810-405c-48ca-a141-1a48e125635c","metadata":{},"source":["## Laboratory Events\n","\n","**Purpose**: Extract top 20 most common lab tests as features for ML models.\n","\n","**Process**:\n","1. **Identify**: Find 20 most frequent LABEVENTS (blood tests, chemistry panels)\n","2. **Time Window**: 6 hours before ICU admission + first 24 hours in ICU (30h total)\n","3. **Calculate**: Average value of each lab test within the 30-hour window\n","4. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n","\n","**Time Range**: ICU_INTIME - 6h to ICU_INTIME + 24h\n","\n","**Result**: 20 lab test features with consistent -1 encoding for missing data, capturing pre-ICU and early ICU clinical status."]},{"cell_type":"code","execution_count":null,"id":"5b685f0e-f499-4adf-9fd9-a6f92e22a1cf","metadata":{},"outputs":[],"source":["print(\"\\nğŸ§ª Creating laboratory features from LABEVENTS...\")\n","\n","# Step 1: Identify top 20 most frequent lab items\n","print(\"ğŸ“Š Identifying top 20 most frequent lab items...\")\n","top_20_lab_items = labevents_df \\\n","    .filter(col(\"HADM_ID\").isin([row[\"HADM_ID\"] for row in base_icu_df.select(\"HADM_ID\").collect()])) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\") > 0) \\\n","    .groupBy(\"ITEMID\") \\\n","    .count() \\\n","    .orderBy(col(\"count\").desc()) \\\n","    .limit(20) \\\n","    .collect()\n","\n","# Create mapping dictionary (using ITEMID as name if no mapping exists)\n","lab_items = {row[\"ITEMID\"]: f\"LAB_{row['ITEMID']}\" for row in top_20_lab_items}\n","print(f\"ğŸ¯ Top 20 lab items selected: {list(lab_items.keys())}\")\n","\n","# Step 2: Filter lab events within first 24 hours of ICU stay\n","print(\"ğŸ“Š Filtering LABEVENTS for top 20 items...\")\n","labs_24h = labevents_df.alias(\"le\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n","    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n","    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n","    .filter(col(\"le.VALUENUM\") > 0) \\\n","    .filter(\n","        col(\"le.CHARTTIME\").between(\n","            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","# Step 3: Calculate lab statistics with NULL handling\n","print(\"ğŸ“Š Calculating laboratory statistics...\")\n","labs_features = base_icu_df.select(\"ICUSTAY_ID\")\n","\n","for itemid, name in lab_items.items():\n","    #print(f\"   Processing {name} (ITEMID={itemid})...\")\n","    \n","    item_stats = labs_24h \\\n","        .filter(col(\"ITEMID\") == itemid) \\\n","        .groupBy(\"ICUSTAY_ID\") \\\n","        .agg(\n","            coalesce(avg(\"VALUENUM\"), lit(-1)).alias(f\"{name}_AVG\")\n","        )\n","    \n","    labs_features = labs_features.join(item_stats, \"ICUSTAY_ID\", \"left\")\n","\n","# Final NULL fill as safeguard (though coalesce should have handled it)\n","labs_features = labs_features.na.fill(-1)\n","\n","# Cleanup\n","labevents_df.unpersist()\n","labs_24h.unpersist()\n","\n","print(f\"âœ… Created {len(lab_items)} lab features for {labs_features.count():,} ICU stays\")\n","\n","# Show sample of features\n","print(\"ğŸ“Š Sample features:\")\n","labs_features.show(5, truncate=False)"]},{"cell_type":"markdown","id":"a43cd034-b129-46b0-b33f-cf903ee438ae","metadata":{},"source":["## Diagnosis ICD\n","\n","**Purpose**: Extract diagnosis patterns as ML features from ICD-9 codes.\n","\n","**Process**:\n","1. **Identify**: Top 10 most frequent ICD-9 diagnosis codes\n","2. **Count**: Total diagnoses per admission (comorbidity burden)\n","3. **Create**: Binary features for each top 10 diagnosis (HAS_[CODE])\n","\n","**Features Created**:\n","- **TOTAL_DIAGNOSES**: Count of all diagnoses (comorbidity indicator)\n","- **HAS_[ICD9_CODE]**: Binary flags for top 10 most common diagnoses\n","\n","**Result**: 11 diagnosis features (1 count + 10 binary) capturing disease complexity and specific conditions."]},{"cell_type":"code","execution_count":null,"id":"94a414c1-7139-4e27-a5e4-767919a3eead","metadata":{},"outputs":[],"source":["print(\"\\nğŸ¥ Creating diagnosis features from ICD codes...\")\n","\n","# Step 1: Identify top 10 most frequent ICD9 codes\n","print(\"ğŸ“Š Identifying top 10 most frequent diagnoses...\")\n","top_10_diagnoses = diagnoses_df \\\n","    .groupBy(\"ICD9_CODE\") \\\n","    .count() \\\n","    .orderBy(col(\"count\").desc()) \\\n","    .limit(10) \\\n","    .collect()\n","\n","top_10_codes = [row[\"ICD9_CODE\"] for row in top_10_diagnoses]\n","print(f\"ğŸ¯ Top 10 ICD9 codes: {top_10_codes}\")\n","\n","# import spark fn instead builtin\n","from pyspark.sql.functions import count as spark_count, collect_list\n","\n","# Step 2: Count total diagnoses per admission (comorbidity burden)\n","diagnosis_features = diagnoses_df.groupBy(\"HADM_ID\") \\\n","    .agg(\n","        spark_count(\"ICD9_CODE\").alias(\"TOTAL_DIAGNOSES\"),\n","        collect_list(\"ICD9_CODE\").alias(\"DIAGNOSIS_CODES\")\n","    )\n","\n","# Step 3: Create binary features for top 10 diagnoses\n","for code in top_10_codes:\n","    diagnosis_features = diagnosis_features.withColumn(\n","        f\"HAS_{code}\",\n","        when(array_contains(col(\"DIAGNOSIS_CODES\"), code), 1).otherwise(0)\n","    )\n","\n","# Drop the raw codes list\n","diagnosis_features = diagnosis_features.drop(\"DIAGNOSIS_CODES\")\n","\n","print(f\"âœ… Created {len(top_10_codes)} diagnosis features for {diagnosis_features.count():,} admissions\")\n","\n","# Show sample of features\n","print(\"ğŸ“Š Sample features:\")\n","diagnosis_features.show(5, truncate=False)\n","\n","print(f\"\\nâ° Clinical features completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"b27d9140-230d-4bc0-91ae-fd9db043e5a5","metadata":{},"source":["# Joining All Features"]},{"cell_type":"code","execution_count":null,"id":"1fe76c11-fb64-4b12-9a37-8efccf76d55d","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Joining all features...\")\n","\n","# Start with base ICU dataset and join all features\n","final_dataset = base_icu_df \\\n","    .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n","    .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n","    .join(diagnosis_features, \"HADM_ID\", \"left\")\n","\n","# Cleanup\n","base_icu_df.unpersist()\n","vitals_features_top20.unpersist()\n","labs_features.unpersist()\n","diagnosis_features.unpersist()\n","\n","print(f\"âœ… All features joined! Final dataset has {final_dataset.count()} records\")\n","print(\"ğŸ“Š Sample of final dataset:\")\n","final_dataset.show(5, truncate=False)"]},{"cell_type":"code","execution_count":null,"id":"88f6f328-17d8-40a6-ba45-769f22203346","metadata":{},"outputs":[],"source":["print(\"\\nğŸ“‹ Step 3: Selecting final features for regression modeling...\")\n","\n","# Define feature columns for modeling\n","\n","feature_columns = final_dataset.columns\n","print(\"\\nğŸ“‹ Features - Final Dataset Columns:\")\n","print(feature_columns)\n","\n","\n","# Create modeling dataset with selected features\n","modeling_dataset = final_dataset.select(\n","    [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\"] + feature_columns\n",")\n","\n","# Remove any remaining nulls and invalid records\n","modeling_dataset = modeling_dataset.filter(col(\"ICU_LOS_DAYS\").isNotNull()) \\\n","    .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n","    .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80))\n","\n","# Cache the final dataset\n","#modeling_dataset = modeling_dataset.repartition()\n","modeling_dataset.cache()\n","\n","\n","print(f\"âœ… Final modeling dataset prepared!\")\n","print(f\"ğŸ“ Final dataset: {modeling_dataset.count():,} ICU stays\")\n","print(f\"ğŸ“Š Total features: {len(feature_columns)} predictive features\")\n","print(f\"ğŸ¯ Target variable: ICU_LOS_DAYS (continuous)\")\n","\n","\n","# Display sample of final dataset\n","#print(f\"\\nğŸ“‹ Sample of final modeling dataset:\")\n","#modeling_dataset.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"AGE_AT_ICU_ADMISSION\", \n","#                        \"HEART_RATE_AVG\", \"CREATININE_FIRST\", \"HAS_SEPSIS\").show(5)\n","\n","# Basic statistics of target variable\n","#print(f\"\\nğŸ“ˆ Final ICU Length of Stay Statistics:\")\n","#modeling_dataset.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","print(f\"\\nâ° Dataset preparation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(f\"ğŸš€ Ready for train/test split and model training!\")"]},{"cell_type":"markdown","id":"5fa7f975-668b-4ab3-a228-4af82187778e","metadata":{},"source":["## Preparing for Machine Learning"]},{"cell_type":"code","execution_count":null,"id":"50fa7d59-2d01-4dcf-868f-900431b44be9","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Creating train/test split...\")\n","\n","# Split the data (80% train, 20% test)\n","train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=42)\n","\n","# Cache both datasets for performance\n","train_data.cache()\n","test_data.cache()\n","\n","\n","print(f\"âœ… Data split completed:\")\n","#print(f\"   ğŸ“ˆ Training set: {train_data.count():,} ICU stays ({train_data.count()/modeling_dataset.count()*100:.1f}%)\")\n","#print(f\"   ğŸ“Š Test set: {test_data.count():,} ICU stays ({test_data.count()/modeling_dataset.count()*100:.1f}%)\")\n","\n","# Show target variable distribution in both sets\n","#print(f\"\\nğŸ“ˆ Target variable distribution:\")\n","#print(f\"Training set LOS statistics:\")\n","#train_data.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","#print(f\"Test set LOS statistics:\")\n","#test_data.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","# ============================================================================\n","# FEATURE VECTOR ASSEMBLY\n","# ============================================================================\n","\n","print(\"\\nğŸ”§ Step 2: Assembling feature vectors...\")\n","\n","# Create feature vector assembler\n","feature_assembler = VectorAssembler(\n","    inputCols=feature_columns,\n","    outputCol=\"features_raw\"\n",")\n","\n","# Apply feature assembler to training data\n","train_assembled = feature_assembler.transform(train_data)\n","test_assembled = feature_assembler.transform(test_data)\n","\n","\n","train_data.unpersist()\n","test_data.unpersist()\n","\n","\n","print(f\"âœ… Feature vectors assembled:\")\n","print(f\"   ğŸ“Š Feature vector size: {len(feature_columns)} dimensions\")\n","\n","# ============================================================================\n","# FEATURE SCALING\n","# ============================================================================\n","\n","print(\"\\nâš–ï¸ Step 3: Scaling features...\")\n","\n","# Create StandardScaler to normalize features\n","scaler = StandardScaler(\n","    inputCol=\"features_raw\",\n","    outputCol=\"features\",\n","    withStd=True,\n","    withMean=True\n",")\n","\n","# Fit scaler on training data\n","scaler_model = scaler.fit(train_assembled)\n","train_scaled = scaler_model.transform(train_assembled)\n","test_scaled = scaler_model.transform(test_assembled)\n","\n","# Cache the final processed datasets\n","train_scaled.cache()\n","test_scaled.cache()\n","\n","\n","print(f\"âœ… Feature scaling completed:\")\n","print(f\"   ğŸ“Š Features standardized (mean=0, std=1)\")\n","print(f\"   ğŸ”§ Scaler fitted on training data only\")\n"]},{"cell_type":"markdown","id":"8b1aa2af-6e65-4688-84ac-724a79f8ba00","metadata":{},"source":["## Final Dataset Preparation"]},{"cell_type":"code","execution_count":null,"id":"c2397a40-5d63-475b-a2dc-4b9f2fe4233b","metadata":{},"outputs":[],"source":["\n","print(\"\\nğŸ“‹ Step 4: Preparing final ML datasets...\")\n","\n","# Select columns needed for modeling\n","ml_columns = [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features\"]\n","\n","train_final = train_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","test_final = test_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","\n","#train_final = train_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","#test_final = test_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","\n","\n","# train_final = train_assembled.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features_raw\") \\\n","#     .withColumnRenamed(\"ICU_LOS_DAYS\", \"label\") \\\n","#     .withColumnRenamed(\"features_raw\", \"features\")\n","\n","# test_final = test_assembled.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features_raw\") \\\n","#     .withColumnRenamed(\"ICU_LOS_DAYS\", \"label\") \\\n","#     .withColumnRenamed(\"features_raw\", \"features\")\n","\n","\n","\n","\n","print(\"\\nğŸ“‹ Caching...\")\n","\n","# Cache final datasets\n","train_final.cache()\n","test_final.cache()\n","\n","print(f\"âœ… Final ML datasets prepared:\")\n","print(f\"   ğŸ¯ Target variable: 'label' (ICU_LOS_DAYS)\")\n","print(f\"   ğŸ“Š Features: 'features' (scaled vector)\")\n","print(f\"   ğŸ”‘ Identifier: 'ICUSTAY_ID'\")\n","\n","# Show sample of final datasets\n","print(f\"\\nğŸ“‹ Sample of training data structure:\")\n","train_final.select(\"ICUSTAY_ID\", \"label\").show(3)\n","\n","print(f\"\\nğŸ“‹ Feature vector example (first 10 features):\")\n","# Show first few elements of feature vector for one sample\n","sample_features = train_final.select(\"features\").take(1)[0][\"features\"]\n","print(f\"   ğŸ“Š Feature vector sample: {sample_features.toArray()[:10]}...\")\n","print(f\"   ğŸ“ Total feature dimensions: {len(sample_features.toArray())}\")\n","\n","# ============================================================================\n","# DATA QUALITY CHECKS\n","# ============================================================================\n","\n","print(f\"\\nğŸ” Step 5: Final data quality checks...\")\n","\n","# Check for any remaining nulls\n","train_nulls = train_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n","test_nulls = test_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n","\n","print(f\"   ğŸ” Null values in training set: {train_nulls}\")\n","print(f\"   ğŸ” Null values in test set: {test_nulls}\")\n","\n","# Show target variable ranges\n","# train_stats = train_final.agg(\n","#     min(\"label\").alias(\"min_los\"),\n","#     max(\"label\").alias(\"max_los\"), \n","#     avg(\"label\").alias(\"mean_los\"),\n","#     stddev(\"label\").alias(\"std_los\")\n","# ).collect()[0]\n","\n","# print(f\"\\nğŸ“Š Final training set target statistics:\")\n","# print(f\"   ğŸ“‰ Min LOS: {train_stats['min_los']:.2f} days\")\n","# print(f\"   ğŸ“ˆ Max LOS: {train_stats['max_los']:.2f} days\") \n","# print(f\"   ğŸ“Š Mean LOS: {train_stats['mean_los']:.2f} days\")\n","# print(f\"   ğŸ“ Std LOS: {train_stats['std_los']:.2f} days\")\n","\n","print(f\"\\nâœ… Data preprocessing completed successfully!\")\n","print(f\"ğŸš€ Ready for model training with {len(feature_columns)} features\")\n","\n","\n","print(f\"â° Preprocessing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"f0c98beb-ae1b-445a-aa7f-9002b4d9a012","metadata":{},"source":["## Training Multiple Models"]},{"cell_type":"code","execution_count":null,"id":"eda7d1c4-c1ef-4261-a3b5-e05db415626d","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Setting up evaluation metrics...\")\n","\n","# Create regression evaluators\n","rmse_evaluator = RegressionEvaluator(\n","    labelCol=\"label\", \n","    predictionCol=\"prediction\", \n","    metricName=\"rmse\"\n",")\n","\n","mae_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\", \n","    metricName=\"mae\"\n",")\n","\n","r2_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\",\n","    metricName=\"r2\"\n",")\n","\n","print(\"âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\")"]},{"cell_type":"markdown","id":"b0f74323-98d0-44fd-b21f-f2e543449457","metadata":{},"source":["### Linear Regression"]},{"cell_type":"code","execution_count":null,"id":"4a25b20c-1f60-4782-b27c-6fda0121af85","metadata":{},"outputs":[],"source":["print(\"\\nğŸ“ˆ Step 2: Training Linear Regression model...\")\n","print(f\"ğŸ• Started at: {datetime.now().strftime('%H:%M:%S')}\")\n","start_time = time.time()\n","\n","# Create Linear Regression model\n","lr = LinearRegression(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    maxIter=200,                    # Increased for better convergence\n","    regParam=0.001,                 # Lower regularization for healthcare data\n","    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n","    tol=1e-8,                       # Tighter tolerance for precision\n","    standardization=False,          # We're doing manual scaling\n","    fitIntercept=True,\n","    aggregationDepth=3,             # Better for distributed training\n","    loss=\"squaredError\",\n","    solver=\"normal\"                 # Best for small-medium datasets\n",")\n","\n","\n","# Train the model\n","print(\"   ğŸ”„ Training Linear Regression...\")\n","lr_model = lr.fit(train_final)\n","\n","print(\"   ğŸ”„ Linear Regression - Making predictions (test data)...\")\n","lr_predictions = lr_model.transform(test_final)\n","\n","print(\"   ğŸ”„ Linear Regression - Evaluation...\")\n","lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n","lr_mae = mae_evaluator.evaluate(lr_predictions)\n","lr_r2 = r2_evaluator.evaluate(lr_predictions)\n","\n","print(f\"âœ… Linear Regression Results:\")\n","print(f\"   ğŸ“‰ RMSE: {lr_rmse:.3f} days\")\n","print(f\"   ğŸ“Š MAE: {lr_mae:.3f} days\")\n","print(f\"   ğŸ“ˆ RÂ²: {lr_r2:.3f}\")\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"ğŸ• Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n","print(f\"â±ï¸ Total elapsed time: {elapsed_time:.2f} seconds\")\n","\n","\n","# Linear Regression Predictions\n","\n","print(\"\\nğŸ“ˆ Linear Regression Predictions (Sample 20):\")\n","lr_display = lr_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","lr_display.show(20, truncate=False)"]},{"cell_type":"markdown","id":"6847f98b-4da8-41ff-b26f-f2943c8baa38","metadata":{},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"id":"52707ea4-eb6a-449d-8160-7013057b2c3f","metadata":{},"outputs":[],"source":["\n","print(\"\\nğŸŒ² Step 3: Training Random Forest model...\")\n","print(f\"ğŸ• Started at: {datetime.now().strftime('%H:%M:%S')}\")\n","start_time = time.time()\n","\n","# Create Random Forest model\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    numTrees=200,                   # More trees = better accuracy (if enough cores/memory)\n","    maxDepth=12,                    # Deeper trees capture more complexity\n","    minInstancesPerNode=2,          # Allows more granular splits\n","    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n","    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n","    seed=42                         # Reproducibility\n",")\n","\n","print(\"   ğŸ”„ Training Random Forest...\")\n","rf_model = rf.fit(train_final)\n","\n","print(\"   ğŸ”„ Random Forest - Making predictions (test data)...\")\n","rf_predictions = rf_model.transform(test_final)\n","\n","print(\"   ğŸ”„ Random Forest - Evaluation...\")\n","rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n","rf_mae = mae_evaluator.evaluate(rf_predictions)\n","rf_r2 = r2_evaluator.evaluate(rf_predictions)\n","\n","print(f\"âœ… Random Forest Results:\")\n","print(f\"   ğŸ“‰ RMSE: {rf_rmse:.3f} days\")\n","print(f\"   ğŸ“Š MAE: {rf_mae:.3f} days\")\n","print(f\"   ğŸ“ˆ RÂ²: {rf_r2:.3f}\")\n","\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"ğŸ• Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n","print(f\"â±ï¸ Total elapsed time: {elapsed_time:.2f} seconds\")\n","\n","\n","# Random Forest Predictions\n","print(\"\\nğŸŒ² Random Forest Predictions (Sample 20):\")\n","rf_display = rf_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","rf_display.show(20, truncate=False)\n"]},{"cell_type":"markdown","id":"5cc0d610-37c0-4c99-9fc0-289846b54033","metadata":{},"source":["## Model Comparison"]},{"cell_type":"code","execution_count":null,"id":"e3ff000d-87eb-4b26-976a-65db81b057f0","metadata":{},"outputs":[],"source":["print(\"\\nğŸ† Step 5: Model Performance Comparison...\")\n","\n","# Create comparison summary\n","results_data = [\n","    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n","    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n","]\n","\n","results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n","\n","print(\"ğŸ“Š Model Performance Summary:\")\n","results_df.show(truncate=False)\n","\n","# Find best model\n","import operator\n","import builtins\n","best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n","best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n","\n","print(f\"\\nğŸ¥‡ Best Models:\")\n","print(f\"   ğŸ¯ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n","print(f\"   ğŸ“ˆ Highest RÂ²: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"]},{"cell_type":"markdown","id":"400e166a-1c18-4e22-938c-d6a5569bd296","metadata":{},"source":["## Display Predictions"]},{"cell_type":"code","execution_count":null,"id":"068fa69e-a7fb-40f2-a287-6300bf41e03d","metadata":{},"outputs":[],"source":["\n","# Linear Regression Predictions\n","\n","print(\"\\nğŸ“ˆ Linear Regression Predictions (Sample 20):\")\n","lr_display = lr_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","lr_display.show(20, truncate=False)\n","\n","\n","\n","# Random Forest Predictions\n","print(\"\\nğŸŒ² Random Forest Predictions (Sample 20):\")\n","rf_display = rf_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","rf_display.show(20, truncate=False)"]},{"cell_type":"code","execution_count":null,"id":"343e733a","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}