{"cells":[{"cell_type":"markdown","id":"da1d1427","metadata":{},"source":["# ICU Length of Stay Prediction - MIMIC-III Pipeline\n","\n","## 🎯 Objective\n","Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n","\n","## 📊 Data & Constraints\n","- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n","- **Filters**: \n","        - Patient Age 18-80\n","        - LOS 0.1-15 days\n","        - Valid time sequences\n","- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n","\n","\n","## 🌀 Big Data Processing\n","\n","- **Storage**: We used Google Cloud Dataproc and Google Storage Buckets for MIMIC-III storage \n","- **CHARTEVENTS**: Chart Events table has +330 million rows\n","- **Parquet**: Converted \"CHARTEVENTS\" and \"LABEVENTS\" tables to Parquet format for efficient storage and processing\n","- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n","\n","## 🔧 Features (39 total)\n","- **Demographics (2)**: Age, gender\n","- **Admission (8)**: Emergency/elective, timing, insurance\n","- **ICU Units (6)**: Care unit types, transfers\n","- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n","- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n","- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n","\n","## 🤖 Models & Results\n","- **Linear Regression**: \n","- **Random Forest**: \n","\n","## ☁️ Infrastructure\n","- **GCP Dataproc**: 1x Master and 2x Workers, n2-standard-4  (12 vCPUs, 48GB RAM, 400GB Disk Storage)\n","- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n","\n","\n","\n"]},{"cell_type":"markdown","id":"57a166c4","metadata":{},"source":["## Cenas a acresentar no relatorio:\n","\n","* justificar o pq de cada uma das colunas\n","* dar tune aos hiperparametros do modelo\n","* referencias e bibliografias :\n","    *"]},{"cell_type":"markdown","id":"f7c8375f-7f35-415f-8288-2ff2193e6af0","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"id":"89ed6638-09bf-4620-89fe-2bb2c00d86e6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","✅ All imports loaded successfully!\n","⏰ Notebook started at: 2025-06-06 20:36:32\n","\n"]}],"source":["# 📦 PySpark Core Imports\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","\n","\n","# 🔢 Data Processing & Feature Engineering\n","from pyspark.ml.feature import (\n","    VectorAssembler,\n","    StandardScaler,\n","    StringIndexer,\n","    MinMaxScaler,\n","    Imputer\n",")\n","from pyspark.ml.functions import vector_to_array\n","\n","\n","# 🤖 Machine Learning Models\n","from pyspark.ml.regression import (\n","    RandomForestRegressor,\n","    LinearRegression\n","    # GBTRegressor\n",")\n","\n","\n","# 📊 Model Evaluation & Tuning\n","from pyspark.ml.evaluation import RegressionEvaluator\n","# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.ml import Pipeline\n","import operator\n","import builtins\n","\n","\n","# ⏱️ Date/Time Utilities\n","from datetime import datetime, timedelta\n","import time\n","\n","\n","print(\"\\n✅ All imports loaded successfully!\")\n","print(f\"⏰ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"]},{"cell_type":"markdown","id":"4f9d1810-67b2-471e-97d2-b8fda7db9728","metadata":{},"source":["## Setup Spark Session"]},{"cell_type":"code","execution_count":2,"id":"a15f1fca-5bf8-4e10-bdca-a6faa11ca17a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","25/06/06 20:36:36 INFO SparkEnv: Registering MapOutputTracker\n","25/06/06 20:36:36 INFO SparkEnv: Registering BlockManagerMaster\n","25/06/06 20:36:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/06/06 20:36:36 INFO SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["✅ Spark session created successfully!\n","📊 Spark Version: 3.5.3\n","🔧 Application Name: Forecast-LOS\n","💾 Available cores: 2\n","\n","⏰ Spark session initialised at: 2025-06-06 20:36:44\n"]}],"source":["spark = SparkSession.builder \\\n","    .appName(\"Forecast-LOS\") \\\n","    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n","    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n","    \\\n","    .config(\"spark.executor.memory\", \"5g\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .config(\"spark.executor.instances\", \"2\") \\\n","    \\\n","    .config(\"spark.driver.memory\", \"10g\") \\\n","    .config(\"spark.driver.cores\", \"3\") \\\n","    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","    \\\n","    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n","    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n","    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n","    .config(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"32MB\") \\\n","    \\\n","    .config(\"spark.network.timeout\", \"600s\") \\\n","    .config(\"spark.sql.broadcastTimeout\", \"300s\") \\\n","    .config(\"spark.rpc.askTimeout\", \"300s\") \\\n","    \\\n","    .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n","    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n","    \\\n","    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n","    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n","    \\\n","    .config(\"spark.executor.memoryOffHeap.enabled\", \"true\") \\\n","    .config(\"spark.executor.memoryOffHeap.size\", \"1g\") \\\n","    \\\n","    .getOrCreate()\n","print(\"✅ Spark session created successfully!\")\n","print(f\"📊 Spark Version: {spark.version}\")\n","print(f\"🔧 Application Name: {spark.sparkContext.appName}\")\n","print(f\"💾 Available cores: {spark.sparkContext.defaultParallelism}\")\n","print(f\"\\n⏰ Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"7e13163e-55dc-4046-95b0-4815723d0581","metadata":{},"source":["# Load Data"]},{"cell_type":"markdown","id":"6d4de5cb","metadata":{},"source":["Strategy: Pre-filter CHARTEVENTS to find ICU stays with required vital signs, then efficiently load all tables using broadcast joins and lookup tables.\n","Key Steps:\n","\n","- Filter for ICU stays with ≥1 of 6 vital signs (HR, BP, RR, Temp, SpO2)\n","- Create lookup tables for ICUSTAY_ID, HADM_ID, SUBJECT_ID\n","- Load all tables with pre-filtering using broadcast joins\n","- Convert large files to \"Parquet\" for performance\n","\n","Result: Memory-efficient loading of only relevant data with quality assurance that all ICU stays have vital signs measurements."]},{"cell_type":"code","execution_count":3,"id":"82afa3e7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["🏥 Loading MIMIC-III data...\n","📂 Loading CHARTEVENTS...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["✅ Loaded CHARTEVENTS from parquet\n","\n","📂 Loading and filtering ICUSTAYS...\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 3:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["📋 Creating ID lookup tables...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📂 Loading PATIENTS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📂 Loading ADMISSIONS table...\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 34:>                                                         (0 + 1) / 1]\r","\r","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📂 Loading DIAGNOSES_ICD table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📂 Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\n","📂 Loading LABEVENTS table... [FILTERING BY HADM_ID]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["\n","✅ Data loading complete!\n","📊 ICUSTAYS: 61,532 rows\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📊 PATIENTS: 46,476 rows\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 48:>                                                         (0 + 1) / 1]\r","\r","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📊 ADMISSIONS: 57,786 rows\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📊 DIAGNOSES_ICD: 642,624 rows\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["📊 CHARTEVENTS (filtered): 330,414,954 rows\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 63:=============================>                            (2 + 2) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["📊 LABEVENTS (filtered): 22,072,543 rows\n","\n","⏰ Data loaded at: 2025-06-06 20:37:58\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Configuration flags\n","SAMPLE_ENABLE = False\n","SAMPLE_SIZE = 20000\n","MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n","\n","\n","\n","print(\"🏥 Loading MIMIC-III data...\")\n","\n","# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n","print(\"📂 Loading CHARTEVENTS...\")\n","\n","try:\n","   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","   print(\"✅ Loaded CHARTEVENTS from parquet\")\n","except:\n","   print(\"📄 Converting CHARTEVENTS.csv.gz to parquet...\")\n","   chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n","   chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n","   print(\"✅ Converted and loaded CHARTEVENTS\")\n","\n","\n","\n","# Step 2: Load ICUSTAYS \n","print(\"\\n📂 Loading and filtering ICUSTAYS...\")\n","icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n","\n","\n","\n","# Step 3: Apply sampling if enabled\n","if SAMPLE_ENABLE:\n","   print(f\"🎯 Sampling {SAMPLE_SIZE} ICU stays...\")\n","   icustays_df = icustays_df.limit(SAMPLE_SIZE)\n","   icustays_df.cache()\n","   actual_sample_size = icustays_df.count()\n","   print(f\"✅ Final sample: {actual_sample_size} ICU stays\")\n","else:\n","   icustays_df.cache()\n","   actual_sample_size = icustays_df.count()\n","\n","   \n","   \n","# Step 4: Create efficient lookup tables\n","print(\"📋 Creating ID lookup tables...\")\n","icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n","hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n","subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n","\n","icu_lookup.count()  # Trigger caching\n","hadm_lookup.count()\n","subject_lookup.count()\n","\n","# Step 5: Load other tables with optimized joins\n","print(\"📂 Loading PATIENTS table...\")\n","patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n","patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n","\n","print(\"📂 Loading ADMISSIONS table...\")\n","admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n","admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","print(\"📂 Loading DIAGNOSES_ICD table...\")\n","diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n","diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# Step 6: Load and filter CHARTEVENTS efficiently\n","print(\"📂 Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n","chartevents_df = chartevents_df \\\n","   .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n","   .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n","\n","# Step 7: Load LABEVENTS\n","print(\"📂 Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n","try:\n","   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","except:\n","   print(\"📄 Converting LABEVENTS.csv.gz to parquet...\")\n","   labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n","   labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n","\n","labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# Final summary\n","print(\"\\n✅ Data loading complete!\")\n","print(f\"📊 ICUSTAYS: {icustays_df.count():,} rows\")\n","print(f\"📊 PATIENTS: {patients_df.count():,} rows\") \n","print(f\"📊 ADMISSIONS: {admissions_df.count():,} rows\")\n","print(f\"📊 DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n","print(f\"📊 CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n","print(f\"📊 LABEVENTS (filtered): {labevents_df.count():,} rows\")\n","print(f\"\\n⏰ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"4d0db241","metadata":{},"source":["TIRAR ISTO ANTES DE ENTREGAR: ISTO E PARA CORRER LOCALMENTE!!!"]},{"cell_type":"code","execution_count":4,"id":"62936c54-45ab-4c03-a8ec-fc3d87f68721","metadata":{"scrolled":true},"outputs":[],"source":["# #Configuration flags\n","# SAMPLE_ENABLE = False\n","# SAMPLE_SIZE = 20000\n","# MIMIC_PATH = \"mimic-db-short\"\n","\n","\n","\n","# print(\"🏥 Loading MIMIC-III data...\")\n","\n","# # Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n","# print(\"📂 Loading CHARTEVENTS...\")\n","\n","\n","# chartevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n","# print(\"✅ Loaded CHARTEVENTS from parquet\")\n","\n","\n","\n","\n","# # Step 2: Load ICUSTAYS \n","# print(\"\\n📂 Loading and filtering ICUSTAYS...\")\n","# icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n","\n","\n","\n","# # Step 3: Apply sampling if enabled\n","# if SAMPLE_ENABLE:\n","#     print(f\"🎯 Sampling {SAMPLE_SIZE} ICU stays...\")\n","#     icustays_df = icustays_df.limit(SAMPLE_SIZE)\n","#     icustays_df.cache()\n","#     actual_sample_size = icustays_df.count()\n","#     print(f\"✅ Final sample: {actual_sample_size} ICU stays\")\n","# else:\n","#     icustays_df.cache()\n","#     actual_sample_size = icustays_df.count()\n","\n","    \n","    \n","# # Step 4: Create efficient lookup tables\n","# print(\"📋 Creating ID lookup tables...\")\n","# icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n","# hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n","# subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n","\n","# icu_lookup.count()  # Trigger caching\n","# hadm_lookup.count()\n","# subject_lookup.count()\n","\n","# # Step 5: Load other tables with optimized joins\n","# print(\"📂 Loading PATIENTS table...\")\n","# patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n","# patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n","\n","# print(\"📂 Loading ADMISSIONS table...\")\n","# admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n","# admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# print(\"📂 Loading DIAGNOSES_ICD table...\")\n","# diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n","# diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# # Step 6: Load and filter CHARTEVENTS efficiently\n","# print(\"📂 Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n","# chartevents_df = chartevents_df \\\n","#     .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n","#     .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n","\n","# # Step 7: Load LABEVENTS\n","# print(\"📂 Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n","\n","# labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n","\n","\n","# labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n","\n","# # Final summary\n","# print(\"\\n✅ Data loading complete!\")\n","# print(f\"📊 ICUSTAYS: {icustays_df.count():,} rows\")\n","# print(f\"📊 PATIENTS: {patients_df.count():,} rows\") \n","# print(f\"📊 ADMISSIONS: {admissions_df.count():,} rows\")\n","# print(f\"📊 DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n","# print(f\"📊 CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n","# print(f\"📊 LABEVENTS (filtered): {labevents_df.count():,} rows\")\n","# print(f\"\\n⏰ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"9e8e7157-1230-41d3-8f41-1117b62fdb55","metadata":{},"source":["# Features Engineering\n","\n"]},{"cell_type":"markdown","id":"8763e6d7-4ba5-439b-8e2b-ba16cf607a3e","metadata":{},"source":["## Extracting Data From ICUSTAYS\n","\n","**Purpose**: Create comprehensive ICU dataset by joining ICU stays with patient demographics and admission details.\n","\n","**Key Features**:\n","- **Target Variable**: ICU_LOS_DAYS (length of stay)\n","- **Demographics**: Age (18-80), gender, ethnicity\n","- **Clinical**: Care units, admission type/location, insurance\n","- **Outcomes**: Hospital/patient death flags\n","- **Identifiers**: ICUSTAY_ID, SUBJECT_ID, HADM_ID\n","\n","**Age Filter**: Adults only (18-80 years) to exclude pediatric/very elderly edge cases.\n","\n","**Alive Filter**: Only include people who did survive the ICU stay.\n","\n","**LOS Filter**: Get only LOS values within a range that does'nt include outliers.\n","\n","**Result**: Clean base dataset ready for vital signs feature engineering."]},{"cell_type":"code","execution_count":5,"id":"9f1d7adb-eac0-4071-b252-a04268f0c767","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 1: Creating base ICU dataset with patient demographics...\n","✅ Created base ICU dataset!\n"]}],"source":["print(\"📊 Step 1: Creating base ICU dataset with patient demographics...\")\n","\n","base_icu_df = icustays_df.alias(\"icu\") \\\n","    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n","    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n","    .select(\n","        # ICU stay identifiers\n","        col(\"icu.ICUSTAY_ID\"),\n","        col(\"icu.SUBJECT_ID\"), \n","        col(\"icu.HADM_ID\"),\n","        \n","        # Target variable - Length of Stay in ICU (days)\n","        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n","        \n","        # ICU characteristics\n","        col(\"icu.FIRST_CAREUNIT\"),\n","        col(\"icu.LAST_CAREUNIT\"), \n","        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n","        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n","        \n","        # Patient demographics\n","        col(\"pat.GENDER\"),\n","        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n","        col(\"pat.DOB\"),\n","        \n","        # Admission details\n","        col(\"adm.ADMITTIME\"),\n","        col(\"adm.DISCHTIME\"), \n","        col(\"adm.ADMISSION_TYPE\"),\n","        col(\"adm.ADMISSION_LOCATION\"),\n","        col(\"adm.INSURANCE\"),\n","        col(\"adm.ETHNICITY\"),\n","        col(\"adm.MARITAL_STATUS\"),\n","        col(\"adm.RELIGION\"),\n","        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n","        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n","    )\n","\n","# Calculate age at ICU admission\n","base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n","                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n","                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80)) \\\n","                                    .filter(col(\"PATIENT_DIED\").isin(0))\n","\n","\n","icustays_df.unpersist()\n","\n","\n","print(\"✅ Created base ICU dataset!\")"]},{"cell_type":"code","execution_count":6,"id":"51b6ccac","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","📈 ICU Length of Stay Statistics (Days):\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+-----------------+\n","|summary|     ICU_LOS_DAYS|\n","+-------+-----------------+\n","|  count|            26198|\n","|   mean|3.485478273150616|\n","| stddev|4.992118173063299|\n","|    min|           4.0E-4|\n","|    max|          101.739|\n","+-------+-----------------+\n","\n"]}],"source":["print(\"\\n📈 ICU Length of Stay Statistics (Days):\")\n","base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()"]},{"cell_type":"markdown","id":"4f6b3ede","metadata":{},"source":["We kept every ICU STAY that had duration (LOS) between 0.0 and 9.1 days, considered normal legnths since:\n","\n","| Statistic                | Value (days)                                    |\n","| ------------------------ | ----------------------------------------------- |\n","| **Minimum**              | 0.0 (can be admission + discharge on same day)  |\n","| **25th percentile (Q1)** | \\~1.1                                           |\n","| **Median (Q2)**          | \\~2.1                                           |\n","| **75th percentile (Q3)** | \\~4.3                                           |\n","| **Maximum**              | \\~88 (but can go slightly higher in edge cases) |\n","| **Mean**                 | \\~3.3–3.5                                       |\n","\n","Using interquartile range (IQR) method:\n","\n","* IQR = Q3 - Q1 = 4.3 - 1.1 = ~3.2\n","\n","* Upper Bound for outliers = Q3 + 1.5 × IQR ≈ 4.3 + 4.8 = ~9.1 days\n","\n","* Lower Bound = Q1 - 1.5 × IQR ≈ 1.1 - 4.8 = < 0, which is ignored since LOS can’t be negative\n","\n","So:\n","\n","* Typical ICU LOS: 1.1 to 4.3 days\n","\n","* Outliers: ICU stays longer than ~9.1 days"]},{"cell_type":"code","execution_count":7,"id":"fc345d7b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\r","[Stage 86:>                                                         (0 + 1) / 1]\r","\r","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of rows before removing LOS outliers: 26199\n","📊 Cleaning target variable...\n","✅ Base ICU Dataset - Outliers Removed\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 104:=========================================>             (24 + 2) / 32]\r"]},{"name":"stdout","output_type":"stream","text":["Number of rows after removing LOS outliers: 24211\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Print initial dataset size\n","print(f\"Number of rows before removing LOS outliers: {base_icu_df.count()}\")\n","\n","print(\"📊 Cleaning target variable...\")\n","\n","# Filter to keep only records with ICU_LOS_DAYS between 0 and 9.1 days\n","base_icu_df = base_icu_df.filter(\n","    (col(\"ICU_LOS_DAYS\") >= 0.0) & \n","    (col(\"ICU_LOS_DAYS\") <= 9.1)\n",").cache()\n","\n","print(\"✅ Base ICU Dataset - Outliers Removed\")\n","\n","# Print filtered dataset size\n","print(f\"Number of rows after removing LOS outliers: {base_icu_df.count()}\")"]},{"cell_type":"markdown","id":"595638e6-74f7-4f2f-a7e5-1fc692089206","metadata":{},"source":["## Extracting Categorical Features\n","\n","**Features Created**:\n","- **GENDER_BINARY**: Male = 1, Female = 0\n","- **CAME_FROM_ER**: Emergency admission = 1\n","- **HAS_INSURANCE**: Medicare = 1, other = 0\n","- **ADMISSION_TYPE_ENCODED**: Emergency=1, Elective=2, Urgent=3, Other=0\n","- **ETHNICITY_ENCODED**: White=1, Black=2, Hispanic=3, Asian=4, Other=5\n","- **MARITAL_STATUS_ENCODED**: Married=1, Single=2, Divorced=3, Widowed=4, Separated=5, LifePartener=6, Other=0\n","- **RELIGION_ENCODED**: Catholic=1, Protestant=2, Jewish=3, Other=0\n","\n","\n","\n","**Result**: Categorical variables converted to numerical format for ML models."]},{"cell_type":"code","execution_count":8,"id":"d28d34e6-83c3-40ae-95d0-71f9929397a1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 2: Engineering categorical features...\n","✅ Base ICU Dataset - Categorical Features\n"]}],"source":["print(\"📊 Step 2: Engineering categorical features...\")\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n","    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n","    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n","                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n","                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n","                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n","                .otherwise(0)) \\\n","    .withColumn(\"ETHNICITY_ENCODED\",\n","                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1)\n","                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2)\n","                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3)\n","                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4)\n","                .otherwise(5)) \\\n","    .withColumn(\"MARITAL_STATUS_ENCODED\",\n","                when(col(\"MARITAL_STATUS\") == \"MARRIED\", 1)\n","                .when(col(\"MARITAL_STATUS\") == \"SINGLE\", 2)\n","                .when(col(\"MARITAL_STATUS\") == \"DIVORCED\", 3)\n","                .when(col(\"MARITAL_STATUS\") == \"WIDOWED\", 4)\n","                .when(col(\"MARITAL_STATUS\") == \"SEPARATED\", 5)\n","                .when(col(\"MARITAL_STATUS\") == \"LIFE PARTNER\", 6)\n","                .otherwise(0)) \\\n","    .withColumn(\"RELIGION_ENCODED\",\n","                when(col(\"RELIGION\").contains(\"CATHOLIC\"), 1)\n","                .when(col(\"RELIGION\").contains(\"PROTESTANT\"), 2)\n","                .when(col(\"RELIGION\").contains(\"JEWISH\"), 3)\n","                .otherwise(0))\n","\n","print(\"✅ Base ICU Dataset - Categorical Features\")\n"]},{"cell_type":"markdown","id":"9ca22bc8-8dd5-4d30-9cb9-0f366da21d76","metadata":{},"source":["## Extracting ICU Unit Types\n","\n","**Purpose**: Create categorical features for ICU unit types and transfers.\n","\n","**Features Created**:\n","- **FIRST_UNIT_ENCODED**: Numerical encoding of ICU units\n"," - MICU (Medical) = 1\n"," - SICU (Surgical) = 2  \n"," - CSRU (Cardiac Surgery) = 3\n"," - CCU (Coronary Care) = 4\n"," - TSICU (Trauma Surgical) = 5\n"," - Other = 0\n","- **CHANGED_ICU_UNIT**: Binary flag (1 if patient transferred between units)\n","\n","**Clinical Significance**: Different ICU types have varying complexity and typical LOS patterns. Unit transfers often indicate complications.\n","\n","**Result**: Enhanced dataset with ICU unit complexity and transfer indicators."]},{"cell_type":"code","execution_count":9,"id":"6617c3f2-75b9-4fa0-93a8-7161d0c2dd57","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 3: Creating ICU unit type features...\n","✅ Base ICU Dataset - Unit Type Features\n"]}],"source":["print(\"📊 Step 3: Creating ICU unit type features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"FIRST_UNIT_ENCODED\", \n","                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n","                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n","                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n","                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n","                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n","                .otherwise(0)) \\\n","    .withColumn(\"CHANGED_ICU_UNIT\", \n","                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n","\n","\n","print(\"✅ Base ICU Dataset - Unit Type Features\")"]},{"cell_type":"markdown","id":"74662ab6-f263-402e-913f-dc04804eadff","metadata":{},"source":["## Extracting Time-based Features\n","\n","**Action**: Filter out invalid records where INTIME >= OUTTIME.\n"]},{"cell_type":"code","execution_count":10,"id":"ef431386-85f1-4867-9aa1-f5bb84d7c8ae","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 4: Creating time-based features...\n","✅ Base ICU Dataset - Time Based Features\n"]}],"source":["print(\"📊 Step 4: Creating time-based features...\")\n","base_icu_df = base_icu_df \\\n","    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n","print(\"✅ Base ICU Dataset - Time Based Features\")"]},{"cell_type":"markdown","id":"a8a29626","metadata":{},"source":["Select useful columns brom the base df"]},{"cell_type":"code","execution_count":11,"id":"224c495f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/06/06 20:38:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"]},{"name":"stdout","output_type":"stream","text":["+----------+----------+-------+------------+--------------+-------------+-------------------+-------------------+------+------------+-------------------+-------------------+-------------------+--------------+--------------------+---------+--------------------+--------------+--------------+--------------+--------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n","|ICUSTAY_ID|SUBJECT_ID|HADM_ID|ICU_LOS_DAYS|FIRST_CAREUNIT|LAST_CAREUNIT|         ICU_INTIME|        ICU_OUTTIME|GENDER|PATIENT_DIED|                DOB|          ADMITTIME|          DISCHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|INSURANCE|           ETHNICITY|MARITAL_STATUS|      RELIGION|HOSPITAL_DEATH| ADMISSION_DIAGNOSIS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|\n","+----------+----------+-------+------------+--------------+-------------+-------------------+-------------------+------+------------+-------------------+-------------------+-------------------+--------------+--------------------+---------+--------------------+--------------+--------------+--------------+--------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n","|    268261|       125| 157348|      1.2281|          CSRU|         CSRU|2179-02-16 09:36:46|2179-02-17 15:05:13|     F|           0|2128-02-29 00:00:00|2179-02-14 21:23:00|2179-02-21 18:03:00|      ELECTIVE|PHYS REFERRAL/NOR...|  Private|               ASIAN|       MARRIED| NOT SPECIFIED|             0|AR\\AORTIC VALVE R...|                  50|            0|           0|            0|                     2|                4|                     1|               0|                 3|               0|\n","|    269111|       142| 131357|      1.8695|          CSRU|         CSRU|2143-04-01 14:19:37|2143-04-03 11:11:38|     F|           0|2095-02-27 00:00:00|2143-04-01 07:15:00|2143-04-05 18:40:00|      ELECTIVE|PHYS REFERRAL/NOR...|  Private|               WHITE|       MARRIED| NOT SPECIFIED|             0|MR\\MITRAL VALVE R...|                  48|            0|           0|            0|                     2|                1|                     1|               0|                 3|               0|\n","|    239673|       492| 114598|      3.5921|          MICU|         MICU|2174-04-13 01:26:47|2174-04-16 15:39:28|     M|           0|2134-03-16 00:00:00|2174-04-13 01:25:00|2174-04-16 15:39:00|     EMERGENCY|TRANSFER FROM HOS...|  Private|               WHITE|       MARRIED|      CATHOLIC|             0| ASTHMA EXACERBATION|                  40|            1|           0|            0|                     1|                1|                     1|               1|                 1|               0|\n","|    231758|       406| 100765|      6.9653|          MICU|          CCU|2126-03-11 23:07:15|2126-03-18 22:17:14|     F|           0|2058-01-29 00:00:00|2126-03-11 23:06:00|2126-03-26 16:17:00|     EMERGENCY|CLINIC REFERRAL/P...| Medicare|               WHITE|       MARRIED|      CATHOLIC|             0|              SEPSIS|                  68|            0|           0|            1|                     1|                1|                     1|               1|                 1|               1|\n","|    228132|       202| 108295|      1.6262|          MICU|         MICU|2145-10-22 01:31:32|2145-10-23 16:33:12|     F|           0|2070-01-17 00:00:00|2145-10-22 01:31:00|2145-10-24 14:45:00|     EMERGENCY|EMERGENCY ROOM ADMIT| Medicare|               WHITE|       MARRIED|  UNOBTAINABLE|             0|           UROSEPSIS|                  75|            0|           1|            1|                     1|                1|                     1|               0|                 1|               0|\n","|    298509|       904| 158591|      2.6187|          MICU|         MICU|2114-04-10 05:27:03|2114-04-12 20:18:01|     M|           0|2083-09-05 00:00:00|2114-04-10 05:26:00|2114-04-17 14:00:00|     EMERGENCY|EMERGENCY ROOM ADMIT| Medicaid|               WHITE|        SINGLE| NOT SPECIFIED|             0|ANTICHOLINERGIC T...|                  30|            1|           1|            0|                     1|                1|                     2|               0|                 1|               0|\n","|    228333|       587| 150352|      2.0682|          MICU|         MICU|2100-11-09 14:11:20|2100-11-11 15:49:36|     M|           0|2044-02-20 00:00:00|2100-10-26 18:18:00|2100-12-01 19:00:00|     EMERGENCY|EMERGENCY ROOM ADMIT|  Private|               WHITE|       MARRIED| NOT SPECIFIED|             0|       SPLENOMEGALIA|                  56|            1|           1|            0|                     1|                1|                     1|               0|                 1|               0|\n","|    290128|       629| 165308|       2.084|           CCU|          CCU|2185-01-08 14:03:00|2185-01-10 16:04:00|     M|           0|2140-10-08 00:00:00|2185-01-08 14:02:00|2185-01-12 14:58:00|     EMERGENCY|TRANSFER FROM HOS...|  Private|               WHITE|        SINGLE|      CATHOLIC|             0|ACUTE MYOCARDIAL ...|                  44|            1|           0|            0|                     1|                1|                     2|               1|                 4|               0|\n","|    248841|       672| 119707|       2.206|          MICU|         MICU|2199-04-23 10:45:55|2199-04-25 15:42:37|     M|           0|2152-10-21 00:00:00|2199-04-23 10:43:00|2199-04-25 15:00:00|     EMERGENCY|EMERGENCY ROOM ADMIT|  Private|               WHITE|       MARRIED|      CATHOLIC|             0|DIABETIC KETOACID...|                  46|            1|           1|            0|                     1|                1|                     1|               1|                 1|               0|\n","|    274975|       554| 174947|      3.5725|           CCU|          CCU|2117-10-24 22:55:17|2117-10-28 12:39:41|     M|           0|2080-06-16 00:00:00|2117-10-24 22:54:00|2117-10-31 15:13:00|     EMERGENCY|TRANSFER FROM HOS...|  Private|               WHITE|       MARRIED|  UNOBTAINABLE|             0|     UNSTABLE ANGINA|                  37|            1|           0|            0|                     1|                1|                     1|               0|                 4|               0|\n","|    293353|      1013| 158123|      0.7464|          CSRU|         CSRU|2163-01-13 18:08:00|2163-01-14 12:02:50|     M|           0|2093-09-08 00:00:00|2163-01-13 08:00:00|2163-01-22 15:55:00|      ELECTIVE|PHYS REFERRAL/NOR...| Medicare|               WHITE|       MARRIED|        JEWISH|             0|MR\\MITRAL VALVE R...|                  69|            1|           0|            1|                     2|                1|                     1|               3|                 3|               0|\n","|    218921|      1102| 174166|       3.124|          CSRU|          CCU|2139-12-15 10:53:02|2139-12-18 13:51:32|     M|           0|2080-02-07 00:00:00|2139-12-15 07:15:00|2139-12-21 15:55:00|      ELECTIVE|PHYS REFERRAL/NOR...|  Private|UNKNOWN/NOT SPECI...|          NULL|  UNOBTAINABLE|             0|CORONARY ARTERY D...|                  59|            1|           0|            0|                     2|                5|                     0|               0|                 3|               1|\n","|    299724|       948| 142734|      1.2674|           CCU|          CCU|2107-11-18 12:58:00|2107-11-19 19:23:00|     F|           0|2078-11-09 00:00:00|2107-11-18 03:50:00|2107-11-24 14:47:00|     EMERGENCY|TRANSFER FROM HOS...|  Private|               WHITE|       MARRIED|      CATHOLIC|             0|               FEVER|                  29|            0|           0|            0|                     1|                1|                     1|               1|                 4|               0|\n","|    209800|       953| 175948|       1.242|          CSRU|         CSRU|2103-12-25 11:02:08|2103-12-26 16:50:35|     M|           0|2032-10-24 00:00:00|2103-12-20 20:10:00|2103-12-30 16:10:00|     EMERGENCY|EMERGENCY ROOM ADMIT| Medicare|     WHITE - RUSSIAN|       MARRIED|GREEK ORTHODOX|             0|          CHEST PAIN|                  71|            1|           1|            1|                     1|                1|                     1|               0|                 3|               0|\n","|    296634|       963| 159921|      0.8569|           CCU|          CCU|2195-04-27 18:22:00|2195-04-28 14:56:00|     F|           0|2134-03-12 00:00:00|2195-04-24 14:50:00|2195-04-30 13:27:00|     EMERGENCY|TRANSFER FROM HOS...|  Private|               WHITE|      DIVORCED|          NULL|             0|                +ETT|                  61|            0|           0|            0|                     1|                1|                     3|               0|                 4|               0|\n","|    217391|      1634| 115518|      4.1544|          CSRU|         CSRU|2157-05-16 08:23:25|2157-05-20 12:05:47|     F|           0|2080-06-04 00:00:00|2157-05-13 13:38:00|2157-05-30 14:10:00|     EMERGENCY|TRANSFER FROM HOS...| Medicare|UNKNOWN/NOT SPECI...|       WIDOWED|      CATHOLIC|             0|        SYNCOPE\\CATH|                  76|            0|           0|            1|                     1|                5|                     4|               1|                 3|               0|\n","|    249458|      1693| 182343|      1.9481|          CSRU|         CSRU|2182-05-14 11:02:26|2182-05-16 09:47:38|     M|           0|2133-03-26 00:00:00|2182-05-14 07:15:00|2182-05-18 12:45:00|      ELECTIVE|PHYS REFERRAL/NOR...|  Private|               WHITE|       MARRIED| NOT SPECIFIED|             0|MR\\MITRAL VALVE A...|                  49|            1|           0|            0|                     2|                1|                     1|               0|                 3|               0|\n","|    210745|      1316| 126277|      2.2265|          MICU|         MICU|2184-10-14 06:06:30|2184-10-16 11:32:36|     F|           0|2138-04-20 00:00:00|2184-10-14 06:04:00|2184-10-20 18:00:00|     EMERGENCY|EMERGENCY ROOM ADMIT| Medicaid|BLACK/AFRICAN AME...|      DIVORCED|      CATHOLIC|             0|ELECTROLYTE IMBAL...|                  46|            0|           1|            0|                     1|                2|                     3|               1|                 1|               0|\n","|    234173|      1374| 137853|      3.8382|         TSICU|        TSICU|2170-04-20 18:13:00|2170-04-24 14:20:00|     M|           0|2106-02-25 00:00:00|2170-04-20 18:12:00|2170-04-28 08:27:00|     EMERGENCY|TRANSFER FROM HOS...|  Private|UNKNOWN/NOT SPECI...|       MARRIED|  UNOBTAINABLE|             0|  CAROTID DISSECTION|                  64|            1|           0|            0|                     1|                5|                     1|               0|                 5|               0|\n","|    292165|      1478| 158399|      2.7386|           CCU|          CCU|2142-09-17 21:21:28|2142-09-20 15:05:00|     F|           0|2090-01-30 00:00:00|2142-09-17 21:20:00|2142-09-25 14:30:00|     EMERGENCY|CLINIC REFERRAL/P...|  Private|               WHITE|       MARRIED|        JEWISH|             0|CORONARY ARTERY D...|                  52|            0|           0|            0|                     1|                1|                     1|               3|                 4|               0|\n","+----------+----------+-------+------------+--------------+-------------+-------------------+-------------------+------+------------+-------------------+-------------------+-------------------+--------------+--------------------+---------+--------------------+--------------+--------------+--------------+--------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n","only showing top 20 rows\n","\n"]}],"source":["base_icu_df.show()\n"]},{"cell_type":"code","execution_count":12,"id":"79066063","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 5: Dropping useless columns...\n","✅ Base ICU Dataset - Finalized\n","+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n","|ICUSTAY_ID|SUBJECT_ID|HADM_ID|ICU_LOS_DAYS|         ICU_INTIME|        ICU_OUTTIME|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|\n","+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n","|    268261|       125| 157348|      1.2281|2179-02-16 09:36:46|2179-02-17 15:05:13|                  50|            0|           0|            0|                     2|                4|                     1|               0|                 3|               0|\n","|    269111|       142| 131357|      1.8695|2143-04-01 14:19:37|2143-04-03 11:11:38|                  48|            0|           0|            0|                     2|                1|                     1|               0|                 3|               0|\n","|    239673|       492| 114598|      3.5921|2174-04-13 01:26:47|2174-04-16 15:39:28|                  40|            1|           0|            0|                     1|                1|                     1|               1|                 1|               0|\n","|    231758|       406| 100765|      6.9653|2126-03-11 23:07:15|2126-03-18 22:17:14|                  68|            0|           0|            1|                     1|                1|                     1|               1|                 1|               1|\n","|    228132|       202| 108295|      1.6262|2145-10-22 01:31:32|2145-10-23 16:33:12|                  75|            0|           1|            1|                     1|                1|                     1|               0|                 1|               0|\n","+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n","only showing top 5 rows\n","\n"]}],"source":["print(\"📊 Step 5: Dropping useless columns...\")\n","\n","# List of columns to drop (fixed syntax)\n","drop_cols = [\n","    \"FIRST_CAREUNIT\",\n","    \"LAST_CAREUNIT\",\n","    \"GENDER\",\n","    \"PATIENT_DIED\",\n","    \"DOB\",\n","    \"ADMITTIME\",\n","    \"DISCHTIME\",\n","    \"ADMISSION_TYPE\",\n","    \"ADMISSION_LOCATION\",\n","    \"INSURANCE\",\n","    \"ETHNICITY\",\n","    \"MARITAL_STATUS\",\n","    \"RELIGION\",\n","    \"HOSPITAL_DEATH\",\n","    \"ADMISSION_DIAGNOSIS\"\n","]\n","\n","# Keep all columns except those in drop_cols\n","base_icu_df = base_icu_df.drop(*drop_cols)\n","\n","print(\"✅ Base ICU Dataset - Finalized\")\n","base_icu_df.show(5)  # Showing first 5 rows for brevity"]},{"cell_type":"markdown","id":"268129f5-72e2-40a5-b676-cbd825ae84c8","metadata":{},"source":["## Extracting Clinical Events\n","\n","**Purpose**: Extract top 20 most common CHARTEVENTS as features for ML models.\n","\n","**Process**:\n","1. **Identify**: Find 20 most frequent CHARTEVENTS (typically vital signs)\n","2. **Calculate**: Average value of each test in first 24 hours of ICU stay\n","3. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n","\n","**Time Window**: First 24 hours after ICU admission (INTIME + 24h)\n","\n","**Result**: 20 vital signs features with consistent **-1** encoding for missing data, ensuring ML algorithm compatibility.\n"]},{"cell_type":"code","execution_count":14,"id":"9dc92782-4f16-4c82-bd8a-44ed78e92965","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Identifying top 20 most frequent tests from CHARTEVENTS...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["🎯 Top 20 chart items selected: {'211': 'VITAL_211', '742': 'VITAL_742', '646': 'VITAL_646', '618': 'VITAL_618', '220045': 'VITAL_220045', '220210': 'VITAL_220210', '220277': 'VITAL_220277', '51': 'VITAL_51', '8368': 'VITAL_8368', '52': 'VITAL_52', '8549': 'VITAL_8549', '5815': 'VITAL_5815', '5820': 'VITAL_5820', '8554': 'VITAL_8554', '8553': 'VITAL_8553', '5819': 'VITAL_5819', '834': 'VITAL_834', '3450': 'VITAL_3450', '8518': 'VITAL_8518', '3603': 'VITAL_3603'}\n","📊 Filtering CHARTEVENTS for top 20 items...\n","📊 Calculating aggregates for top 20 vitals...\n","✅ Created 20 features from top 20 vital signs\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 206:=====================================================> (31 + 1) / 32]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+-----------------+-------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+--------------+--------------+------------------+--------------+-----------------+--------------+--------------+--------------+\n","|ICUSTAY_ID|    VITAL_211_AVG|VITAL_742_AVG|    VITAL_646_AVG|     VITAL_618_AVG| VITAL_220045_AVG|  VITAL_220210_AVG| VITAL_220277_AVG|      VITAL_51_AVG|    VITAL_8368_AVG|     VITAL_52_AVG|    VITAL_8549_AVG|    VITAL_5815_AVG|VITAL_5820_AVG|VITAL_8554_AVG|    VITAL_8553_AVG|VITAL_5819_AVG|    VITAL_834_AVG|VITAL_3450_AVG|VITAL_8518_AVG|VITAL_3603_AVG|\n","+----------+-----------------+-------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+--------------+--------------+------------------+--------------+-----------------+--------------+--------------+--------------+\n","|    228132|83.38888888888889|          1.0|96.22222222222223|              17.5|             NULL|              NULL|             NULL|              NULL|              NULL|             NULL|             130.0|              60.0|          90.0|         100.0|              30.0|           8.0|             NULL|          NULL|          NULL|          NULL|\n","|    268261|97.37931034482759|          1.0|98.74074074074075|              21.5|             NULL|              NULL|             NULL|112.53571428571429| 60.92857142857143|77.77777777777777|126.08695652173913|              60.0|           8.0|          30.0| 96.08695652173913|          92.0|96.57142857142857|          NULL|          NULL|          NULL|\n","|    239673|90.22222222222223|          1.0|             96.4|13.185185185185185|             NULL|              NULL|             NULL|              NULL|              NULL|             NULL|124.07407407407408|58.148148148148145|          90.0|         100.0|34.074074074074076|           8.0|             NULL|          NULL|          NULL|          NULL|\n","|    269111|91.38297872340425|          1.0|98.93478260869566|           17.5625|             NULL|              NULL|             NULL| 95.45833333333333|53.916666666666664|66.95744680851064|              NULL|              NULL|          NULL|          NULL|              NULL|          NULL|             99.0|          NULL|          NULL|          NULL|\n","|    298509|             NULL|         NULL|             NULL|              NULL|89.41666666666667|24.391304347826086|96.16666666666667|              NULL|              NULL|             NULL|              NULL|              NULL|          NULL|          NULL|              NULL|          NULL|             NULL|          NULL|          NULL|          NULL|\n","+----------+-----------------+-------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+--------------+--------------+------------------+--------------+-----------------+--------------+--------------+--------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"📊 Identifying top 20 most frequent tests from CHARTEVENTS...\")\n","\n","\n","# Get frequency count of each ITEMID in CHARTEVENTS\n","itemid_counts = chartevents_df \\\n","    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"CHARTTIME\").isNotNull()) \\\n","    .groupBy(\"ITEMID\") \\\n","    .count() \\\n","    .orderBy(col(\"count\").desc()) \\\n","    .limit(20) \\\n","    .collect()\n","\n","# Create mapping dictionary for top 20 items\n","top_20_items = {row[\"ITEMID\"]: f\"VITAL_{row['ITEMID']}\" for row in itemid_counts}\n","print(f\"🎯 Top 20 chart items selected: {top_20_items}\")\n","\n","print(\"📊 Filtering CHARTEVENTS for top 20 items...\")\n","\n","chartevents_top20 = chartevents_df \\\n","    .filter(col(\"ITEMID\").isin(list(top_20_items.keys()))) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"ICUSTAY_ID\").isNotNull()) \\\n","    .filter(col(\"CHARTTIME\").isNotNull()) \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(col(\"CHARTTIME\").between(col(\"ICU_INTIME\"), col(\"ICU_OUTTIME\"))) \\\n","    .select(\"ICUSTAY_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\")\n","\n","# Process first 24 hours\n","vitals_24h_top20 = chartevents_top20.alias(\"ce\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(\n","        col(\"ce.CHARTTIME\").between(\n","            col(\"ICU_INTIME\"), \n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","\n","chartevents_top20.unpersist()\n","\n","print(\"📊 Calculating aggregates for top 20 vitals...\")\n","\n","# Initialize with ICUSTAY_ID\n","vitals_features_top20 = base_icu_df.select(\"ICUSTAY_ID\")\n","\n","# Process each vital sign\n","for itemid, name in top_20_items.items():\n","    #print(f\"Processing {name} (ITEMID={itemid})...\")\n","    \n","    vital_stats = vitals_24h_top20 \\\n","        .filter(col(\"ITEMID\") == itemid) \\\n","        .groupBy(\"ICUSTAY_ID\") \\\n","        .agg(avg(\"VALUENUM\").alias(f\"{name}_AVG\"))\n","    \n","    # Left join (without filling NULLs yet)\n","    vitals_features_top20 = vitals_features_top20.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n","\n","# Cleanup\n","chartevents_df.unpersist()\n","vitals_24h_top20.unpersist()\n","\n","# Verify no NULLs remain\n","print(f\"✅ Created {len(top_20_items)} features from top 20 vital signs\")\n","vitals_features_top20.show(5)"]},{"cell_type":"markdown","id":"7c08c810-405c-48ca-a141-1a48e125635c","metadata":{},"source":["## Extracting Laboratory Events\n","\n","**Purpose**: Extract top 20 most common lab tests as features for ML models.\n","\n","**Process**:\n","1. **Identify**: Find 20 most frequent LABEVENTS (blood tests, chemistry panels)\n","2. **Time Window**: 6 hours before ICU admission + first 24 hours in ICU (30h total)\n","3. **Calculate**: Average value of each lab test within the 30-hour window\n","\n","**Time Range**: ICU_INTIME - 6h to ICU_INTIME + 24h\n","\n","**Result**: 20 lab test features with consistent -1 encoding for missing data, capturing pre-ICU and early ICU clinical status."]},{"cell_type":"code","execution_count":17,"id":"a676e670","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","🧪 Creating laboratory features from LABEVENTS...\n","📊 Identifying top 20 most frequent lab items...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 20:46:26 WARN DAGScheduler: Broadcasting large task binary with size 1053.0 KiB\n","25/06/06 20:46:30 WARN DAGScheduler: Broadcasting large task binary with size 1050.2 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["🎯 Top 20 lab items selected: ['51221', '50971', '51006', '50983', '50902', '51265', '50882', '51222', '51301', '50931', '51249', '50868', '51248', '51250', '51279', '51277', '50960', '50893', '50970', '50820']\n","📊 Filtering LABEVENTS for top 20 items...\n","📊 Calculating laboratory statistics...\n","✅ Created 20 lab features for 24,211 ICU stays\n","📊 Sample features:\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------+-----------------+-----------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-----------------+\n","|ICUSTAY_ID|LAB_51221_AVG    |LAB_50971_AVG    |LAB_51006_AVG|LAB_50983_AVG     |LAB_50902_AVG     |LAB_51265_AVG     |LAB_50882_AVG     |LAB_51222_AVG     |LAB_51301_AVG     |LAB_50931_AVG|LAB_51249_AVG     |LAB_50868_AVG     |LAB_51248_AVG    |LAB_51250_AVG    |LAB_51279_AVG     |LAB_51277_AVG     |LAB_50960_AVG     |LAB_50893_AVG    |LAB_50970_AVG     |LAB_50820_AVG    |\n","+----------+-----------------+-----------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-----------------+\n","|228132    |30.35            |3.1              |51.0         |141.5             |110.0             |215.5             |17.5              |10.0              |22.7              |96.5         |32.95             |17.5              |29.0             |88.0             |3.45              |17.1              |1.7000000000000002|7.449999999999999|3.6500000000000004|NULL             |\n","|268261    |30.425           |4.6              |16.5         |140.0             |109.0             |88.75             |23.5              |10.466666666666667|14.6              |135.0        |35.23333333333333 |15.0              |32.26666666666667|91.33333333333333|3.2533333333333334|16.133333333333336|NULL              |NULL             |NULL              |7.369411764705882|\n","|239673    |42.35            |4.2              |16.0         |138.0             |106.0             |234.0             |23.0              |13.8              |19.6              |176.0        |32.7              |13.0              |27.5             |84.0             |5.0               |14.0              |2.0               |8.1              |1.9               |7.305            |\n","|269111    |28.875           |3.7              |6.5          |140.0             |108.0             |141.0             |29.0              |9.5               |16.6              |97.0         |35.1              |7.0               |31.8             |91.0             |2.98              |12.6              |1.5               |NULL             |NULL              |7.377            |\n","|298509    |44.73333333333333|3.766666666666667|8.0          |141.66666666666666|105.66666666666667|243.33333333333334|25.333333333333332|15.5              |11.933333333333332|106.0        |34.666666666666664|14.333333333333334|29.96666666666667|86.33333333333333|5.18              |13.066666666666668|2.15              |8.95             |3.25              |NULL             |\n","+----------+-----------------+-----------------+-------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-----------------+\n","only showing top 5 rows\n","\n"]}],"source":["print(\"\\n🧪 Creating laboratory features from LABEVENTS...\")\n","\n","# Step 1: Identify top 20 most frequent lab items\n","print(\"📊 Identifying top 20 most frequent lab items...\")\n","top_20_lab_items = labevents_df \\\n","    .filter(col(\"HADM_ID\").isin([row[\"HADM_ID\"] for row in base_icu_df.select(\"HADM_ID\").collect()])) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\") > 0) \\\n","    .groupBy(\"ITEMID\") \\\n","    .count() \\\n","    .orderBy(col(\"count\").desc()) \\\n","    .limit(20) \\\n","    .collect()\n","\n","# Create mapping dictionary with clean LAB_[ITEMID] format\n","lab_items = {row[\"ITEMID\"]: f\"LAB_{row['ITEMID']}\" for row in top_20_lab_items}\n","print(f\"🎯 Top 20 lab items selected: {list(lab_items.keys())}\")\n","\n","\n","\n","# Step 2: Filter lab events within first 24 hours of ICU stay\n","print(\"📊 Filtering LABEVENTS for top 20 items...\")\n","labs_24h = labevents_df.alias(\"le\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n","    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n","    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n","    .filter(col(\"le.VALUENUM\") > 0) \\\n","    .filter(\n","        col(\"le.CHARTTIME\").between(\n","            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","# Step 3: Calculate lab statistics with clean column names\n","print(\"📊 Calculating laboratory statistics...\")\n","labs_features = base_icu_df.select(\"ICUSTAY_ID\")\n","\n","for itemid, name in lab_items.items():\n","    item_stats = labs_24h \\\n","        .filter(col(\"ITEMID\") == itemid) \\\n","        .groupBy(\"ICUSTAY_ID\") \\\n","        .agg(\n","            avg(\"VALUENUM\").alias(f\"{name}_AVG\")  # Simple alias without coalesce in the name\n","        )\n","    \n","    labs_features = labs_features.join(item_stats, \"ICUSTAY_ID\", \"left\")\n","\n","\n","print(f\"✅ Created {len(lab_items)} lab features for {labs_features.count():,} ICU stays\")\n","\n","# Cleanup\n","labevents_df.unpersist()\n","labs_24h.unpersist()\n","\n","# Show sample of features with clean column names\n","print(\"📊 Sample features:\")\n","labs_features.select(\n","    \"ICUSTAY_ID\",\n","    *[col for col in labs_features.columns if col != \"ICUSTAY_ID\"]\n",").show(5, truncate=False)"]},{"cell_type":"markdown","id":"a43cd034-b129-46b0-b33f-cf903ee438ae","metadata":{},"source":["## Diagnosis ICD\n","\n","**Purpose**: Extract diagnosis patterns as ML features from ICD-9 codes.\n","\n","**Process**:\n","1. **Top 3**: Get top 3 diagnoses by person, using HADM_ID, to future join with other tables. \n","2. **Encode**: Encode the ICD9 diagnoses into a wide range of diagnoses.\n","3. **Pivot**: Pivot to create the 3 columns with the encoded diagnose type.\n","4. **Handle Missing Values**: Input -1 in the NULL entries of the table.\n","\n","\n","**Features Created**:\n","- **TOTAL_DIAGNOSES**: Count of all diagnoses (comorbidity indicator)\n","- **PRIMARY_DIAGNOSIS**: Most significant diagnose, encoded.\n","- **SECONDARY_DIAGNOSIS**: Second most significant diagnose, encoded.\n","- **TERCIARY_DIAGNOSIS**: Third most significant diagnose, encoded.\n","\n","**Result**: ??????????????????????????????????????????????????????????"]},{"cell_type":"code","execution_count":18,"id":"4b3682a8","metadata":{},"outputs":[],"source":["def icd9_to_chapter(code):\n","    # Convert to string and clean\n","    code_str = str(code).strip()\n","    \n","    # Handle V codes (supplementary classification)\n","    if code_str.startswith('V'):\n","        return 18 #'Supplemental'\n","    \n","    # Handle E codes (external causes of injury)\n","    if code_str.startswith('E'):\n","        return 19 #'External_Injury'\n","    \n","    # Extract first 3 digits for numeric codes\n","    try:\n","        # Handle codes like '4280' (convert to 428) or '486' (stays 486)\n","        numeric_part = code_str.split('.')[0] if '.' in code_str else code_str\n","        code_num = float(numeric_part[:3])\n","    except:\n","        return 0 #'Unknown'\n","    \n","    # Map to chapters\n","    if 1 <= code_num <= 139: return 1 #'Infectious'\n","    elif 140 <= code_num <= 239: return 2 # 'Neoplasms'\n","    elif 240 <= code_num <= 279: return 3 #'Endocrine'\n","    elif 280 <= code_num <= 289: return 4 #'Blood'\n","    elif 290 <= code_num <= 319: return 5 #'Mental'\n","    elif 320 <= code_num <= 389: return 6 #'Nervous'\n","    elif 390 <= code_num <= 459: return 7 #'Circulatory'\n","    elif 460 <= code_num <= 519: return 8 #'Respiratory'\n","    elif 520 <= code_num <= 579: return 9 #'Digestive'\n","    elif 580 <= code_num <= 629: return 10 #'Genitourinary'\n","    elif 630 <= code_num <= 679: return 11 #'Pregnancy'\n","    elif 680 <= code_num <= 709: return 12 #'Skin'\n","    elif 710 <= code_num <= 739: return 13 #'Musculoskeletal'\n","    elif 740 <= code_num <= 759: return 14 #'Congenital'\n","    elif 760 <= code_num <= 779: return 15 #'Perinatal'\n","    elif 780 <= code_num <= 799: return 16 #'Ill-defined'\n","    elif 800 <= code_num <= 999: return 17 #'Injury'\n","    else: return 20 #'Other' "]},{"cell_type":"code","execution_count":19,"id":"94a414c1-7139-4e27-a5e4-767919a3eead","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","🏥 Creating diagnosis features (optimized pipeline)...\n","📊 Optimized diagnosis features:\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 395:>                                                        (0 + 2) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+---------------+-----------------+-------------------+------------------+\n","|HADM_ID|TOTAL_DIAGNOSES|PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|\n","+-------+---------------+-----------------+-------------------+------------------+\n","|100834 |18             |1                |9                  |2                 |\n","|101627 |9              |9                |7                  |13                |\n","|102725 |9              |17               |7                  |5                 |\n","|103555 |6              |18               |15                 |18                |\n","|103609 |10             |9                |9                  |10                |\n","|104165 |3              |18               |18                 |18                |\n","|104929 |12             |7                |8                  |8                 |\n","|106283 |6              |8                |8                  |2                 |\n","|107958 |3              |18               |18                 |18                |\n","|109162 |10             |7                |7                  |8                 |\n","|109645 |11             |5                |6                  |10                |\n","|111132 |8              |7                |7                  |7                 |\n","|111995 |8              |18               |14                 |15                |\n","|112182 |13             |17               |17                 |17                |\n","|112987 |9              |9                |17                 |7                 |\n","|113267 |6              |18               |15                 |15                |\n","|113517 |9              |18               |15                 |15                |\n","|114003 |9              |10               |3                  |4                 |\n","|114677 |12             |1                |16                 |8                 |\n","|114818 |12             |7                |7                  |10                |\n","+-------+---------------+-----------------+-------------------+------------------+\n","only showing top 20 rows\n","\n","⏰ Completed in: 15.85s\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"\\n🏥 Creating diagnosis features (optimized pipeline)...\")\n","\n","start_time = time.time()\n","\n","# 1. First filter to only top 3 diagnoses per admission\n","window_spec = Window.partitionBy(\"HADM_ID\").orderBy(\"SEQ_NUM\")\n","\n","top_3_filtered = diagnoses_df \\\n","    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n","    .filter(col(\"row_num\") <= 3) \\\n","    .cache()\n","\n","# 2. Register UDF with Integer return type\n","icd9_chapter_udf = udf(icd9_to_chapter, IntegerType())  # Changed to IntegerType\n","\n","# 3. Encode ONLY the top 3 diagnoses\n","top_3_encoded = top_3_filtered.withColumn(\n","    \"DISEASE_CHAPTER\", \n","    icd9_chapter_udf(col(\"ICD9_CODE\"))\n",")\n","\n","\n","diagnosis_count = diagnoses_df.groupBy(\"HADM_ID\").count().withColumnRenamed(\"count\", \"TOTAL_DIAGNOSES\")\n","\n","\n","diagnoses_df.unpersist()\n","\n","\n","# 4. Pivot to create columns\n","diagnosis_features = top_3_encoded \\\n","    .groupBy(\"HADM_ID\") \\\n","    .pivot(\"row_num\", [1, 2, 3]) \\\n","    .agg(first(\"DISEASE_CHAPTER\")) \\\n","    .select(\n","        \"HADM_ID\",\n","        col(\"1\").alias(\"PRIMARY_DIAGNOSIS\").cast(IntegerType()),\n","        col(\"2\").alias(\"SECONDARY_DIAGNOSIS\").cast(IntegerType()),\n","        col(\"3\").alias(\"TERTIARY_DIAGNOSIS\").cast(IntegerType())\n","    ) \\\n","    .join(diagnosis_count, \"HADM_ID\", \"left\")\n","\n","# 5. Fill NULLs and ensure consistent types\n","diagnosis_features = diagnosis_features.fillna(-1, subset=[\n","    \"PRIMARY_DIAGNOSIS\",\n","    \"SECONDARY_DIAGNOSIS\",\n","    \"TERTIARY_DIAGNOSIS\"\n","])\n","\n","\n","print(\"📊 Optimized diagnosis features:\")\n","diagnosis_features.select(\n","    \"HADM_ID\",\n","    \"TOTAL_DIAGNOSES\",\n","    \"PRIMARY_DIAGNOSIS\",\n","    \"SECONDARY_DIAGNOSIS\",\n","    \"TERTIARY_DIAGNOSIS\"\n",").show(20, truncate=False)\n","\n","print(f\"⏰ Completed in: {time.time() - start_time:.2f}s\")"]},{"cell_type":"markdown","id":"b27d9140-230d-4bc0-91ae-fd9db043e5a5","metadata":{},"source":["## Joining All Features"]},{"cell_type":"code","execution_count":20,"id":"1fe76c11-fb64-4b12-9a37-8efccf76d55d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Joining all features and selecting final features for regression modeling...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["✅ Final modeling dataset created with 24211 records\n","📋 Sample of final modeling dataset:\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 20:56:16 WARN DAGScheduler: Broadcasting large task binary with size 1369.8 KiB\n","25/06/06 20:56:17 WARN DAGScheduler: Broadcasting large task binary with size 1412.6 KiB\n"]},{"name":"stdout","output_type":"stream","text":["+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-----------------+-------------+-----------------+-------------+----------------+----------------+-----------------+----------------+-----------------+-----------------+--------------+--------------+--------------+--------------+------------------+--------------+-------------+--------------+--------------+--------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+-------------+-------------+------------------+-------------+------------------+-------------+-------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+-------------------+------------------+---------------+\n","|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|VITAL_211_AVG    |VITAL_742_AVG|VITAL_646_AVG    |VITAL_618_AVG|VITAL_220045_AVG|VITAL_220210_AVG|VITAL_220277_AVG |VITAL_51_AVG    |VITAL_8368_AVG   |VITAL_52_AVG     |VITAL_8549_AVG|VITAL_5815_AVG|VITAL_5820_AVG|VITAL_8554_AVG|VITAL_8553_AVG    |VITAL_5819_AVG|VITAL_834_AVG|VITAL_3450_AVG|VITAL_8518_AVG|VITAL_3603_AVG|LAB_51221_AVG     |LAB_50971_AVG     |LAB_51006_AVG     |LAB_50983_AVG     |LAB_50902_AVG     |LAB_51265_AVG|LAB_50882_AVG     |LAB_51222_AVG|LAB_51301_AVG|LAB_50931_AVG     |LAB_51249_AVG|LAB_50868_AVG     |LAB_51248_AVG|LAB_51250_AVG|LAB_51279_AVG     |LAB_51277_AVG     |LAB_50960_AVG    |LAB_50893_AVG     |LAB_50970_AVG    |LAB_50820_AVG    |PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n","+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-----------------+-------------+-----------------+-------------+----------------+----------------+-----------------+----------------+-----------------+-----------------+--------------+--------------+--------------+--------------+------------------+--------------+-------------+--------------+--------------+--------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+-------------+-------------+------------------+-------------+------------------+-------------+-------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+-------------------+------------------+---------------+\n","|2.0859      |54                  |1            |0           |1            |1                     |1                |5                     |0               |1                 |1               |NULL             |NULL         |NULL             |NULL         |72.12           |17.2            |97.73076923076923|NULL            |NULL             |NULL             |NULL          |NULL          |NULL          |NULL          |NULL              |NULL          |NULL         |NULL          |NULL          |NULL          |34.2              |4.2               |14.5              |137.5             |108.0             |179.0        |21.5              |11.45        |9.45         |118.0             |33.5         |12.0              |29.85        |89.5         |3.835             |12.75             |1.7              |8.1               |2.3              |NULL             |7                |7                  |7                 |7              |\n","|1.8463      |40                  |1            |0           |0            |3                     |5                |0                     |0               |1                 |0               |91.82142857142857|1.0          |97.5             |23.96        |NULL            |NULL            |NULL             |NULL            |NULL             |NULL             |NULL          |NULL          |NULL          |NULL          |NULL              |NULL          |NULL         |NULL          |NULL          |NULL          |31.2              |4.05              |29.0              |134.5             |103.0             |167.0        |15.5              |10.3         |35.55        |93.5              |32.95        |20.0              |28.85        |87.5         |3.565             |16.700000000000003|1.4              |8.350000000000001 |5.4              |7.45             |1                |8                  |9                 |9              |\n","|2.4403      |59                  |0            |0           |0            |1                     |1                |2                     |1               |2                 |0               |82.9090909090909 |1.0          |96.72727272727273|18.0         |NULL            |NULL            |NULL             |NULL            |NULL             |NULL             |120.0         |60.0          |93.0          |100.0         |35.0              |8.0           |NULL         |NULL          |NULL          |NULL          |28.200000000000003|4.0               |55.0              |129.5             |97.5              |199.0        |19.0              |9.15         |9.7          |128.5             |32.3         |17.0              |28.3         |88.0         |3.215             |16.4              |2.8              |7.05              |4.800000000000001|NULL             |12               |7                  |1                 |15             |\n","|3.2897      |58                  |0            |0           |0            |1                     |5                |0                     |0               |3                 |0               |107.5            |1.0          |99.84375         |18.65625     |NULL            |NULL            |NULL             |108.75          |59.96875         |76.8125          |NULL          |NULL          |NULL          |NULL          |NULL              |NULL          |98.5         |NULL          |NULL          |NULL          |34.6              |4.1               |14.0              |139.0             |110.0             |157.0        |22.0              |11.4         |16.3         |90.0              |33.5         |11.0              |26.6         |80.0         |4.29              |14.7              |1.8              |NULL              |NULL             |7.403333333333332|7                |3                  |7                 |5              |\n","|3.5466      |58                  |1            |1           |0            |1                     |1                |1                     |0               |4                 |0               |72.90625         |1.0          |97.09677419354838|16.03125     |NULL            |NULL            |NULL             |94.1923076923077|44.34615384615385|61.92307692307692|120.0         |50.0          |90.0          |100.0         |32.166666666666664|8.0           |97.6         |NULL          |NULL          |NULL          |32.28             |3.9499999999999993|46.857142857142854|136.42857142857142|104.57142857142857|231.0        |23.285714285714285|11.45        |12.125       |208.28571428571428|35.0         |12.428571428571429|31.35        |89.75        |3.6550000000000002|13.025            |1.866666666666667|7.8500000000000005|2.733333333333333|7.32875          |7                |3                  |7                 |4              |\n","+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-----------------+-------------+-----------------+-------------+----------------+----------------+-----------------+----------------+-----------------+-----------------+--------------+--------------+--------------+--------------+------------------+--------------+-------------+--------------+--------------+--------------+------------------+------------------+------------------+------------------+------------------+-------------+------------------+-------------+-------------+------------------+-------------+------------------+-------------+-------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+-------------------+------------------+---------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"📊 Joining all features and selecting final features for regression modeling...\")\n","\n","# Define feature columns to exclude\n","exclude_columns = {\"ICUSTAY_ID\", \"HADM_ID\", \"SUBJECT_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"}\n","\n","# Join all features and immediately select desired columns\n","modeling_dataset = base_icu_df \\\n","    .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n","    .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n","    .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n","    .select(*[name for name in base_icu_df \\\n","        .join(vitals_features_top20, \"ICUSTAY_ID\", \"left\") \\\n","        .join(labs_features, \"ICUSTAY_ID\", \"left\") \\\n","        .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n","        .columns if name not in exclude_columns])\n","\n","# Cleanup\n","base_icu_df.unpersist()\n","vitals_features_top20.unpersist()\n","labs_features.unpersist()\n","diagnosis_features.unpersist()\n","\n","# Display final info\n","print(f\"✅ Final modeling dataset created with {modeling_dataset.count()} records\")\n","print(\"📋 Sample of final modeling dataset:\")\n","modeling_dataset.show(5, truncate=False)\n"]},{"cell_type":"markdown","id":"22d8fbbc","metadata":{},"source":["## Normalization & Handeling Missing Values"]},{"cell_type":"markdown","id":"b87356b3","metadata":{},"source":["Display Missing Values by column."]},{"cell_type":"code","execution_count":null,"id":"0a7a7674","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import sum as spark_sum\n","\n","null_counts = modeling_dataset.select(\n","    [spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in modeling_dataset.columns]\n",").collect()[0]\n","\n","\n","\n","#null_counts_dict = {col: null_counts[col] for col in modeling_dataset.columns}\n","#print(null_counts_dict)"]},{"cell_type":"markdown","id":"5a097880","metadata":{},"source":["## Normalization & Missing Values Strategy\n","\n","**Approach**: Min-Max scaling chosen over standardization because -1 represents missing values. With standardization (Gaussian approximation), -1 could correspond to an actual test result rather than indicating a missing/non-existent test result.\n","\n","**Implementation**: Applied only to float columns since others are binary or integer (like age). Final results limited to 3 decimal places maximum."]},{"cell_type":"code","execution_count":null,"id":"2414bde8","metadata":{},"outputs":[],"source":["# from pyspark.sql.functions import min as spark_min, max as spark_max\n","\n","# print(\"📊 Filling NULL entries with -1...\")\n","# std_columns = [c for c in modeling_dataset.columns if c.endswith('_AVG')]\n","\n","# modeling_dataset = modeling_dataset.na.fill(-1)\n","\n","# print(\"📊 Computing min-max scaling in _AVG columns, excluding -1 entries...\")\n","# min_max_values = {}\n","# for col_name in std_columns:\n","#     stats = modeling_dataset.filter(col(col_name) != -1.0).agg(\n","#         spark_min(col(col_name)).alias(\"min\"),\n","#         spark_max(col(col_name)).alias(\"max\")\n","#     ).first()\n","#     min_max_values[col_name] = (stats[\"min\"], stats[\"max\"])\n","\n","# for col_name in std_columns:\n","#     min_val, max_val = min_max_values[col_name]\n","#     range_val = max_val - min_val if max_val != min_val else 1.0\n","#     modeling_dataset = modeling_dataset.withColumn(\n","#         col_name, \n","#         when(col(col_name) == -1.0, -1.0).otherwise(\n","#             round((col(col_name) - min_val) / range_val, 5)\n","#         )\n","#     )\n","\n","# print(\"✅ Data set ready for Machine Learning!\")\n","# modeling_dataset.show(5, truncate=False)\n","\n","# num_rows = modeling_dataset.count()\n","# num_cols = len(modeling_dataset.columns)\n","# print(f\"Final DataSet shape: ({num_rows}, {num_cols})\")"]},{"cell_type":"code","execution_count":24,"id":"575d68da","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Filling NULL entries with -1...\n","📊 Applying MinMaxScaler to _AVG columns...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:13:21 WARN DAGScheduler: Broadcasting large task binary with size 1486.4 KiB\n","Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n","\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n","\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n","25/06/06 21:13:24 WARN DAGScheduler: Broadcasting large task binary with size 1348.1 KiB\n","Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n","\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n","\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n"]},{"name":"stdout","output_type":"stream","text":["✅ Scaled 40 _AVG columns\n","✅ Data set ready for Machine Learning!\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:17:50 WARN DAGScheduler: Broadcasting large task binary with size 1369.1 KiB\n","25/06/06 21:17:52 WARN DAGScheduler: Broadcasting large task binary with size 1580.0 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-------------------+-------------+------------------+-------------------+-------------------+---------------------+---------------------+------------+--------------+------------+-------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------+--------------+--------------+--------------+-------------------+-------------------+--------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n","|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|VITAL_211_AVG      |VITAL_742_AVG|VITAL_646_AVG     |VITAL_618_AVG      |VITAL_220045_AVG   |VITAL_220210_AVG     |VITAL_220277_AVG     |VITAL_51_AVG|VITAL_8368_AVG|VITAL_52_AVG|VITAL_8549_AVG     |VITAL_5815_AVG    |VITAL_5820_AVG   |VITAL_8554_AVG    |VITAL_8553_AVG     |VITAL_5819_AVG     |VITAL_834_AVG|VITAL_3450_AVG|VITAL_8518_AVG|VITAL_3603_AVG|LAB_51221_AVG      |LAB_50971_AVG      |LAB_51006_AVG       |LAB_50983_AVG     |LAB_50902_AVG     |LAB_51265_AVG       |LAB_50882_AVG      |LAB_51222_AVG      |LAB_51301_AVG       |LAB_50931_AVG      |LAB_51249_AVG     |LAB_50868_AVG      |LAB_51248_AVG     |LAB_51250_AVG     |LAB_51279_AVG      |LAB_51277_AVG     |LAB_50960_AVG      |LAB_50893_AVG      |LAB_50970_AVG     |LAB_50820_AVG     |PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n","+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-------------------+-------------+------------------+-------------------+-------------------+---------------------+---------------------+------------+--------------+------------+-------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------+--------------+--------------+--------------+-------------------+-------------------+--------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n","|1.0799      |71                  |1            |0           |1            |2                     |5                |1                     |0               |3                 |0               |0.0                |0.0          |0.0               |0.0                |0.366692318455296  |2.0055479568351013E-4|0.0023037866726350036|0.0         |0.0           |0.0         |0.0                |0.0               |0.0              |0.0               |0.0                |0.0                |0.0          |0.5           |0.5           |0.5           |0.4972089314194577 |0.5155131264916468 |0.07392996108949416 |0.7739251040221914|0.7801418439716312|0.07062275509687864 |0.5102040816326531 |0.5691446842525979 |0.026637047093065115|0.15384615384615385|0.9182847896440129|0.25609756097560976|0.713245997088792 |0.6692307692307693|0.5451925056619312 |0.4917574741547918|0.28947368421052627|0.0                |0.0               |0.9645937731044114|7                |7                  |17                |11             |\n","|0.9434      |22                  |1            |1           |0            |1                     |1                |2                     |1               |5                 |0               |0.455026455026455  |1.0          |0.9867986798679869|0.3226433430515063 |0.0                |0.0                  |0.0                  |0.0         |0.0           |0.0         |0.37694704049844235|0.2834078972962459|0.900990099009901|0.9099099099099099|0.23664122137404578|0.09574468085106383|0.0          |0.5           |0.5           |0.5           |0.6985645933014353 |0.4295942720763723 |0.054474708171206226|0.7766990291262136|0.776595744680851 |0.11230839277583853 |0.47959183673469385|0.7841726618705037 |0.0317484804541668  |0.1021978021978022 |0.8932038834951456|0.3048780487804878 |0.7270742358078602|0.7000000000000001|0.7096973440395304 |0.4593461860854987|0.25438596491228066|0.48691099476439786|0.261904761904762 |0.9614721104082805|17               |19                 |-1                |2              |\n","|3.1657      |58                  |1            |1           |0            |1                     |1                |1                     |2               |1                 |0               |0.0                |0.0          |0.0               |0.0                |0.4853620194404893 |3.13623559202113E-4  |0.0022750258995318587|0.0         |0.0           |0.0         |0.0                |0.0               |0.0              |0.0               |0.0                |0.0                |0.0          |0.5           |0.5           |0.5           |0.356060606060606  |0.4964200477326969 |0.10894941634241245 |0.7877947295423023|0.8085106382978723|0.13446653513431478 |0.4081632653061224 |0.407673860911271  |0.005831353552806147|0.12197802197802197|0.8495145631067961|0.34146341463414637|0.6954148471615722|0.7038461538461539|0.42495367510809146|0.5901089689857503|0.25438596491228066|0.4554973821989528 |0.2797619047619048|0.9643473260494537|2                |10                 |7                 |21             |\n","|1.8369      |58                  |1            |1           |0            |1                     |1                |1                     |2               |1                 |0               |0.0                |0.0          |0.0               |0.0                |0.39901357212816574|2.207062344842456E-4 |0.002223841472822871 |0.0         |0.0           |0.0         |0.0                |0.0               |0.0              |0.0               |0.0                |0.0                |0.0          |0.5           |0.5           |0.5           |0.3680223285486443 |0.45346062052505964|0.05966277561608301 |0.7674526121128064|0.7423167848699763|0.035614913745130775|0.5306122448979591 |0.4412470023980815 |0.004643485236493783|0.13406593406593406|0.9283980582524272|0.3333333333333333 |0.7641921397379914|0.7096153846153846|0.4221741815935762 |0.6311818943839062|0.26900584795321636|0.5043630017452007 |0.2162698412698413|0.0               |2                |10                 |7                 |21             |\n","|0.9132      |57                  |0            |1           |0            |1                     |3                |2                     |1               |4                 |0               |0.44160907318802056|1.0          |0.9818481848184819|0.48657357679914065|0.0                |0.0                  |0.0                  |0.0         |0.0           |0.0         |0.3925233644859813 |0.3075277183427349|0.900990099009901|0.9099099099099099|0.23664122137404578|0.09574468085106383|0.0          |0.5           |0.5           |0.5           |0.42663476874003187|0.5107398568019093 |0.16926070038910507 |0.7877947295423023|0.7624113475177304|0.23463348004249507 |0.5306122448979591 |0.47002398081534774|0.03272037271296782 |0.0521978021978022 |0.8519417475728154|0.3536585365853659 |0.6506550218340611|0.6576923076923077|0.5003088326127239 |0.5481978206202851|0.2719298245614035 |0.5392670157068062 |0.386904761904762 |0.0               |7                |10                 |3                 |10             |\n","+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+-------------------+-------------+------------------+-------------------+-------------------+---------------------+---------------------+------------+--------------+------------+-------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------+--------------+--------------+--------------+-------------------+-------------------+--------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 1953:====================================================> (31 + 1) / 32]\r"]},{"name":"stdout","output_type":"stream","text":["Final DataSet shape: (24211, 55)\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n","from pyspark.sql.functions import col\n","from pyspark.ml import Pipeline\n","\n","print(\"📊 Filling NULL entries with -1...\")\n","modeling_dataset = modeling_dataset.na.fill(-1)\n","\n","print(\"📊 Applying MinMaxScaler to _AVG columns...\")\n","std_columns = [c for c in modeling_dataset.columns if c.endswith('_AVG')]\n","\n","if std_columns:\n","    # Create vector assembler for the columns to scale\n","    assembler = VectorAssembler(\n","        inputCols=std_columns,\n","        outputCol=\"features_to_scale\",\n","        handleInvalid=\"keep\"\n","    )\n","    \n","    # Create MinMaxScaler\n","    scaler = MinMaxScaler(\n","        inputCol=\"features_to_scale\",\n","        outputCol=\"scaled_features\"\n","    )\n","    \n","    # Create pipeline\n","    pipeline = Pipeline(stages=[assembler, scaler])\n","    \n","    # Fit and transform\n","    scaler_model = pipeline.fit(modeling_dataset)\n","    scaled_data = scaler_model.transform(modeling_dataset)\n","    \n","    # Extract scaled values back to individual columns\n","    from pyspark.ml.functions import vector_to_array\n","    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n","    \n","    # Replace original columns with scaled values\n","    for i, col_name in enumerate(std_columns):\n","        scaled_data = scaled_data.withColumn(\n","            col_name,\n","            scaled_data[\"scaled_array\"][i]\n","        )\n","    \n","    # Drop temporary columns\n","    modeling_dataset = scaled_data.drop(\"features_to_scale\", \"scaled_features\", \"scaled_array\")\n","    \n","    print(f\"✅ Scaled {len(std_columns)} _AVG columns\")\n","else:\n","    print(\"⚠️ No _AVG columns found to scale\")\n","\n","print(\"✅ Data set ready for Machine Learning!\")\n","modeling_dataset.show(5, truncate=False)\n","\n","num_rows = modeling_dataset.count()\n","num_cols = len(modeling_dataset.columns)\n","print(f\"Final DataSet shape: ({num_rows}, {num_cols})\")"]},{"cell_type":"markdown","id":"8e31c2db","metadata":{},"source":["# Machine Learning"]},{"cell_type":"markdown","id":"5fa7f975-668b-4ab3-a228-4af82187778e","metadata":{},"source":["## Preparing for Machine Learning"]},{"cell_type":"code","execution_count":25,"id":"50fa7d59-2d01-4dcf-868f-900431b44be9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 1: Creating train/test split...\n","✅ Data split completed.\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:25:04 WARN DAGScheduler: Broadcasting large task binary with size 1369.1 KiB\n","25/06/06 21:25:05 WARN DAGScheduler: Broadcasting large task binary with size 1579.4 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["   🚆 Training samples: 19502\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:29:19 WARN DAGScheduler: Broadcasting large task binary with size 1369.1 KiB\n","25/06/06 21:29:21 WARN DAGScheduler: Broadcasting large task binary with size 1579.4 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["   🧪 Test samples: 4709\n","Feature columns: ['AGE_AT_ICU_ADMISSION', 'GENDER_BINARY', 'CAME_FROM_ER', 'HAS_INSURANCE', 'ADMISSION_TYPE_ENCODED', 'ETHNICITY_ENCODED', 'MARITAL_STATUS_ENCODED', 'RELIGION_ENCODED', 'FIRST_UNIT_ENCODED', 'CHANGED_ICU_UNIT', 'VITAL_211_AVG', 'VITAL_742_AVG', 'VITAL_646_AVG', 'VITAL_618_AVG', 'VITAL_220045_AVG', 'VITAL_220210_AVG', 'VITAL_220277_AVG', 'VITAL_51_AVG', 'VITAL_8368_AVG', 'VITAL_52_AVG', 'VITAL_8549_AVG', 'VITAL_5815_AVG', 'VITAL_5820_AVG', 'VITAL_8554_AVG', 'VITAL_8553_AVG', 'VITAL_5819_AVG', 'VITAL_834_AVG', 'VITAL_3450_AVG', 'VITAL_8518_AVG', 'VITAL_3603_AVG', 'LAB_51221_AVG', 'LAB_50971_AVG', 'LAB_51006_AVG', 'LAB_50983_AVG', 'LAB_50902_AVG', 'LAB_51265_AVG', 'LAB_50882_AVG', 'LAB_51222_AVG', 'LAB_51301_AVG', 'LAB_50931_AVG', 'LAB_51249_AVG', 'LAB_50868_AVG', 'LAB_51248_AVG', 'LAB_51250_AVG', 'LAB_51279_AVG', 'LAB_51277_AVG', 'LAB_50960_AVG', 'LAB_50893_AVG', 'LAB_50970_AVG', 'LAB_50820_AVG', 'PRIMARY_DIAGNOSIS', 'SECONDARY_DIAGNOSIS', 'TERTIARY_DIAGNOSIS', 'TOTAL_DIAGNOSES']\n","Target column: ICU_LOS_DAYS\n","📊 Step 2: Creating the final vectorized train/test datasets...\n","✅ Final datasets prepared:\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:33:34 WARN DAGScheduler: Broadcasting large task binary with size 1369.1 KiB\n","25/06/06 21:33:36 WARN DAGScheduler: Broadcasting large task binary with size 1626.3 KiB\n","25/06/06 21:33:43 WARN DAGScheduler: Broadcasting large task binary with size 1630.6 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["   🚆 Training features shape: (19330, 54)\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:37:55 WARN DAGScheduler: Broadcasting large task binary with size 1369.1 KiB\n","25/06/06 21:37:56 WARN DAGScheduler: Broadcasting large task binary with size 1626.3 KiB\n","25/06/06 21:38:01 WARN DAGScheduler: Broadcasting large task binary with size 1630.6 KiB\n","[Stage 3016:==================================================>   (30 + 2) / 32]\r"]},{"name":"stdout","output_type":"stream","text":["   🧪 Test features shape: (4881, 54)\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"📊 Step 1: Creating train/test split...\")\n","train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=42)\n","\n","print(\"✅ Data split completed.\")\n","print(f\"   🚆 Training samples: {train_data.count()}\")\n","print(f\"   🧪 Test samples: {test_data.count()}\")\n","\n","\n","feature_columns = [col for col in modeling_dataset.columns if col != 'ICU_LOS_DAYS']\n","print(\"Feature columns:\", feature_columns)\n","target_column = 'ICU_LOS_DAYS'\n","print(\"Target column:\", target_column)\n","\n","feature_assembler = VectorAssembler(\n","    inputCols=feature_columns,  \n","    outputCol=\"features\"     \n",")\n","\n","print(\"📊 Step 2: Creating the final vectorized train/test datasets...\")\n","train_final = feature_assembler.transform(train_data).select(\n","    \"features\", \n","    target_column\n",").withColumnRenamed(target_column, \"label\")\n","\n","test_final = feature_assembler.transform(test_data).select(\n","    \"features\", \n","    target_column\n",").withColumnRenamed(target_column, \"label\")\n","\n","train_final.cache()\n","test_final.cache()\n","\n","print(\"✅ Final datasets prepared:\")\n","print(f\"   🚆 Training features shape: ({train_final.count()}, {len(feature_columns)})\")\n","print(f\"   🧪 Test features shape: ({test_final.count()}, {len(feature_columns)})\")"]},{"cell_type":"markdown","id":"f0c98beb-ae1b-445a-aa7f-9002b4d9a012","metadata":{},"source":["## Training Multiple Models"]},{"cell_type":"code","execution_count":26,"id":"eda7d1c4-c1ef-4261-a3b5-e05db415626d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["📊 Step 1: Setting up evaluation metrics...\n","✅ Evaluation metrics configured: RMSE, MAE, R²\n"]}],"source":["print(\"📊 Step 1: Setting up evaluation metrics...\")\n","\n","# Create regression evaluators\n","rmse_evaluator = RegressionEvaluator(\n","    labelCol=\"label\", \n","    predictionCol=\"prediction\", \n","    metricName=\"rmse\"\n",")\n","\n","mae_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\", \n","    metricName=\"mae\"\n",")\n","\n","r2_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\",\n","    metricName=\"r2\"\n",")\n","\n","print(\"✅ Evaluation metrics configured: RMSE, MAE, R²\")"]},{"cell_type":"markdown","id":"b0f74323-98d0-44fd-b21f-f2e543449457","metadata":{},"source":["### Linear Regression"]},{"cell_type":"code","execution_count":27,"id":"4a25b20c-1f60-4782-b27c-6fda0121af85","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","📈 Step 2: Training Linear Regression model...\n","🕐 Started at: 21:38:04\n","   🔄 Training Linear Regression...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:38:06 WARN DAGScheduler: Broadcasting large task binary with size 1656.6 KiB\n","25/06/06 21:38:08 WARN DAGScheduler: Broadcasting large task binary with size 1657.6 KiB\n","25/06/06 21:38:12 WARN DAGScheduler: Broadcasting large task binary with size 1650.7 KiB\n","25/06/06 21:38:14 WARN DAGScheduler: Broadcasting large task binary with size 1651.8 KiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["   🔄 Linear Regression - Making predictions (test data)...\n","   🔄 Linear Regression - Evaluation...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:38:15 WARN DAGScheduler: Broadcasting large task binary with size 1652.7 KiB\n","25/06/06 21:38:17 WARN DAGScheduler: Broadcasting large task binary with size 1653.8 KiB\n","25/06/06 21:38:18 WARN DAGScheduler: Broadcasting large task binary with size 1652.7 KiB\n","25/06/06 21:38:20 WARN DAGScheduler: Broadcasting large task binary with size 1653.8 KiB\n","25/06/06 21:38:21 WARN DAGScheduler: Broadcasting large task binary with size 1652.7 KiB\n","25/06/06 21:38:22 WARN DAGScheduler: Broadcasting large task binary with size 1653.8 KiB\n"]},{"name":"stdout","output_type":"stream","text":["✅ Linear Regression Results:\n","   📉 RMSE: 1.563 days\n","   📊 MAE: 1.140 days\n","   📈 R²: 0.177\n","🕐 Completed at: 21:38:23\n","⏱️ Total elapsed time: 18.44 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"\\n📈 Step 2: Training Linear Regression model...\")\n","print(f\"🕐 Started at: {datetime.now().strftime('%H:%M:%S')}\")\n","start_time = time.time()\n","\n","# Create Linear Regression model\n","lr = LinearRegression(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    maxIter=200,                    # Increased for better convergence\n","    regParam=0.001,                 # Lower regularization for healthcare data\n","    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n","    tol=1e-8,                       # Tighter tolerance for precision\n","    standardization=False,          # We're doing manual scaling\n","    fitIntercept=True,\n","    aggregationDepth=3,             # Better for distributed training\n","    loss=\"squaredError\",\n","    solver=\"normal\"                 # Best for small-medium datasets\n",")\n","\n","\n","# Train the model\n","print(\"   🔄 Training Linear Regression...\")\n","lr_model = lr.fit(train_final)\n","\n","print(\"   🔄 Linear Regression - Making predictions (test data)...\")\n","lr_predictions = lr_model.transform(test_final)\n","\n","print(\"   🔄 Linear Regression - Evaluation...\")\n","lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n","lr_mae = mae_evaluator.evaluate(lr_predictions)\n","lr_r2 = r2_evaluator.evaluate(lr_predictions)\n","\n","print(f\"✅ Linear Regression Results:\")\n","print(f\"   📉 RMSE: {lr_rmse:.3f} days\")\n","print(f\"   📊 MAE: {lr_mae:.3f} days\")\n","print(f\"   📈 R²: {lr_r2:.3f}\")\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"🕐 Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n","print(f\"⏱️ Total elapsed time: {elapsed_time:.2f} seconds\")"]},{"cell_type":"markdown","id":"6847f98b-4da8-41ff-b26f-f2943c8baa38","metadata":{},"source":["### Random Forest"]},{"cell_type":"code","execution_count":28,"id":"52707ea4-eb6a-449d-8160-7013057b2c3f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","🌲 Step 3: Training Random Forest model...\n","🕐 Started at: 21:38:23\n","   🔄 Training Random Forest...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:38:24 WARN DAGScheduler: Broadcasting large task binary with size 1654.6 KiB\n","25/06/06 21:38:24 WARN DAGScheduler: Broadcasting large task binary with size 1654.7 KiB\n","25/06/06 21:38:25 WARN DAGScheduler: Broadcasting large task binary with size 1657.8 KiB\n","25/06/06 21:38:29 WARN DAGScheduler: Broadcasting large task binary with size 1690.2 KiB\n","25/06/06 21:38:34 WARN DAGScheduler: Broadcasting large task binary with size 1761.0 KiB\n","25/06/06 21:38:38 WARN DAGScheduler: Broadcasting large task binary with size 1890.3 KiB\n","25/06/06 21:38:42 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n","25/06/06 21:38:48 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n","25/06/06 21:38:56 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n","25/06/06 21:39:06 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n","25/06/06 21:39:16 WARN DAGScheduler: Broadcasting large task binary with size 1144.7 KiB\n","25/06/06 21:39:22 WARN DAGScheduler: Broadcasting large task binary with size 8.9 MiB\n","25/06/06 21:39:38 WARN DAGScheduler: Broadcasting large task binary with size 2036.2 KiB\n","25/06/06 21:39:47 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB\n","25/06/06 21:40:15 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n","25/06/06 21:40:26 WARN DAGScheduler: Broadcasting large task binary with size 23.3 MiB\n","25/06/06 21:41:03 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n","25/06/06 21:41:18 WARN DAGScheduler: Broadcasting large task binary with size 27.2 MiB\n","25/06/06 21:41:56 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n","25/06/06 21:42:13 WARN DAGScheduler: Broadcasting large task binary with size 30.3 MiB\n","25/06/06 21:42:54 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n","25/06/06 21:43:09 WARN DAGScheduler: Broadcasting large task binary with size 20.1 MiB\n","25/06/06 21:43:37 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n","25/06/06 21:43:49 WARN DAGScheduler: Broadcasting large task binary with size 14.9 MiB\n","25/06/06 21:44:10 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n","WARNING: An illegal reflective access operation has occurred                    \n","WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.3.jar) to field java.nio.charset.Charset.name\n","WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n","WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n","WARNING: All illegal access operations will be denied in a future release\n"]},{"name":"stdout","output_type":"stream","text":["   🔄 Random Forest - Making predictions (test data)...\n","   🔄 Random Forest - Evaluation...\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:44:22 WARN DAGScheduler: Broadcasting large task binary with size 1647.4 KiB\n","25/06/06 21:44:29 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n","25/06/06 21:44:30 WARN DAGScheduler: Broadcasting large task binary with size 1647.4 KiB\n","25/06/06 21:44:33 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n","25/06/06 21:44:34 WARN DAGScheduler: Broadcasting large task binary with size 1647.4 KiB\n","25/06/06 21:44:37 WARN DAGScheduler: Broadcasting large task binary with size 1648.5 KiB\n"]},{"name":"stdout","output_type":"stream","text":["✅ Random Forest Results:\n","   📉 RMSE: 1.535 days\n","   📊 MAE: 1.125 days\n","   📈 R²: 0.206\n","🕐 Completed at: 21:44:38\n","⏱️ Total elapsed time: 375.01 seconds\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["\n","print(\"\\n🌲 Step 3: Training Random Forest model...\")\n","print(f\"🕐 Started at: {datetime.now().strftime('%H:%M:%S')}\")\n","start_time = time.time()\n","\n","# Create Random Forest model\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    numTrees=200,                   # More trees = better accuracy (if enough cores/memory)\n","    maxDepth=12,                    # Deeper trees capture more complexity\n","    minInstancesPerNode=2,          # Allows more granular splits\n","    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n","    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n","    seed=42                         # Reproducibility\n",")\n","\n","print(\"   🔄 Training Random Forest...\")\n","rf_model = rf.fit(train_final)\n","\n","print(\"   🔄 Random Forest - Making predictions (test data)...\")\n","rf_predictions = rf_model.transform(test_final)\n","\n","print(\"   🔄 Random Forest - Evaluation...\")\n","rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n","rf_mae = mae_evaluator.evaluate(rf_predictions)\n","rf_r2 = r2_evaluator.evaluate(rf_predictions)\n","\n","print(f\"✅ Random Forest Results:\")\n","print(f\"   📉 RMSE: {rf_rmse:.3f} days\")\n","print(f\"   📊 MAE: {rf_mae:.3f} days\")\n","print(f\"   📈 R²: {rf_r2:.3f}\")\n","\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"🕐 Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n","print(f\"⏱️ Total elapsed time: {elapsed_time:.2f} seconds\")\n"]},{"cell_type":"markdown","id":"5d28b690","metadata":{},"source":["## Model Predictions"]},{"cell_type":"code","execution_count":29,"id":"e59901b5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","📈 Linear Regression Predictions:\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:44:41 WARN DAGScheduler: Broadcasting large task binary with size 1642.4 KiB\n"]},{"name":"stdout","output_type":"stream","text":["+----------+-------------+--------------+-------------+\n","|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n","+----------+-------------+--------------+-------------+\n","|0.1121    |1.392        |1.28          |1141.66      |\n","|0.3678    |1.75         |1.382         |375.86       |\n","|0.4959    |1.31         |0.814         |164.14       |\n","|0.5787    |2.354        |1.775         |306.73       |\n","|0.6307    |2.051        |1.421         |225.25       |\n","|0.6866    |1.91         |1.224         |178.25       |\n","|0.7484    |2.405        |1.657         |221.41       |\n","|0.7872    |2.565        |1.777         |225.78       |\n","|0.8263    |1.662        |0.836         |101.12       |\n","|0.8311    |1.736        |0.905         |108.93       |\n","|0.8354    |2.943        |2.107         |252.27       |\n","|0.8381    |1.759        |0.921         |109.88       |\n","|0.841     |1.706        |0.865         |102.81       |\n","|0.8604    |1.376        |0.516         |59.94        |\n","|0.8787    |2.235        |1.357         |154.4        |\n","|0.8875    |1.168        |0.28          |31.56        |\n","|0.9203    |1.741        |0.821         |89.23        |\n","|0.9811    |2.204        |1.223         |124.68       |\n","|1.001     |2.817        |1.816         |181.4        |\n","|1.0033    |2.025        |1.021         |101.8        |\n","+----------+-------------+--------------+-------------+\n","only showing top 20 rows\n","\n","\n","🌲 Random Forest Predictions:\n","+----------+-------------+--------------+-------------+\n","|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n","+----------+-------------+--------------+-------------+\n","|0.1121    |1.852        |1.739         |1551.72      |\n","|0.3678    |1.901        |1.533         |416.82       |\n","|0.4959    |1.604        |1.108         |223.45       |\n","|0.5787    |2.652        |2.074         |358.31       |\n","|0.6307    |2.295        |1.665         |263.95       |\n","|0.6866    |1.645        |0.959         |139.64       |\n","|0.7484    |2.444        |1.695         |226.52       |\n","|0.7872    |2.614        |1.827         |232.12       |\n","|0.8263    |1.916        |1.09          |131.92       |\n","|0.8311    |1.726        |0.895         |107.66       |\n","|0.8354    |3.067        |2.232         |267.12       |\n","|0.8381    |1.785        |0.947         |112.97       |\n","|0.841     |1.818        |0.977         |116.19       |\n","|0.8604    |1.622        |0.762         |88.55        |\n","|0.8787    |1.966        |1.087         |123.71       |\n","|0.8875    |1.653        |0.765         |86.23        |\n","|0.9203    |1.856        |0.936         |101.72       |\n","|0.9811    |2.389        |1.408         |143.47       |\n","|1.001     |2.767        |1.766         |176.46       |\n","|1.0033    |2.173        |1.17          |116.61       |\n","+----------+-------------+--------------+-------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["25/06/06 21:44:44 WARN DAGScheduler: Broadcasting large task binary with size 1637.0 KiB\n"]}],"source":["evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n","\n","print(\"\\n📈 Linear Regression Predictions:\")\n","lr_display = lr_predictions.select(\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",")\n","\n","lr_display.show(truncate=False)\n","\n","\n","\n","# Random Forest Predictions\n","print(\"\\n🌲 Random Forest Predictions:\")\n","rf_display = rf_predictions.select(\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",")\n","\n","rf_display.show(truncate=False)"]},{"cell_type":"markdown","id":"5cc0d610-37c0-4c99-9fc0-289846b54033","metadata":{},"source":["## Model Comparison"]},{"cell_type":"code","execution_count":30,"id":"e3ff000d-87eb-4b26-976a-65db81b057f0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","🏆 Step 5: Model Performance Comparison...\n","📊 Model Performance Summary:\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 3294:==================>                                     (1 + 2) / 3]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------+------------------+------------------+-------------------+\n","|Model            |RMSE              |MAE               |R2                 |\n","+-----------------+------------------+------------------+-------------------+\n","|Linear Regression|1.5629916528419205|1.1404842651043723|0.17682275320632723|\n","|Random Forest    |1.5352234861748009|1.1250095878257458|0.20581212601755816|\n","+-----------------+------------------+------------------+-------------------+\n","\n","\n","🥇 Best Models:\n","   🎯 Lowest RMSE: Random Forest (1.535 days)\n","   📈 Highest R²: Random Forest (0.206)\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 3294:=====================================>                  (2 + 1) / 3]\r","\r","                                                                                \r"]}],"source":["print(\"\\n🏆 Step 5: Model Performance Comparison...\")\n","\n","# Create comparison summary\n","results_data = [\n","    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n","    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n","]\n","\n","results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n","\n","print(\"📊 Model Performance Summary:\")\n","results_df.show(truncate=False)\n","\n","\n","best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n","best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n","\n","print(f\"\\n🥇 Best Models:\")\n","print(f\"   🎯 Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n","print(f\"   📈 Highest R²: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"]},{"cell_type":"code","execution_count":null,"id":"4fe4810d","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1485f2d1","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}