{"cells":[{"cell_type":"markdown","id":"f7c8375f-7f35-415f-8288-2ff2193e6af0","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"id":"89ed6638-09bf-4620-89fe-2bb2c00d86e6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… All imports loaded successfully!\n","â° Notebook started at: 2025-05-31 23:16:27\n"]}],"source":["# Core PySpark imports\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","\n","# Machine Learning imports\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n","from pyspark.ml.regression import RandomForestRegressor, LinearRegression #, GBTRegressor\n","#from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n","from pyspark.ml.evaluation import RegressionEvaluator #, MulticlassClassificationEvaluator\n","#from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","#from pyspark.ml import Pipeline\n","\n","from datetime import datetime, timedelta\n","\n","print(\"âœ… All imports loaded successfully!\")\n","print(f\"â° Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"4f9d1810-67b2-471e-97d2-b8fda7db9728","metadata":{},"source":["## Setup Spark Session"]},{"cell_type":"code","execution_count":2,"id":"a15f1fca-5bf8-4e10-bdca-a6faa11ca17a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","25/05/31 23:16:31 INFO SparkEnv: Registering MapOutputTracker\n","25/05/31 23:16:31 INFO SparkEnv: Registering BlockManagerMaster\n","25/05/31 23:16:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/05/31 23:16:31 INFO SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["âœ… Spark session created successfully!\n","ğŸ“Š Spark Version: 3.5.3\n","ğŸ”§ Application Name: Forecast-LOS\n","ğŸ’¾ Available cores: 2\n","\n","â° Spark session initialised at: 2025-05-31 23:16:38\n"]}],"source":["spark = SparkSession.builder \\\n","        .appName(\"Forecast-LOS\") \\\n","        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n","        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n","        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n","        .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n","        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n","        .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n","        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n","        .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\") \\\n","        .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\") \\\n","        .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n","        .config(\"spark.network.timeout\", \"800s\") \\\n","        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n","        .config(\"spark.executor.memory\", \"25g\") \\\n","        .config(\"spark.executor.cores\", \"3\") \\\n","        .config(\"spark.executor.instances\", \"12\") \\\n","        .config(\"spark.driver.memory\", \"8g\") \\\n","        .getOrCreate()\n","\n","print(\"âœ… Spark session created successfully!\")\n","print(f\"ğŸ“Š Spark Version: {spark.version}\")\n","print(f\"ğŸ”§ Application Name: {spark.sparkContext.appName}\")\n","print(f\"ğŸ’¾ Available cores: {spark.sparkContext.defaultParallelism}\")\n","print(f\"\\nâ° Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"7e13163e-55dc-4046-95b0-4815723d0581","metadata":{},"source":["# Load Data"]},{"cell_type":"code","execution_count":3,"id":"62936c54-45ab-4c03-a8ec-fc3d87f68721","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ¥ Loading MIMIC-III CSV files...\n","ğŸ“‚ Loading CHARTEVENTS table... [GZIP COMPRESSED]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading LABEVENTS table... [GZIP COMPRESSED]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading INPUTEVENTS_MV table... [GZIP COMPRESSED]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading ICUSTAYS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading PATIENTS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading ADMISSIONS table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“‚ Loading DIAGNOSES_ICD table...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["\n","âœ… Tables loaded successfully!\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“Š ICUSTAYS: 61,532 rows Ã— 12 columns\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“Š PATIENTS: 46,520 rows Ã— 8 columns\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 19:===================>                                      (1 + 2) / 3]\r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“Š ADMISSIONS: 58,976 rows Ã— 19 columns\n","\n","â° Data loaded at: 2025-05-31 23:17:18\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["MIMIC_PATH = \"gs://dataproc-staging-europe-west4-719881989993-sa4vn92s/mimic-data\"\n","\n","print(\"ğŸ¥ Loading MIMIC-III CSV files...\")\n","\n","\n","print(\"ğŸ“‚ Loading CHARTEVENTS table... [GZIP COMPRESSED]\")\n","chartevents_df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"false\") .csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n","\n","print(\"ğŸ“‚ Loading LABEVENTS table... [GZIP COMPRESSED]\")\n","labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n","\n","print(\"ğŸ“‚ Loading INPUTEVENTS_MV table... [GZIP COMPRESSED]\")\n","inputevents_df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"false\") .csv(f\"{MIMIC_PATH}/INPUTEVENTS_MV.csv.gz\")\n","\n","\n","\n","####\n","\n","print(\"ğŸ“‚ Loading ICUSTAYS table...\")\n","icustays_df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n","\n","print(\"ğŸ“‚ Loading PATIENTS table...\")\n","patients_df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n","\n","print(\"ğŸ“‚ Loading ADMISSIONS table...\")\n","admissions_df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n","\n","\n","print(\"ğŸ“‚ Loading DIAGNOSES_ICD table...\")\n","diagnoses_df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n","\n","\n","\n","# Cache the core tables for better performance\n","#chartevents_df.cache()\n","#labevents_df.cache()\n","#inputevents_df.cache()\n","icustays_df.cache()\n","patients_df.cache() \n","admissions_df.cache()\n","diagnoses_df.cache()\n","\n","\n","\n","\n","# Display basic information about loaded tables\n","print(\"\\nâœ… Tables loaded successfully!\")\n","print(f\"ğŸ“Š ICUSTAYS: {icustays_df.count():,} rows Ã— {len(icustays_df.columns)} columns\")\n","print(f\"ğŸ“Š PATIENTS: {patients_df.count():,} rows Ã— {len(patients_df.columns)} columns\") \n","print(f\"ğŸ“Š ADMISSIONS: {admissions_df.count():,} rows Ã— {len(admissions_df.columns)} columns\")\n","#print(f\"ğŸ“Š CHARTEVENTS: {chartevents_df.count():,} rows Ã— {len(chartevents_df.columns)} columns\")\n","#print(f\"ğŸ“Š LABEVENTS: {labevents_df.count():,} rows Ã— {len(labevents_df.columns)} columns\")\n","#print(f\"ğŸ“Š INPUTEVENTS_MV: {inputevents_df.count():,} rows Ã— {len(inputevents_df.columns)} columns\")\n","\n","\n","\n","print(f\"\\nâ° Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"]},{"cell_type":"markdown","id":"9e8e7157-1230-41d3-8f41-1117b62fdb55","metadata":{},"source":["## Features Engineering\n","\n","Current features for regression:\n","\n","- Demographics (age, gender)\n","- Admission characteristics (emergency vs elective, timing)\n","- ICU unit types and transfers\n","- Time-based features (weekend, night admissions)\n","- Medical data\n"]},{"cell_type":"markdown","id":"8763e6d7-4ba5-439b-8e2b-ba16cf607a3e","metadata":{},"source":["## Extracting Data From ICUSTAYS"]},{"cell_type":"code","execution_count":4,"id":"9f1d7adb-eac0-4071-b252-a04268f0c767","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 1: Creating base ICU dataset with patient demographics...\n"]}],"source":["print(\"ğŸ“Š Step 1: Creating base ICU dataset with patient demographics...\")\n","\n","base_icu_df = icustays_df.alias(\"icu\") \\\n","    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n","    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n","    .select(\n","        # ICU stay identifiers\n","        col(\"icu.ICUSTAY_ID\"),\n","        col(\"icu.SUBJECT_ID\"), \n","        col(\"icu.HADM_ID\"),\n","        \n","        # Target variable - Length of Stay in ICU (days)\n","        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n","        \n","        # ICU characteristics\n","        col(\"icu.FIRST_CAREUNIT\"),\n","        col(\"icu.LAST_CAREUNIT\"), \n","        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n","        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n","        \n","        # Patient demographics\n","        col(\"pat.GENDER\"),\n","        col(\"pat.DOB\"),\n","        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n","        \n","        # Admission details\n","        col(\"adm.ADMITTIME\"),\n","        col(\"adm.DISCHTIME\"), \n","        col(\"adm.ADMISSION_TYPE\"),\n","        col(\"adm.ADMISSION_LOCATION\"),\n","        col(\"adm.INSURANCE\"),\n","        col(\"adm.ETHNICITY\"),\n","        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n","        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n","    )\n","\n","# Calculate age at ICU admission\n","base_icu_df = base_icu_df.withColumn(\n","    \"AGE_AT_ICU_ADMISSION\", \n","    floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)\n",")"]},{"cell_type":"markdown","id":"595638e6-74f7-4f2f-a7e5-1fc692089206","metadata":{},"source":["## Extracting Categorical Features"]},{"cell_type":"code","execution_count":5,"id":"d28d34e6-83c3-40ae-95d0-71f9929397a1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 2: Engineering categorical features...\n"]}],"source":["print(\"ğŸ“Š Step 2: Engineering categorical features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n","    .withColumn(\"IS_EMERGENCY_ADMISSION\", \n","                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1).otherwise(0)) \\\n","    .withColumn(\"IS_ELECTIVE_ADMISSION\", \n","                when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 1).otherwise(0)) \\\n","    .withColumn(\"CAME_FROM_ER\", \n","                when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_MEDICARE\", \n","                when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n","    .withColumn(\"IS_WHITE_ETHNICITY\", \n","                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1).otherwise(0))"]},{"cell_type":"markdown","id":"9ca22bc8-8dd5-4d30-9cb9-0f366da21d76","metadata":{},"source":["## Extracting ICU Unit Types"]},{"cell_type":"code","execution_count":6,"id":"6617c3f2-75b9-4fa0-93a8-7161d0c2dd57","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 3: Creating ICU unit type features...\n"]}],"source":["print(\"ğŸ“Š Step 3: Creating ICU unit type features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"FIRST_UNIT_MICU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_SICU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"SICU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_CSRU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_CCU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"CCU\", 1).otherwise(0)) \\\n","    .withColumn(\"FIRST_UNIT_TSICU\", \n","                when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 1).otherwise(0)) \\\n","    .withColumn(\"CHANGED_ICU_UNIT\", \n","                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))"]},{"cell_type":"markdown","id":"74662ab6-f263-402e-913f-dc04804eadff","metadata":{},"source":["## Extracting Time-based Features"]},{"cell_type":"code","execution_count":7,"id":"ef431386-85f1-4867-9aa1-f5bb84d7c8ae","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 4: Creating time-based features...\n"]}],"source":["print(\"ğŸ“Š Step 4: Creating time-based features...\")\n","\n","base_icu_df = base_icu_df \\\n","    .withColumn(\"ADMISSION_TO_ICU_HOURS\", \n","                (unix_timestamp(\"ICU_INTIME\") - unix_timestamp(\"ADMITTIME\")) / 3600) \\\n","    .withColumn(\"ICU_LOS_HOURS\", col(\"ICU_LOS_DAYS\") * 24) \\\n","    .withColumn(\"WEEKEND_ADMISSION\", \n","                when(dayofweek(\"ICU_INTIME\").isin([1, 7]), 1).otherwise(0)) \\\n","    .withColumn(\"NIGHT_ADMISSION\", \n","                when(hour(\"ICU_INTIME\").between(20, 7), 1).otherwise(0))"]},{"cell_type":"markdown","id":"779ac96b-6e5f-4fdd-84a2-043a0beaa06b","metadata":{},"source":["## Remove Outliers (Excessive Length Of Stay)"]},{"cell_type":"code","execution_count":8,"id":"b6ce0e40-182e-4fbd-bb35-2bb23ed0d0de","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 5: Cleaning target variable...\n"]},{"name":"stderr","output_type":"stream","text":["25/05/31 23:17:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"]},{"data":{"text/plain":["DataFrame[ICUSTAY_ID: int, SUBJECT_ID: int, HADM_ID: int, ICU_LOS_DAYS: double, FIRST_CAREUNIT: string, LAST_CAREUNIT: string, ICU_INTIME: timestamp, ICU_OUTTIME: timestamp, GENDER: string, DOB: timestamp, PATIENT_DIED: int, ADMITTIME: timestamp, DISCHTIME: timestamp, ADMISSION_TYPE: string, ADMISSION_LOCATION: string, INSURANCE: string, ETHNICITY: string, HOSPITAL_DEATH: int, ADMISSION_DIAGNOSIS: string, AGE_AT_ICU_ADMISSION: bigint, GENDER_BINARY: int, IS_EMERGENCY_ADMISSION: int, IS_ELECTIVE_ADMISSION: int, CAME_FROM_ER: int, HAS_MEDICARE: int, IS_WHITE_ETHNICITY: int, FIRST_UNIT_MICU: int, FIRST_UNIT_SICU: int, FIRST_UNIT_CSRU: int, FIRST_UNIT_CCU: int, FIRST_UNIT_TSICU: int, CHANGED_ICU_UNIT: int, ADMISSION_TO_ICU_HOURS: double, ICU_LOS_HOURS: double, WEEKEND_ADMISSION: int, NIGHT_ADMISSION: int]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["print(\"ğŸ“Š Step 5: Cleaning target variable...\")\n","\n","base_icu_df = base_icu_df \\\n","    .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n","    .filter(col(\"ICU_LOS_DAYS\") < 15)  # Remove extreme outliers (>15 days)\n","\n","base_icu_df.cache()"]},{"cell_type":"markdown","id":"d4a1aa14-d750-44e7-9b44-c0f37dc4dfaa","metadata":{},"source":["## Show Dataset Info"]},{"cell_type":"code","execution_count":9,"id":"8d6cadf4-d2ab-4242-808d-73575be75130","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… Master ICU dataset created!\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["ğŸ“ Dataset size: 57,424 ICU stays\n","ğŸ“Š Features created: 36 columns\n","\n","ğŸ“‹ Sample of regression features:\n","+----------+--------------------+-------------+------------+--------------+----------------------+----------------------+\n","|ICUSTAY_ID|AGE_AT_ICU_ADMISSION|GENDER_BINARY|ICU_LOS_DAYS|FIRST_CAREUNIT|IS_EMERGENCY_ADMISSION|ADMISSION_TO_ICU_HOURS|\n","+----------+--------------------+-------------+------------+--------------+----------------------+----------------------+\n","|    280836|                  65|            0|       3.249|          MICU|                     1|     81.79388888888889|\n","|    206613|                  40|            1|      3.2788|          MICU|                     1|  0.024722222222222222|\n","|    220345|                  80|            1|      2.8939|           CCU|                     0|    20.655555555555555|\n","|    249196|                  45|            0|        2.06|          MICU|                     1|     4.278333333333333|\n","|    210407|                  67|            1|      1.6202|           CCU|                     1|  0.034444444444444444|\n","+----------+--------------------+-------------+------------+--------------+----------------------+----------------------+\n","only showing top 5 rows\n","\n","\n","ğŸ“ˆ ICU Length of Stay Statistics:\n","+-------+------------------+\n","|summary|      ICU_LOS_DAYS|\n","+-------+------------------+\n","|  count|             57424|\n","|   mean|2.9715664217052145|\n","| stddev|2.9124911623722074|\n","|    min|            1.0E-4|\n","|    max|           14.9998|\n","+-------+------------------+\n","\n","\n","â° Feature engineering completed at: 2025-05-31 23:17:23\n"]}],"source":["print(\"âœ… Master ICU dataset created!\")\n","print(f\"ğŸ“ Dataset size: {base_icu_df.count():,} ICU stays\")\n","print(f\"ğŸ“Š Features created: {len(base_icu_df.columns)} columns\")\n","\n","# Display sample of the dataset\n","print(\"\\nğŸ“‹ Sample of regression features:\")\n","base_icu_df.select(\n","    \"ICUSTAY_ID\", \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\", \"ICU_LOS_DAYS\", \n","    \"FIRST_CAREUNIT\", \"IS_EMERGENCY_ADMISSION\", \"ADMISSION_TO_ICU_HOURS\"\n",").show(5)\n","\n","# Show basic statistics of target variable\n","print(\"\\nğŸ“ˆ ICU Length of Stay Statistics:\")\n","base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","print(f\"\\nâ° Feature engineering completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"268129f5-72e2-40a5-b676-cbd825ae84c8","metadata":{},"source":["## Extracting Clinical Features"]},{"cell_type":"code","execution_count":10,"id":"9dc92782-4f16-4c82-bd8a-44ed78e92965","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ¯ Processing 57424 ICU stays for 6 vital signs\n","ğŸ“Š Filtering sampled CHARTEVENTS...\n","ğŸ“Š Processing vital signs within first 24 hours...\n","âœ… 24-hour vital signs data ready\n","   ğŸ“Š Processing HEART_RATE...\n","   ğŸ“Š Processing SBP...\n","   ğŸ“Š Processing DBP...\n","   ğŸ“Š Processing RESP_RATE...\n","   ğŸ“Š Processing TEMPERATURE...\n","   ğŸ“Š Processing SPO2...\n","âœ… Vital signs features created for vital_features_count ICU stays\n","ğŸ“Š Sample features:\n"]}],"source":["# Key vital signs ITEMID mappings (common across MIMIC-III)\n","vital_signs_items = {\n","    220045: \"HEART_RATE\",      # Heart Rate\n","    220050: \"SBP\",             # Systolic BP  \n","    220051: \"DBP\",             # Diastolic BP\n","    220210: \"RESP_RATE\",       # Respiratory Rate\n","    223762: \"TEMPERATURE\",     # Temperature Celsius\n","    220277: \"SPO2\"             # Oxygen Saturation\n","}\n","\n","# Get ICU IDs (using existing approach)\n","icu_ids_list = [row[\"ICUSTAY_ID\"] for row in base_icu_df.select(\"ICUSTAY_ID\").collect()]\n","print(f\"ğŸ¯ Processing {len(icu_ids_list)} ICU stays for {len(vital_signs_items)} vital signs\")\n","\n","vital_items_list = list(vital_signs_items.keys())\n","\n","\n","\n","#chartevents_sample = chartevents_df.sample(0.5, seed=42)  # 50% of 33GB\n","#print(\"âœ… Created CHARTEVENTS sample for processing\")\n","\n","\n","\n","# Now apply filters to the SAMPLE (much faster!)\n","print(\"ğŸ“Š Filtering sampled CHARTEVENTS...\")\n","chartevents_prefiltered = chartevents_df \\\n","    .filter(col(\"ITEMID\").isin(vital_items_list)) \\\n","    .filter(col(\"VALUENUM\").isNotNull()) \\\n","    .filter(col(\"VALUENUM\") > 0) \\\n","    .filter(col(\"VALUENUM\") < 1000) \\\n","    .filter(col(\"ICUSTAY_ID\").isin(icu_ids_list)) \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(col(\"CHARTTIME\") >= col(\"ICU_INTIME\")) \\\n","    .filter(col(\"CHARTTIME\") <= col(\"ICU_OUTTIME\")) \\\n","    .drop(\"ICU_INTIME\", \"ICU_OUTTIME\") \\\n","    .repartition(200, \"ICUSTAY_ID\")\n","\n","# This should complete quickly now!\n","#filtered_count = chartevents_prefiltered.count()\n","#print(f\"âœ… Filtered CHARTEVENTS sample: {filtered_count:,} rows\")\n","\n","# Continue with your processing pipeline\n","print(\"ğŸ“Š Processing vital signs within first 24 hours...\")\n","\n","vitals_24h = chartevents_prefiltered.alias(\"ce\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\") \\\n","    .filter(\n","        col(\"ce.CHARTTIME\").between(\n","            col(\"ICU_INTIME\"), \n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","print(\"âœ… 24-hour vital signs data ready\")\n","\n","# Instead of pivot, create features for each vital sign separately (much faster!)\n","vital_signs_items = {\n","    220045: \"HEART_RATE\",\n","    220050: \"SBP\", \n","    220051: \"DBP\",\n","    220210: \"RESP_RATE\",\n","    223762: \"TEMPERATURE\",\n","    220277: \"SPO2\"\n","}\n","\n","# Start with base ICU dataframe\n","vitals_features = base_icu_df.select(\"ICUSTAY_ID\")\n","\n","# Add each vital sign as separate joins (faster than pivot)\n","for itemid, name in vital_signs_items.items():\n","    print(f\"   ğŸ“Š Processing {name}...\")\n","    \n","    vital_stats = vitals_24h \\\n","        .filter(col(\"ITEMID\") == itemid) \\\n","        .groupBy(\"ICUSTAY_ID\") \\\n","        .agg(\n","            avg(\"VALUENUM\").alias(f\"{name}_AVG\"),\n","            min(\"VALUENUM\").alias(f\"{name}_MIN\"),\n","            max(\"VALUENUM\").alias(f\"{name}_MAX\"),\n","            stddev(\"VALUENUM\").alias(f\"{name}_STD\"),\n","            count(\"VALUENUM\").alias(f\"{name}_COUNT\")\n","        )\n","    \n","    # Left join to maintain all ICU stays\n","    vitals_features = vitals_features.join(vital_stats, \"ICUSTAY_ID\", \"left\")\n","\n","#print(\"ğŸ“Š Counting final features...\")\n","#feature_count = vitals_features.count()\n","print(f\"âœ… Vital signs features created for vital_features_count ICU stays\")\n","\n","# Show sample of features\n","#print(\"ğŸ“Š Sample features:\")\n","#vitals_features.show(5)\n","\n"]},{"cell_type":"code","execution_count":11,"id":"5b685f0e-f499-4adf-9fd9-a6f92e22a1cf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","ğŸ§ª Step 2: Creating laboratory features from LABEVENTS...\n","   ğŸ“Š Calculating laboratory statistics (first 24h)...\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 44:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["   âœ… Laboratory features created for 55,760 ICU stays\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"\\nğŸ§ª Step 2: Creating laboratory features from LABEVENTS...\")\n","\n","# Key lab test ITEMID mappings\n","lab_items = {\n","    50912: \"CREATININE\",       # Creatinine\n","    50902: \"CHLORIDE\",         # Chloride\n","    50931: \"GLUCOSE\",          # Glucose\n","    50983: \"SODIUM\",           # Sodium\n","    50971: \"POTASSIUM\",        # Potassium\n","    51222: \"HEMOGLOBIN\",       # Hemoglobin\n","    51265: \"PLATELET\",         # Platelet Count\n","    51301: \"WBC\",              # White Blood Cells\n","    50820: \"PH\"                # pH\n","}\n","\n","# Filter lab events within first 24 hours of ICU stay\n","labs_24h = labevents_df.alias(\"le\") \\\n","    .join(base_icu_df.select(\"ICUSTAY_ID\", \"HADM_ID\", \"ICU_INTIME\"), \"HADM_ID\", \"inner\") \\\n","    .filter(col(\"le.ITEMID\").isin(list(lab_items.keys()))) \\\n","    .filter(col(\"le.VALUENUM\").isNotNull()) \\\n","    .filter(col(\"le.VALUENUM\") > 0) \\\n","    .filter(\n","        col(\"le.CHARTTIME\").between(\n","            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),  # Include pre-ICU labs\n","            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n","        )\n","    )\n","\n","# Calculate lab value statistics\n","print(\"   ğŸ“Š Calculating laboratory statistics (first 24h)...\")\n","\n","labs_stats = labs_24h.groupBy(\"ICUSTAY_ID\", \"ITEMID\") \\\n","    .agg(\n","        avg(\"VALUENUM\").alias(\"avg_value\"),\n","        min(\"VALUENUM\").alias(\"min_value\"),\n","        max(\"VALUENUM\").alias(\"max_value\"),\n","        first(\"VALUENUM\").alias(\"first_value\")  # First available value\n","    )\n","\n","# Pivot lab results\n","labs_features = labs_stats.groupBy(\"ICUSTAY_ID\").pivot(\"ITEMID\").agg(\n","    first(\"avg_value\").alias(\"avg\"),\n","    first(\"first_value\").alias(\"first\")\n",")\n","\n","# Rename lab columns\n","for itemid, name in lab_items.items():\n","    labs_features = labs_features \\\n","        .withColumnRenamed(f\"{itemid}_avg\", f\"{name}_AVG\") \\\n","        .withColumnRenamed(f\"{itemid}_first\", f\"{name}_FIRST\")\n","\n","print(f\"   âœ… Laboratory features created for {labs_features.count():,} ICU stays\")"]},{"cell_type":"code","execution_count":12,"id":"94a414c1-7139-4e27-a5e4-767919a3eead","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","ğŸ¥ Step 3: Creating diagnosis features from ICD codes...\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 51:==============================================>           (4 + 1) / 5]\r"]},{"name":"stdout","output_type":"stream","text":["   âœ… Diagnosis features created for 58,976 admissions\n","\n","â° Clinical features completed at: 2025-05-31 23:20:41\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["print(\"\\nğŸ¥ Step 3: Creating diagnosis features from ICD codes...\")\n","\n","# Count number of diagnoses per admission (comorbidity burden)\n","diagnosis_counts = diagnoses_df.groupBy(\"HADM_ID\") \\\n","    .agg(\n","        count(\"ICD9_CODE\").alias(\"TOTAL_DIAGNOSES\"),\n","        collect_list(\"ICD9_CODE\").alias(\"DIAGNOSIS_CODES\")\n","    )\n","\n","# Create features for common diagnosis categories\n","diagnosis_features = diagnosis_counts \\\n","    .withColumn(\"HAS_SEPSIS\", \n","                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"99591\") | \n","                     array_contains(col(\"DIAGNOSIS_CODES\"), \"99592\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_RESPIRATORY_FAILURE\",\n","                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"51881\") |\n","                     array_contains(col(\"DIAGNOSIS_CODES\"), \"51882\"), 1).otherwise(0)) \\\n","    .withColumn(\"HAS_CARDIAC_ARREST\",\n","                when(array_contains(col(\"DIAGNOSIS_CODES\"), \"4275\"), 1).otherwise(0)) \\\n","    .drop(\"DIAGNOSIS_CODES\")\n","\n","print(f\"   âœ… Diagnosis features created for {diagnosis_features.count():,} admissions\")\n","\n","print(f\"\\nâ° Clinical features completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"b27d9140-230d-4bc0-91ae-fd9db043e5a5","metadata":{},"source":["# Joining All Features"]},{"cell_type":"code","execution_count":13,"id":"1fe76c11-fb64-4b12-9a37-8efccf76d55d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 1: Joining base features with clinical data...\n","   ğŸ«€ Adding vital signs features...\n","   ğŸ§ª Adding laboratory features...\n","   ğŸ¥ Adding diagnosis features...\n","âœ… All features joined! Final Dataset\n","\n","ğŸ”§ Step 2: Handling missing values...\n","âœ… Missing values handled\n","ğŸ”§ Fixing data types for ML...\n","âœ… Data types fixed!\n"]}],"source":["print(\"ğŸ“Š Step 1: Joining base features with clinical data...\")\n","\n","# Start with base ICU dataset\n","final_dataset = base_icu_df\n","\n","print(\"   ğŸ«€ Adding vital signs features...\")\n","final_dataset = final_dataset.join(vitals_features, \"ICUSTAY_ID\", \"left\")\n","\n","print(\"   ğŸ§ª Adding laboratory features...\")\n","final_dataset = final_dataset.join(labs_features, \"ICUSTAY_ID\", \"left\")\n","\n","print(\"   ğŸ¥ Adding diagnosis features...\")\n","final_dataset = final_dataset.join(diagnosis_features, \"HADM_ID\", \"left\")\n","\n","print(f\"âœ… All features joined! Final Dataset\")\n","\n","# ============================================================================\n","# HANDLE MISSING VALUES\n","# ============================================================================\n","\n","print(\"\\nğŸ”§ Step 2: Handling missing values...\")\n","\n","# Fill missing diagnosis counts with 0\n","final_dataset = final_dataset.fillna({\n","    \"TOTAL_DIAGNOSES\": 0,\n","    \"HAS_SEPSIS\": 0, \n","    \"HAS_RESPIRATORY_FAILURE\": 0,\n","    \"HAS_CARDIAC_ARREST\": 0\n","})\n","\n","# Fill missing vital signs with population medians (approximate values)\n","vital_defaults = {\n","    \"HEART_RATE_AVG\": 80, \"HEART_RATE_MIN\": 65, \"HEART_RATE_MAX\": 100, \"HEART_RATE_STD\": 15,\n","    \"SBP_AVG\": 120, \"SBP_MIN\": 100, \"SBP_MAX\": 140, \"SBP_STD\": 20,\n","    \"DBP_AVG\": 70, \"DBP_MIN\": 55, \"DBP_MAX\": 85, \"DBP_STD\": 15,\n","    \"RESP_RATE_AVG\": 18, \"RESP_RATE_MIN\": 12, \"RESP_RATE_MAX\": 24, \"RESP_RATE_STD\": 6,\n","    \"TEMPERATURE_AVG\": 37.0, \"TEMPERATURE_MIN\": 36.5, \"TEMPERATURE_MAX\": 37.5, \"TEMPERATURE_STD\": 0.5,\n","    \"SPO2_AVG\": 97, \"SPO2_MIN\": 95, \"SPO2_MAX\": 99, \"SPO2_STD\": 2\n","}\n","\n","final_dataset = final_dataset.fillna(vital_defaults)\n","\n","# Fill missing lab values with population medians\n","lab_defaults = {\n","    \"CREATININE_AVG\": 1.0, \"CREATININE_FIRST\": 1.0,\n","    \"CHLORIDE_AVG\": 102, \"CHLORIDE_FIRST\": 102,\n","    \"GLUCOSE_AVG\": 120, \"GLUCOSE_FIRST\": 120,\n","    \"SODIUM_AVG\": 140, \"SODIUM_FIRST\": 140,\n","    \"POTASSIUM_AVG\": 4.0, \"POTASSIUM_FIRST\": 4.0,\n","    \"HEMOGLOBIN_AVG\": 11.0, \"HEMOGLOBIN_FIRST\": 11.0,\n","    \"PLATELET_AVG\": 250, \"PLATELET_FIRST\": 250,\n","    \"WBC_AVG\": 8.5, \"WBC_FIRST\": 8.5,\n","    \"PH_AVG\": 7.4, \"PH_FIRST\": 7.4\n","}\n","\n","final_dataset = final_dataset.fillna(lab_defaults)\n","\n","# Fill remaining missing values with 0\n","final_dataset = final_dataset.fillna(0)\n","\n","\n","print(\"âœ… Missing values handled\")\n","\n","print(\"ğŸ”§ Fixing data types for ML...\")\n","\n","# Cast problematic string columns to double\n","from pyspark.sql.functions import col\n","\n","string_columns = [\n","    \"CREATININE_FIRST\", \"GLUCOSE_FIRST\", \"SODIUM_FIRST\", \"POTASSIUM_FIRST\",\n","    \"HEMOGLOBIN_FIRST\", \"PLATELET_FIRST\", \"WBC_FIRST\", \"PH_FIRST\"\n","]\n","\n","for col_name in string_columns:\n","    if col_name in final_dataset.columns:\n","        final_dataset = final_dataset.withColumn(\n","            col_name, \n","            col(col_name).cast(\"double\")\n","        )\n","\n","# Fill any nulls created during conversion\n","final_dataset = final_dataset.fillna({\n","    \"CREATININE_FIRST\": 1.0,\n","    \"GLUCOSE_FIRST\": 120.0,\n","    \"SODIUM_FIRST\": 140.0,\n","    \"POTASSIUM_FIRST\": 4.0,\n","    \"HEMOGLOBIN_FIRST\": 11.0,\n","    \"PLATELET_FIRST\": 250.0,\n","    \"WBC_FIRST\": 8.5,\n","    \"PH_FIRST\": 7.4\n","})\n","\n","print(\"âœ… Data types fixed!\")\n"]},{"cell_type":"code","execution_count":14,"id":"88f6f328-17d8-40a6-ba45-769f22203346","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","ğŸ“‹ Step 3: Selecting final features for regression modeling...\n","âœ… Final modeling dataset prepared!\n","ğŸ“Š Total features: 39 predictive features\n","ğŸ¯ Target variable: ICU_LOS_DAYS (continuous)\n","\n","ğŸ“‹ Feature categories:\n","   ğŸ‘¤ Demographics: 2 features\n","   ğŸ¥ Admission: 8 features\n","   ğŸ¢ ICU Unit: 6 features\n","   ğŸ«€ Vital Signs: 11 features\n","   ğŸ§ª Laboratory: 8 features\n","   ğŸ©º Diagnoses: 4 features\n","\n","â° Dataset preparation completed at: 2025-05-31 23:21:10\n","ğŸš€ Ready for train/test split and model training!\n"]}],"source":["print(\"\\nğŸ“‹ Step 3: Selecting final features for regression modeling...\")\n","\n","# Define feature columns for modeling\n","feature_columns = [\n","    # Demographics\n","    \"AGE_AT_ICU_ADMISSION\", \"GENDER_BINARY\",\n","    \n","    # Admission characteristics\n","    \"IS_EMERGENCY_ADMISSION\", \"IS_ELECTIVE_ADMISSION\", \"CAME_FROM_ER\",\n","    \"HAS_MEDICARE\", \"IS_WHITE_ETHNICITY\", \"ADMISSION_TO_ICU_HOURS\",\n","    \"WEEKEND_ADMISSION\", \"NIGHT_ADMISSION\",\n","    \n","    # ICU unit features\n","    \"FIRST_UNIT_MICU\", \"FIRST_UNIT_SICU\", \"FIRST_UNIT_CSRU\", \n","    \"FIRST_UNIT_CCU\", \"FIRST_UNIT_TSICU\", \"CHANGED_ICU_UNIT\",\n","    \n","    # Vital signs (averages)\n","    \"HEART_RATE_AVG\", \"SBP_AVG\", \"DBP_AVG\", \"RESP_RATE_AVG\", \n","    \"TEMPERATURE_AVG\", \"SPO2_AVG\",\n","    \n","    # Vital signs (variability)\n","    \"HEART_RATE_STD\", \"SBP_STD\", \"DBP_STD\", \"RESP_RATE_STD\", \"SPO2_STD\",\n","    \n","    # Laboratory values\n","    \"CREATININE_FIRST\", \"GLUCOSE_FIRST\", \"SODIUM_FIRST\", \"POTASSIUM_FIRST\",\n","    \"HEMOGLOBIN_FIRST\", \"PLATELET_FIRST\", \"WBC_FIRST\", \"PH_FIRST\",\n","    \n","    # Diagnosis features\n","    \"TOTAL_DIAGNOSES\", \"HAS_SEPSIS\", \"HAS_RESPIRATORY_FAILURE\", \"HAS_CARDIAC_ARREST\"\n","]\n","\n","# Create modeling dataset with selected features\n","modeling_dataset = final_dataset.select(\n","    [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\"] + feature_columns\n",")\n","\n","# Remove any remaining nulls and invalid records\n","modeling_dataset = modeling_dataset.filter(col(\"ICU_LOS_DAYS\").isNotNull()) \\\n","                                 .filter(col(\"ICU_LOS_DAYS\") > 0) \\\n","                                 .filter(col(\"AGE_AT_ICU_ADMISSION\") >= 18)  # Adults only\n","\n","# Cache the final dataset\n","modeling_dataset = modeling_dataset.repartition(50)\n","modeling_dataset.cache()\n","\n","\n","print(f\"âœ… Final modeling dataset prepared!\")\n","#print(f\"ğŸ“ Final dataset: {modeling_dataset.count():,} ICU stays\")\n","print(f\"ğŸ“Š Total features: {len(feature_columns)} predictive features\")\n","print(f\"ğŸ¯ Target variable: ICU_LOS_DAYS (continuous)\")\n","\n","# Show feature summary\n","print(f\"\\nğŸ“‹ Feature categories:\")\n","print(f\"   ğŸ‘¤ Demographics: 2 features\")\n","print(f\"   ğŸ¥ Admission: 8 features\") \n","print(f\"   ğŸ¢ ICU Unit: 6 features\")\n","print(f\"   ğŸ«€ Vital Signs: 11 features\")\n","print(f\"   ğŸ§ª Laboratory: 8 features\")\n","print(f\"   ğŸ©º Diagnoses: 4 features\")\n","\n","# Display sample of final dataset\n","#print(f\"\\nğŸ“‹ Sample of final modeling dataset:\")\n","#modeling_dataset.select(\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"AGE_AT_ICU_ADMISSION\", \n","#                       \"HEART_RATE_AVG\", \"CREATININE_FIRST\", \"HAS_SEPSIS\").show(5)\n","\n","# Basic statistics of target variable\n","#print(f\"\\nğŸ“ˆ Final ICU Length of Stay Statistics:\")\n","#modeling_dataset.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","print(f\"\\nâ° Dataset preparation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(f\"ğŸš€ Ready for train/test split and model training!\")"]},{"cell_type":"markdown","id":"5fa7f975-668b-4ab3-a228-4af82187778e","metadata":{},"source":["## Preparing for Machine Learning"]},{"cell_type":"code","execution_count":15,"id":"50fa7d59-2d01-4dcf-868f-900431b44be9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ“Š Step 1: Creating train/test split...\n","âœ… Data split completed:\n","\n","ğŸ”§ Step 2: Assembling feature vectors...\n","âœ… Feature vectors assembled:\n","   ğŸ“Š Feature vector size: 39 dimensions\n"]},{"data":{"text/plain":["'\\nprint(\"\\nâš–ï¸ Step 3: Scaling features...\")\\n\\n# Create StandardScaler to normalize features\\nscaler = StandardScaler(\\n    inputCol=\"features_raw\",\\n    outputCol=\"features\",\\n    withStd=True,\\n    withMean=True\\n)\\n\\n# Fit scaler on training data\\nscaler_model = scaler.fit(train_assembled)\\ntrain_scaled = scaler_model.transform(train_assembled)\\ntest_scaled = scaler_model.transform(test_assembled)\\n\\n# Cache the final processed datasets\\ntrain_scaled.cache()\\ntest_scaled.cache()\\n\\n\\nprint(f\"âœ… Feature scaling completed:\")\\nprint(f\"   ğŸ“Š Features standardized (mean=0, std=1)\")\\nprint(f\"   ğŸ”§ Scaler fitted on training data only\")\\n'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["print(\"ğŸ“Š Step 1: Creating train/test split...\")\n","\n","# Split the data (80% train, 20% test)\n","train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=42)\n","\n","# Cache both datasets for performance\n","train_data.cache()\n","test_data.cache()\n","\n","print(f\"âœ… Data split completed:\")\n","#print(f\"   ğŸ“ˆ Training set: {train_data.count():,} ICU stays ({train_data.count()/modeling_dataset.count()*100:.1f}%)\")\n","#print(f\"   ğŸ“Š Test set: {test_data.count():,} ICU stays ({test_data.count()/modeling_dataset.count()*100:.1f}%)\")\n","\n","# Show target variable distribution in both sets\n","#print(f\"\\nğŸ“ˆ Target variable distribution:\")\n","#print(f\"Training set LOS statistics:\")\n","#train_data.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","#print(f\"Test set LOS statistics:\")\n","#test_data.select(\"ICU_LOS_DAYS\").describe().show()\n","\n","# ============================================================================\n","# FEATURE VECTOR ASSEMBLY\n","# ============================================================================\n","\n","print(\"\\nğŸ”§ Step 2: Assembling feature vectors...\")\n","\n","# Create feature vector assembler\n","feature_assembler = VectorAssembler(\n","    inputCols=feature_columns,\n","    outputCol=\"features_raw\"\n",")\n","\n","# Apply feature assembler to training data\n","train_assembled = feature_assembler.transform(train_data)\n","test_assembled = feature_assembler.transform(test_data)\n","\n","print(f\"âœ… Feature vectors assembled:\")\n","print(f\"   ğŸ“Š Feature vector size: {len(feature_columns)} dimensions\")\n","\n","# ============================================================================\n","# FEATURE SCALING\n","# ============================================================================\n","'''\n","print(\"\\nâš–ï¸ Step 3: Scaling features...\")\n","\n","# Create StandardScaler to normalize features\n","scaler = StandardScaler(\n","    inputCol=\"features_raw\",\n","    outputCol=\"features\",\n","    withStd=True,\n","    withMean=True\n",")\n","\n","# Fit scaler on training data\n","scaler_model = scaler.fit(train_assembled)\n","train_scaled = scaler_model.transform(train_assembled)\n","test_scaled = scaler_model.transform(test_assembled)\n","\n","# Cache the final processed datasets\n","train_scaled.cache()\n","test_scaled.cache()\n","\n","\n","print(f\"âœ… Feature scaling completed:\")\n","print(f\"   ğŸ“Š Features standardized (mean=0, std=1)\")\n","print(f\"   ğŸ”§ Scaler fitted on training data only\")\n","'''"]},{"cell_type":"markdown","id":"8b1aa2af-6e65-4688-84ac-724a79f8ba00","metadata":{},"source":["## Final Dataset Preparation"]},{"cell_type":"code","execution_count":null,"id":"c2397a40-5d63-475b-a2dc-4b9f2fe4233b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","ğŸ“‹ Step 4: Preparing final ML datasets...\n"]}],"source":["\n","print(\"\\nğŸ“‹ Step 4: Preparing final ML datasets...\")\n","\n","# Select columns needed for modeling\n","ml_columns = [\"ICUSTAY_ID\", \"ICU_LOS_DAYS\", \"features\"]\n","\n","#train_final = train_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","#test_final = test_scaled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","\n","train_final = train_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","test_final = test_assembled.select(ml_columns).withColumnRenamed(\"ICU_LOS_DAYS\", \"label\")\n","\n","print(\"\\nğŸ“‹ Caching...\")\n","\n","# Cache final datasets\n","train_final.cache()\n","test_final.cache()\n","\n","print(f\"âœ… Final ML datasets prepared:\")\n","print(f\"   ğŸ¯ Target variable: 'label' (ICU_LOS_DAYS)\")\n","print(f\"   ğŸ“Š Features: 'features' (scaled vector)\")\n","print(f\"   ğŸ”‘ Identifier: 'ICUSTAY_ID'\")\n","\n","# Show sample of final datasets\n","#print(f\"\\nğŸ“‹ Sample of training data structure:\")\n","#train_final.select(\"ICUSTAY_ID\", \"label\").show(5)\n","\n","print(f\"\\nğŸ“‹ Feature vector example (first 10 features):\")\n","# Show first few elements of feature vector for one sample\n","sample_features = train_final.select(\"features\").take(1)[0][\"features\"]\n","print(f\"   ğŸ“Š Feature vector sample: {sample_features.toArray()[:10]}...\")\n","print(f\"   ğŸ“ Total feature dimensions: {len(sample_features.toArray())}\")\n","\n","# ============================================================================\n","# DATA QUALITY CHECKS\n","# ============================================================================\n","\n","'''\n","print(f\"\\nğŸ” Step 5: Final data quality checks...\")\n","\n","# Check for any remaining nulls\n","train_nulls = train_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n","test_nulls = test_final.filter(col(\"label\").isNull() | col(\"features\").isNull()).count()\n","\n","print(f\"   ğŸ” Null values in training set: {train_nulls}\")\n","print(f\"   ğŸ” Null values in test set: {test_nulls}\")\n","\n","# Show target variable ranges\n","train_stats = train_final.agg(\n","    min(\"label\").alias(\"min_los\"),\n","    max(\"label\").alias(\"max_los\"), \n","    avg(\"label\").alias(\"mean_los\"),\n","    stddev(\"label\").alias(\"std_los\")\n",").collect()[0]\n","\n","print(f\"\\nğŸ“Š Final training set target statistics:\")\n","print(f\"   ğŸ“‰ Min LOS: {train_stats['min_los']:.2f} days\")\n","print(f\"   ğŸ“ˆ Max LOS: {train_stats['max_los']:.2f} days\") \n","print(f\"   ğŸ“Š Mean LOS: {train_stats['mean_los']:.2f} days\")\n","print(f\"   ğŸ“ Std LOS: {train_stats['std_los']:.2f} days\")\n","\n","print(f\"\\nâœ… Data preprocessing completed successfully!\")\n","print(f\"ğŸš€ Ready for model training with {len(feature_columns)} features\")\n","'''\n","\n","print(f\"â° Preprocessing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"]},{"cell_type":"markdown","id":"f0c98beb-ae1b-445a-aa7f-9002b4d9a012","metadata":{},"source":["## Training Multiple Models"]},{"cell_type":"code","execution_count":null,"id":"eda7d1c4-c1ef-4261-a3b5-e05db415626d","metadata":{},"outputs":[],"source":["print(\"ğŸ“Š Step 1: Setting up evaluation metrics...\")\n","\n","# Create regression evaluators\n","rmse_evaluator = RegressionEvaluator(\n","    labelCol=\"label\", \n","    predictionCol=\"prediction\", \n","    metricName=\"rmse\"\n",")\n","\n","mae_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\", \n","    metricName=\"mae\"\n",")\n","\n","r2_evaluator = RegressionEvaluator(\n","    labelCol=\"label\",\n","    predictionCol=\"prediction\",\n","    metricName=\"r2\"\n",")\n","\n","print(\"âœ… Evaluation metrics configured: RMSE, MAE, RÂ²\")"]},{"cell_type":"markdown","id":"6847f98b-4da8-41ff-b26f-f2943c8baa38","metadata":{},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"id":"52707ea4-eb6a-449d-8160-7013057b2c3f","metadata":{},"outputs":[],"source":["print(\"\\nğŸŒ² Step 3: Training Random Forest model...\")\n","\n","# Create Random Forest model\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"label\",\n","    numTrees=30,        # Reduced trees\n","    maxDepth=6,         # Reduced depth\n","    minInstancesPerNode=10,  # More pruning\n","    subsamplingRate=0.8, # Sample training data\n","    seed=42\n",")\n","\n","# Train the model\n","print(\"   ğŸ”„ Training Random Forest...\")\n","rf_model = rf.fit(train_final)\n","\n","# Make predictions\n","rf_predictions = rf_model.transform(test_final)\n","\n","# Evaluate Random Forest\n","rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n","rf_mae = mae_evaluator.evaluate(rf_predictions)\n","rf_r2 = r2_evaluator.evaluate(rf_predictions)\n","\n","print(f\"âœ… Random Forest Results:\")\n","print(f\"   ğŸ“‰ RMSE: {rf_rmse:.3f} days\")\n","print(f\"   ğŸ“Š MAE: {rf_mae:.3f} days\")\n","print(f\"   ğŸ“ˆ RÂ²: {rf_r2:.3f}\")"]},{"cell_type":"markdown","id":"b0f74323-98d0-44fd-b21f-f2e543449457","metadata":{},"source":["### Linear Regression"]},{"cell_type":"code","execution_count":null,"id":"4a25b20c-1f60-4782-b27c-6fda0121af85","metadata":{},"outputs":[],"source":["print(\"\\nğŸ“ˆ Step 2: Training Linear Regression model...\")\n","\n","# Create Linear Regression model\n","lr = LinearRegression(\n","    featuresCol=\"features\",\n","    labelCol=\"label\", \n","    maxIter=20,        # Reduced iterations\n","    regParam=0.1,      # Higher regularization (faster convergence)\n","    elasticNetParam=0.0,\n","    tol=1e-4          # Larger tolerance (stops early)\n",")\n","\n","\n","# Train the model\n","print(\"   ğŸ”„ Training Linear Regression...\")\n","lr_model = lr.fit(train_final)\n","\n","# Make predictions\n","lr_predictions = lr_model.transform(test_final)\n","\n","# Evaluate Linear Regression\n","lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n","lr_mae = mae_evaluator.evaluate(lr_predictions)\n","lr_r2 = r2_evaluator.evaluate(lr_predictions)\n","\n","print(f\"âœ… Linear Regression Results:\")\n","print(f\"   ğŸ“‰ RMSE: {lr_rmse:.3f} days\")\n","print(f\"   ğŸ“Š MAE: {lr_mae:.3f} days\")\n","print(f\"   ğŸ“ˆ RÂ²: {lr_r2:.3f}\")"]},{"cell_type":"markdown","id":"5cc0d610-37c0-4c99-9fc0-289846b54033","metadata":{},"source":["## Model Comparison"]},{"cell_type":"code","execution_count":null,"id":"e3ff000d-87eb-4b26-976a-65db81b057f0","metadata":{},"outputs":[],"source":["print(\"\\nğŸ† Step 5: Model Performance Comparison...\")\n","\n","# Create comparison summary\n","results_data = [\n","    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n","    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n","]\n","\n","results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n","\n","print(\"ğŸ“Š Model Performance Summary:\")\n","results_df.show(truncate=False)\n","\n","# Find best model\n","import operator\n","import builtins\n","best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n","best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n","\n","print(f\"\\nğŸ¥‡ Best Models:\")\n","print(f\"   ğŸ¯ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n","print(f\"   ğŸ“ˆ Highest RÂ²: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"]},{"cell_type":"markdown","id":"400e166a-1c18-4e22-938c-d6a5569bd296","metadata":{},"source":["## Display Predictions"]},{"cell_type":"code","execution_count":null,"id":"068fa69e-a7fb-40f2-a287-6300bf41e03d","metadata":{},"outputs":[],"source":["\n","# Linear Regression Predictions\n","\n","print(\"\\nğŸ“ˆ Linear Regression Predictions (Sample 20):\")\n","lr_display = lr_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","lr_display.show(20, truncate=False)\n","\n","\n","\n","# Random Forest Predictions\n","print(\"\\nğŸŒ² Random Forest Predictions (Sample 20):\")\n","rf_display = rf_predictions.select(\n","    \"ICUSTAY_ID\",\n","    col(\"label\").alias(\"Actual_LOS\"),\n","    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n","    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n","    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",").orderBy(\"ICUSTAY_ID\")\n","\n","rf_display.show(20, truncate=False)"]},{"cell_type":"code","execution_count":null,"id":"905be484-c471-4032-9b55-207d934e8cdb","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"343e733a","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}