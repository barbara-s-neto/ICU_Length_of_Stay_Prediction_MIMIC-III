{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1d1427",
   "metadata": {},
   "source": [
    "# ICU Length of Stay Prediction - MIMIC-III Pipeline\n",
    "\n",
    "## üéØ Objective\n",
    "Predict ICU stay duration using PySpark ML on MIMIC-III dataset\n",
    "\n",
    "## üìä Data & Constraints\n",
    "- **Sources**: 6 MIMIC-III tables (CHARTEVENTS, LABEVENTS, ICUSTAYS, etc.)\n",
    "- **Filters**: \n",
    "        - Patient Age 18-80\n",
    "        - LOS 0.1-15 days\n",
    "        - Valid time sequences\n",
    "- **Timeframe**: Vitals (first 24h), Labs (6h pre to 24h post ICU)\n",
    "\n",
    "\n",
    "## üåÄ Big Data Processing\n",
    "\n",
    "- **Storage**: We used Google Cloud Dataproc and Google Storage Buckets for MIMIC-III storage \n",
    "- **CHARTEVENTS**: Chart Events table has +330 million rows\n",
    "- **Parquet**: Converted \"CHARTEVENTS\" and \"LABEVENTS\" tables to Parquet format for efficient storage and processing\n",
    "- **Filtering**: We filtered immediately when loading to optimize CHARTEVENTS DataFrame\n",
    "\n",
    "## üîß Features (39 total)\n",
    "- **Demographics (2)**: Age, gender\n",
    "- **Admission (8)**: Emergency/elective, timing, insurance\n",
    "- **ICU Units (6)**: Care unit types, transfers\n",
    "- **Vitals (11)**: HR, BP, RR, temp, SpO2 (avg/std)\n",
    "- **Labs (8)**: Creatinine, glucose, electrolytes, blood counts\n",
    "- **Diagnoses (4)**: Total count, sepsis, respiratory failure\n",
    "\n",
    "## ü§ñ Models & Results\n",
    "- **Linear Regression**: \n",
    "- **Random Forest**: \n",
    "\n",
    "## ‚òÅÔ∏è Infrastructure\n",
    "- **GCP Dataproc**: 1x Master and 2x Workers, n2-standard-4  (12 vCPUs, 48GB RAM, 400GB Disk Storage)\n",
    "- **Optimizations**: Smart sampling, aggressive filtering, 80/20 split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a166c4",
   "metadata": {},
   "source": [
    "## Cenas a acresentar no relatorio:\n",
    "\n",
    "* justificar o pq de cada uma das colunas\n",
    "* dar tune aos hiperparametros do modelo\n",
    "* referencias e bibliografias :\n",
    "    *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8375f-7f35-415f-8288-2ff2193e6af0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ed6638-09bf-4620-89fe-2bb2c00d86e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All imports loaded successfully!\n",
      "‚è∞ Notebook started at: 2025-06-07 12:46:29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üì¶ PySpark Core Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, sum as sql_sum, count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# üî¢ Data Processing & Feature Engineering\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StandardScaler,\n",
    "    StringIndexer,\n",
    "    MinMaxScaler,\n",
    "    Imputer\n",
    ")\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "# ü§ñ Machine Learning Models\n",
    "from pyspark.ml.regression import (\n",
    "    RandomForestRegressor,\n",
    "    LinearRegression\n",
    "    # GBTRegressor\n",
    ")\n",
    "\n",
    "\n",
    "# üìä Model Evaluation & Tuning\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import operator\n",
    "import builtins\n",
    "\n",
    "\n",
    "# ‚è±Ô∏è Date/Time Utilities\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ All imports loaded successfully!\")\n",
    "print(f\"‚è∞ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1810-67b2-471e-97d2-b8fda7db9728",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15f1fca-5bf8-4e10-bdca-a6faa11ca17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/07 12:46:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created successfully!\n",
      "üìä Spark Version: 4.0.0\n",
      "üîß Application Name: Forecast-LOS\n",
      "üíæ Available cores: 8\n",
      "\n",
      "‚è∞ Spark session initialised at: 2025-06-07 12:46:33\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Forecast-LOS\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"32MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"300s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"300s\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memoryOffHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOffHeap.size\", \"1g\") \\\n",
    "    \\\n",
    "    .getOrCreate()\n",
    "print(\"‚úÖ Spark session created successfully!\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"üîß Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üíæ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\n‚è∞ Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13163e-55dc-4046-95b0-4815723d0581",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4de5cb",
   "metadata": {},
   "source": [
    "Strategy: Pre-filter CHARTEVENTS to find ICU stays with required vital signs, then efficiently load all tables using broadcast joins and lookup tables.\n",
    "Key Steps:\n",
    "\n",
    "- Filter for ICU stays with ‚â•1 of 6 vital signs (HR, BP, RR, Temp, SpO2)\n",
    "- Create lookup tables for ICUSTAY_ID, HADM_ID, SUBJECT_ID\n",
    "- Load all tables with pre-filtering using broadcast joins\n",
    "- Convert large files to \"Parquet\" for performance\n",
    "\n",
    "Result: Memory-efficient loading of only relevant data with quality assurance that all ICU stays have vital signs measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb886092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Loading MIMIC-III data...\n",
      "üìÇ Loading CHARTEVENTS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:46:34 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data/CHARTEVENTS.parquet.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/07 12:46:34 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data/CHARTEVENTS.csv.gz.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Converting CHARTEVENTS.csv.gz to parquet...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n\t\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\t\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\t\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\t\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\t\tat scala.collection.immutable.List.map(List.scala:247)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m    chartevents_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMIMIC_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/CHARTEVENTS.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Loaded CHARTEVENTS from parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:642\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    633\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    634\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    640\u001b[0m )\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o77.parquet.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n\t\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\t\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\t\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\t\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\t\tat scala.collection.immutable.List.map(List.scala:247)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 23 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÑ Converting CHARTEVENTS.csv.gz to parquet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m    chartevents_csv \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMIMIC_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/CHARTEVENTS.csv.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m    chartevents_csv\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMIMIC_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/CHARTEVENTS.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m    chartevents_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMIMIC_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/CHARTEVENTS.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:838\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3581)\n\t\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\t\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\t\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\t\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\t\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)\n\t\tat scala.collection.immutable.List.map(List.scala:247)\n\t\tat scala.collection.immutable.List.map(List.scala:79)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# Configuration flags\n",
    "SAMPLE_ENABLE = False\n",
    "SAMPLE_SIZE = 20000\n",
    "MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"üè• Loading MIMIC-III data...\")\n",
    "\n",
    "# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "\n",
    "try:\n",
    "   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "   print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "except:\n",
    "   print(\"üìÑ Converting CHARTEVENTS.csv.gz to parquet...\")\n",
    "   chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n",
    "   chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "   print(\"‚úÖ Converted and loaded CHARTEVENTS\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load ICUSTAYS \n",
    "print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Apply sampling if enabled\n",
    "if SAMPLE_ENABLE:\n",
    "   print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "   icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "   icustays_df.cache()\n",
    "   actual_sample_size = icustays_df.count()\n",
    "   print(f\"‚úÖ Final sample: {actual_sample_size} ICU stays\")\n",
    "else:\n",
    "   icustays_df.cache()\n",
    "   actual_sample_size = icustays_df.count()\n",
    "\n",
    "   \n",
    "   \n",
    "# Step 4: Create efficient lookup tables\n",
    "print(\"üìã Creating ID lookup tables...\")\n",
    "icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "\n",
    "icu_lookup.count()  # Trigger caching\n",
    "hadm_lookup.count()\n",
    "subject_lookup.count()\n",
    "\n",
    "# Step 5: Load other tables with optimized joins\n",
    "print(\"üìÇ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n",
    "patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n",
    "admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n",
    "diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Step 6: Load and filter CHARTEVENTS efficiently\n",
    "print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "chartevents_df = chartevents_df \\\n",
    "   .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "   .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "\n",
    "# Step 7: Load LABEVENTS\n",
    "print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "try:\n",
    "   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "except:\n",
    "   print(\"üìÑ Converting LABEVENTS.csv.gz to parquet...\")\n",
    "   labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n",
    "   labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "\n",
    "labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n‚úÖ Data loading complete!\")\n",
    "print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62936c54-45ab-4c03-a8ec-fc3d87f68721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Loading MIMIC-III data...\n",
      "üìÇ Loading CHARTEVENTS...\n",
      "‚úÖ Loaded CHARTEVENTS from parquet\n",
      "\n",
      "üìÇ Loading and filtering ICUSTAYS...\n",
      "üìã Creating ID lookup tables...\n",
      "üìÇ Loading PATIENTS table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:41:40 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/07 12:41:40 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/07 12:41:40 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading ADMISSIONS table...\n",
      "üìÇ Loading DIAGNOSES_ICD table...\n",
      "üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\n",
      "üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\n",
      "\n",
      "‚úÖ Data loading complete!\n",
      "üìä ICUSTAYS: 20 rows\n",
      "üìä PATIENTS: 20 rows\n",
      "üìä ADMISSIONS: 20 rows\n",
      "üìä DIAGNOSES_ICD: 212 rows\n",
      "üìä CHARTEVENTS (filtered): 57,973 rows\n",
      "üìä LABEVENTS (filtered): 5,895 rows\n",
      "\n",
      "‚è∞ Data loaded at: 2025-06-07 12:41:41\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#Configuration flags\n",
    "SAMPLE_ENABLE = False\n",
    "SAMPLE_SIZE = 20000\n",
    "MIMIC_PATH = \"mimic-db-short\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"üè• Loading MIMIC-III data...\")\n",
    "\n",
    "# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "\n",
    "\n",
    "chartevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "d_items_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/D_ITEMS.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Load ICUSTAYS \n",
    "print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Apply sampling if enabled\n",
    "if SAMPLE_ENABLE:\n",
    "    print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "    print(f\"‚úÖ Final sample: {actual_sample_size} ICU stays\")\n",
    "else:\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "\n",
    "  \n",
    "  \n",
    "# Step 4: Create efficient lookup tables\n",
    "print(\"üìã Creating ID lookup tables...\")\n",
    "icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "\n",
    "icu_lookup.count()  # Trigger caching\n",
    "hadm_lookup.count()\n",
    "subject_lookup.count()\n",
    "\n",
    "# Step 5: Load other tables with optimized joins\n",
    "print(\"üìÇ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Step 6: Load and filter CHARTEVENTS efficiently\n",
    "print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "chartevents_df = chartevents_df \\\n",
    "    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "\n",
    "# Step 7: Load LABEVENTS\n",
    "print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "\n",
    "labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "d_labitems_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/D_LABITEMS.csv\")\n",
    "\n",
    "\n",
    "labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n‚úÖ Data loading complete!\")\n",
    "print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e7157-1230-41d3-8f41-1117b62fdb55",
   "metadata": {},
   "source": [
    "# Features Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763e6d7-4ba5-439b-8e2b-ba16cf607a3e",
   "metadata": {},
   "source": [
    "## Extracting Data From ICUSTAYS\n",
    "\n",
    "**Purpose**: Create comprehensive ICU dataset by joining ICU stays with patient demographics and admission details.\n",
    "\n",
    "**Key Features**:\n",
    "- **Target Variable**: ICU_LOS_DAYS (length of stay)\n",
    "- **Demographics**: Age (18-80), gender, ethnicity\n",
    "- **Clinical**: Care units, admission type/location, insurance\n",
    "- **Outcomes**: Hospital/patient death flags\n",
    "- **Identifiers**: ICUSTAY_ID, SUBJECT_ID, HADM_ID\n",
    "\n",
    "**Age Filter**: Adults only (18-80 years) to exclude pediatric/very elderly edge cases.\n",
    "\n",
    "**Alive Filter**: Only include people who did survive the ICU stay.\n",
    "\n",
    "**LOS Filter**: Get only LOS values within a range that does'nt include outliers.\n",
    "\n",
    "**Result**: Clean base dataset ready for vital signs feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d7adb-eac0-4071-b252-a04268f0c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 1: Creating base ICU dataset with patient demographics...\n",
      "‚úÖ Created base ICU dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 1: Creating base ICU dataset with patient demographics...\")\n",
    "\n",
    "base_icu_df = icustays_df.alias(\"icu\") \\\n",
    "    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n",
    "    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        # ICU stay identifiers\n",
    "        col(\"icu.ICUSTAY_ID\"),\n",
    "        col(\"icu.SUBJECT_ID\"), \n",
    "        col(\"icu.HADM_ID\"),\n",
    "        \n",
    "        # Target variable - Length of Stay in ICU (days)\n",
    "        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n",
    "        \n",
    "        # ICU characteristics\n",
    "        col(\"icu.FIRST_CAREUNIT\"),\n",
    "        col(\"icu.LAST_CAREUNIT\"), \n",
    "        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n",
    "        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n",
    "        \n",
    "        # Patient demographics\n",
    "        col(\"pat.GENDER\"),\n",
    "        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n",
    "        col(\"pat.DOB\"),\n",
    "        \n",
    "        # Admission details\n",
    "        col(\"adm.ADMITTIME\"),\n",
    "        col(\"adm.DISCHTIME\"), \n",
    "        col(\"adm.ADMISSION_TYPE\"),\n",
    "        col(\"adm.ADMISSION_LOCATION\"),\n",
    "        col(\"adm.INSURANCE\"),\n",
    "        col(\"adm.ETHNICITY\"),\n",
    "        col(\"adm.MARITAL_STATUS\"),\n",
    "        col(\"adm.RELIGION\"),\n",
    "        col(\"adm.HOSPITAL_EXPIRE_FLAG\").alias(\"HOSPITAL_DEATH\"),\n",
    "        col(\"adm.DIAGNOSIS\").alias(\"ADMISSION_DIAGNOSIS\")\n",
    "    )\n",
    "\n",
    "# Calculate age at ICU admission\n",
    "base_icu_df = base_icu_df.withColumn(\"AGE_AT_ICU_ADMISSION\", \\\n",
    "                                     floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25)) \\\n",
    "                                     .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18,80)) \\\n",
    "                                    .filter(col(\"PATIENT_DIED\").isin(0))\n",
    "\n",
    "\n",
    "icustays_df.unpersist()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Created base ICU dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6ccac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà ICU Length of Stay Statistics (Days):\n",
      "+-------+-----------------+\n",
      "|summary|     ICU_LOS_DAYS|\n",
      "+-------+-----------------+\n",
      "|  count|               12|\n",
      "|   mean|2.360116666666667|\n",
      "| stddev|2.264151572918985|\n",
      "|    min|            0.848|\n",
      "|    max|           8.9163|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà ICU Length of Stay Statistics (Days):\")\n",
    "base_icu_df.select(\"ICU_LOS_DAYS\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b3ede",
   "metadata": {},
   "source": [
    "We kept every ICU STAY that had duration (LOS) between 0.0 and 9.1 days, considered normal legnths since:\n",
    "\n",
    "| Statistic                | Value (days)                                    |\n",
    "| ------------------------ | ----------------------------------------------- |\n",
    "| **Minimum**              | 0.0 (can be admission + discharge on same day)  |\n",
    "| **25th percentile (Q1)** | \\~1.1                                           |\n",
    "| **Median (Q2)**          | \\~2.                                            |\n",
    "| **75th percentile (Q3)** | \\~4.3                                           |\n",
    "| **Maximum**              | \\~101.739                                       |\n",
    "| **Mean**                 | \\~3.49                                          |\n",
    "\n",
    "Using interquartile range (IQR) method:\n",
    "\n",
    "* IQR = Q3 - Q1 = 4.3 - 1.1 = ~3.2\n",
    "\n",
    "* Upper Bound for outliers = Q3 + 1.5 √ó IQR ‚âà 4.3 + 4.8 = ~9.1 days\n",
    "\n",
    "* Lower Bound = Q1 - 1.5 √ó IQR ‚âà 1.1 - 4.8 = < 0, which is ignored since LOS can‚Äôt be negative\n",
    "\n",
    "So:\n",
    "\n",
    "* Typical ICU LOS: 1.1 to 4.3 days\n",
    "\n",
    "* Outliers: ICU stays longer than ~9.1 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc345d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing LOS outliers: 12\n",
      "üìä Cleaning target variable...\n",
      "‚úÖ Base ICU Dataset - Outliers Removed\n",
      "Number of rows after removing LOS outliers: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:41:42 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Print initial dataset size\n",
    "print(f\"Number of rows before removing LOS outliers: {base_icu_df.count()}\")\n",
    "\n",
    "print(\"üìä Cleaning target variable...\")\n",
    "\n",
    "# Filter to keep only records with ICU_LOS_DAYS between 0 and 9.1 days\n",
    "base_icu_df = base_icu_df.filter(\n",
    "    (col(\"ICU_LOS_DAYS\") >= 0.0) & \n",
    "    (col(\"ICU_LOS_DAYS\") <= 9.1)\n",
    ").cache()\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Outliers Removed\")\n",
    "\n",
    "# Print filtered dataset size\n",
    "print(f\"Number of rows after removing LOS outliers: {base_icu_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595638e6-74f7-4f2f-a7e5-1fc692089206",
   "metadata": {},
   "source": [
    "## Extracting Categorical Features\n",
    "\n",
    "**Features Created**:\n",
    "- **GENDER_BINARY**: Male = 1, Female = 0\n",
    "- **CAME_FROM_ER**: Emergency admission = 1\n",
    "- **HAS_INSURANCE**: Medicare = 1, other = 0\n",
    "- **ADMISSION_TYPE_ENCODED**: Emergency=1, Elective=2, Urgent=3, Other=0\n",
    "- **ETHNICITY_ENCODED**: White=1, Black=2, Hispanic=3, Asian=4, Other=5\n",
    "- **MARITAL_STATUS_ENCODED**: Married=1, Single=2, Divorced=3, Widowed=4, Separated=5, LifePartener=6, Other=0\n",
    "- **RELIGION_ENCODED**: Catholic=1, Protestant=2, Jewish=3, Other=0\n",
    "- **FIRST_UNIT_ENCODED**: Numerical encoding of ICU units MICU (Medical) = 1, SICU (Surgical) = 2,  CSRU (Cardiac Surgery) = 3, CCU (Coronary Care) = 4, TSICU (Trauma Surgical) = 5, Other = 0\n",
    "- **CHANGED_ICU_UNIT**: Binary flag (1 if patient transferred between units)\n",
    "\n",
    "**Clinical Significance**: Different ICU types have varying complexity and typical LOS patterns. Unit transfers often indicate complications.\n",
    "\n",
    "**Result**: Categorical variables converted to numerical format for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d34e6-83c3-40ae-95d0-71f9929397a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 2: Engineering categorical features...\n",
      "‚úÖ Base ICU Dataset - Categorical Features\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 2: Engineering categorical features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"ETHNICITY_ENCODED\",\n",
    "                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1)\n",
    "                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2)\n",
    "                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3)\n",
    "                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4)\n",
    "                .otherwise(5)) \\\n",
    "    .withColumn(\"MARITAL_STATUS_ENCODED\",\n",
    "                when(col(\"MARITAL_STATUS\") == \"MARRIED\", 1)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"SINGLE\", 2)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"DIVORCED\", 3)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"WIDOWED\", 4)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"SEPARATED\", 5)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"LIFE PARTNER\", 6)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"RELIGION_ENCODED\",\n",
    "                when(col(\"RELIGION\").contains(\"CATHOLIC\"), 1)\n",
    "                .when(col(\"RELIGION\").contains(\"PROTESTANT\"), 2)\n",
    "                .when(col(\"RELIGION\").contains(\"JEWISH\"), 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"FIRST_UNIT_ENCODED\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"CHANGED_ICU_UNIT\", \n",
    "                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Categorical Features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74662ab6-f263-402e-913f-dc04804eadff",
   "metadata": {},
   "source": [
    "## Extracting Time-based Features\n",
    "\n",
    "**Action**: Filter out invalid records where INTIME >= OUTTIME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef431386-85f1-4867-9aa1-f5bb84d7c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 4: Creating time-based features...\n",
      "‚úÖ Base ICU Dataset - Time Based Features\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 4: Creating time-based features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n",
    "print(\"‚úÖ Base ICU Dataset - Time Based Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79066063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 5: Dropping useless columns...\n",
      "‚úÖ Base ICU Dataset - Finalized\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "|ICUSTAY_ID|SUBJECT_ID|HADM_ID|ICU_LOS_DAYS|         ICU_INTIME|        ICU_OUTTIME|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "|    231977|      8470| 184688|      0.9792|2174-09-01 18:14:58|2174-09-02 17:45:00|                  30|            0|           0|            0|                     1|                4|                     2|               0|                 1|               0|\n",
      "|    264061|     22862| 108676|      1.0576|2178-08-07 20:44:01|2178-08-08 22:06:54|                  51|            1|           0|            0|                     1|                1|                     2|               1|                 3|               0|\n",
      "|    248205|     18322| 163177|        4.05|2103-06-30 15:27:26|2103-07-04 16:39:27|                  47|            0|           1|            1|                     1|                1|                     3|               1|                 1|               0|\n",
      "|    298190|     45871| 178380|      1.2597|2192-10-25 11:14:14|2192-10-26 17:28:12|                  73|            0|           0|            1|                     1|                1|                     3|               1|                 3|               0|\n",
      "|    271202|     41841| 125602|      1.7742|2175-03-02 21:22:54|2175-03-04 15:57:42|                  59|            0|           0|            0|                     2|                1|                     2|               2|                 1|               0|\n",
      "+----------+----------+-------+------------+-------------------+-------------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 5: Dropping useless columns...\")\n",
    "\n",
    "# List of columns to drop (fixed syntax)\n",
    "drop_cols = [\n",
    "    \"FIRST_CAREUNIT\",\n",
    "    \"LAST_CAREUNIT\",\n",
    "    \"GENDER\",\n",
    "    \"PATIENT_DIED\",\n",
    "    \"DOB\",\n",
    "    \"ADMITTIME\",\n",
    "    \"DISCHTIME\",\n",
    "    \"ADMISSION_TYPE\",\n",
    "    \"ADMISSION_LOCATION\",\n",
    "    \"INSURANCE\",\n",
    "    \"ETHNICITY\",\n",
    "    \"MARITAL_STATUS\",\n",
    "    \"RELIGION\",\n",
    "    \"HOSPITAL_DEATH\",\n",
    "    \"ADMISSION_DIAGNOSIS\"\n",
    "]\n",
    "\n",
    "# Keep all columns except those in drop_cols\n",
    "base_icu_df = base_icu_df.drop(*drop_cols)\n",
    "\n",
    "print(\"‚úÖ Base ICU Dataset - Finalized\")\n",
    "base_icu_df.show(5)  # Showing first 5 rows for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268129f5-72e2-40a5-b676-cbd825ae84c8",
   "metadata": {},
   "source": [
    "## Extracting Clinical Events\n",
    "\n",
    "**Purpose**: Extract top 20 most common CHARTEVENTS as features for ML models.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Find 20 most frequent CHARTEVENTS (typically vital signs)\n",
    "2. **Calculate**: Average value of each test in first 24 hours of ICU stay\n",
    "3. **Handle Missing**: Set missing values to **-1** (not null) for ML compatibility\n",
    "\n",
    "**Time Window**: First 24 hours after ICU admission (INTIME + 24h)\n",
    "\n",
    "**Result**: 20 vital signs features with consistent **-1** encoding for missing data, ensuring ML algorithm compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e7380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:41:42 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Filtering to 12 ICU stays\n"
     ]
    }
   ],
   "source": [
    "icu_stay_ids = base_icu_df.select(\"ICUSTAY_ID\").distinct()\n",
    "\n",
    "icu_stay_ids.cache()\n",
    "print(f\"üìå Filtering to {icu_stay_ids.count()} ICU stays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d9e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+--------------------------+-----------------+------------------+------------------+-----------+-----------+-------------+-----------------+-------------------+-----------------+-------------------+\n",
      "|ICUSTAY_ID|Routine Vital Signs__sum|Routine Vital Signs__count| Respiratory__sum|Respiratory__count|         Labs__sum|Labs__count|Alarms__sum|Alarms__count|Neurological__sum|Neurological__count|Hemodynamics__sum|Hemodynamics__count|\n",
      "+----------+------------------------+--------------------------+-----------------+------------------+------------------+-----------+-----------+-------------+-----------------+-------------------+-----------------+-------------------+\n",
      "|    234929|                 21690.4|                       285|         20308.37|               201| 8817.480000000001|        146|     2743.0|           61|             75.0|                 21|7460.600000000001|                292|\n",
      "|    207525|                 12608.6|                       169|           9279.8|               103| 7816.879999999999|        132|     3213.5|           66|            157.0|                 43|           2947.0|                123|\n",
      "|    298190|                 12080.9|                       162|         19498.05|               178|10046.969999999996|        151|     3068.0|           79|             79.0|                 21|           3450.4|                151|\n",
      "|    253828|       96096.20000000001|                      1101|68804.70000000003|               709|31333.869999999974|        357|    17792.0|          361|           2025.0|                575|           8713.0|                282|\n",
      "|    252713|                  7552.9|                        95|           3995.5|                67|1671.3999999999999|         35|     1499.0|           30|             75.0|                 15|              0.0|                  0|\n",
      "|    290009|                 22232.6|                       259|67015.79999999999|               582| 5859.960000000001|        104|     4879.0|          128|            185.0|                 48|              0.0|                  0|\n",
      "|    235298|                 22361.1|                       280|           7319.0|               135|3630.7299999999996|         63|     3088.0|           66|            119.0|                 24|              0.0|                  0|\n",
      "|    271202|                 23436.7|                       246|           4813.0|                95|            4515.8|         99|     5394.0|          100|            285.0|                 57|             54.0|                 11|\n",
      "|    264061|       7644.599999999999|                        98|           2782.0|                52|          10229.11|         36|     1513.0|           40|            246.0|                 60|              0.0|                  0|\n",
      "|    259725|      15873.699999999999|                       205|           5767.0|               101| 2911.620000000001|         54|     3026.0|           68|            462.0|                110|              0.0|                  0|\n",
      "+----------+------------------------+--------------------------+-----------------+------------------+------------------+-----------+-----------+-------------+-----------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache the filtered data only once\n",
    "chartevents_filtered = chartevents_df.select(\n",
    "    \"ICUSTAY_ID\", \"ITEMID\", \"VALUENUM\", \"CHARTTIME\"\n",
    ").join(\n",
    "    icu_stay_ids, \"ICUSTAY_ID\", \"inner\"\n",
    ").filter(\n",
    "    col(\"VALUENUM\").isNotNull() & col(\"CHARTTIME\").isNotNull()\n",
    ").cache()\n",
    "\n",
    "# Join with d_items and cache since we'll use it multiple times\n",
    "chartevents_with_categories = chartevents_filtered.join(\n",
    "    d_items_df.select(\"ITEMID\", \"CATEGORY\"), \"ITEMID\", \"left\"\n",
    ").cache()\n",
    "\n",
    "# Get top categories with handling for \"null\"\n",
    "top_categories = chartevents_with_categories.groupBy(\"CATEGORY\").agg(\n",
    "    count(\"*\").alias(\"count\")\n",
    ").orderBy(\n",
    "    col(\"count\").desc()\n",
    ").limit(7).select(\"CATEGORY\").collect()\n",
    "\n",
    "top_categories = [cat for cat in [row[\"CATEGORY\"] for row in top_categories] if cat != None  ]\n",
    "\n",
    "chartevents_top_categories = chartevents_with_categories.filter(\n",
    "    col(\"CATEGORY\").isin(top_categories)\n",
    ").cache()\n",
    "\n",
    "patient_category_stats = chartevents_top_categories.groupBy(\"ICUSTAY_ID\").pivot(\n",
    "    \"CATEGORY\", top_categories\n",
    ").agg(\n",
    "    sql_sum(\"VALUENUM\").alias(\"_sum\"), \n",
    "    count(\"VALUENUM\").alias(\"_count\")\n",
    ").fillna(0)  \n",
    "\n",
    "# Clean up cached DataFrames\n",
    "chartevents_filtered.unpersist()\n",
    "chartevents_with_categories.unpersist()\n",
    "chartevents_top_categories.unpersist()\n",
    "\n",
    "patient_category_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08c810-405c-48ca-a141-1a48e125635c",
   "metadata": {},
   "source": [
    "## Extracting Laboratory Events\n",
    "\n",
    "**Purpose**: Extract top 20 most common lab tests as features for ML models.\n",
    "\n",
    "**Process**:\n",
    "1. **Identify**: Find 20 most frequent LABEVENTS (blood tests, chemistry panels)\n",
    "2. **Time Window**: 6 hours before ICU admission + first 24 hours in ICU (30h total)\n",
    "3. **Calculate**: Average value of each lab test within the 30-hour window\n",
    "\n",
    "**Time Range**: ICU_INTIME - 6h to ICU_INTIME + 24h\n",
    "\n",
    "**Result**: 20 lab test features with consistent -1 encoding for missing data, capturing pre-ICU and early ICU clinical status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Filtering to 12 HADM ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:41:43 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "hadm_ids = base_icu_df.select(\"HADM_ID\").distinct()\n",
    "\n",
    "hadm_ids.cache()\n",
    "print(f\"üìå Filtering to {hadm_ids.count()} HADM ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d89e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:41:43 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+----------------+---------------+---------------+\n",
      "|HADM_ID|    Hematology_sum|     Chemistry_sum|     Blood Gas_sum|Hematology_count|Chemistry_count|Blood Gas_count|\n",
      "+-------+------------------+------------------+------------------+----------------+---------------+---------------+\n",
      "| 152943|12508.405999999997|21614.250000000007| 6574.119999999999|             224|            281|             80|\n",
      "| 163177|2428.5839999999994|1625.6999999999998|              40.0|              68|             42|              2|\n",
      "| 109820|           3414.12|            4182.5|            585.46|              63|             52|             13|\n",
      "| 181763| 3908.620000000001|1009.1999999999999| 7738.740000000001|              86|             30|            121|\n",
      "| 110972| 8419.420999999998| 6029.299999999999|1772.8200000000002|             188|            141|             31|\n",
      "| 109131|2850.6750000000006| 3791.999999999999|           5323.58|              92|             72|             89|\n",
      "| 135117|            1098.5|             906.4|               0.0|              26|             19|              0|\n",
      "| 184688| 5934.809999999998|2371.9000000000005| 5814.920000000001|             161|             71|             59|\n",
      "| 108676|          2561.709|36463.219999999994|            594.69|              58|             89|             12|\n",
      "| 178380| 5684.192999999999|3274.5999999999995|6955.1500000000015|             131|             80|            114|\n",
      "| 117313|1709.6379999999997|            4536.5|               2.9|              63|             66|              2|\n",
      "| 125602|2485.6699999999996|            6695.7|            745.61|              60|            139|             13|\n",
      "+-------+------------------+------------------+------------------+----------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, filter the labevents data similarly to how you filtered chartevents\n",
    "labevents_filtered = labevents_df \\\n",
    "    .select(\"HADM_ID\", \"ITEMID\", \"VALUENUM\", \"CHARTTIME\") \\\n",
    "    .join(hadm_ids, \"HADM_ID\", \"inner\") \\\n",
    "    .filter(col(\"VALUENUM\").isNotNull()) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull())\n",
    "labevents_filtered.cache()\n",
    "\n",
    "# Join with d_labitems to get the categories\n",
    "labevents_with_categories = labevents_filtered \\\n",
    "    .join(d_labitems_df.select(\"ITEMID\", \"CATEGORY\"), \"ITEMID\", \"left\")\n",
    "\n",
    "# Get the top 20 categories\n",
    "top_lab_categories = labevents_with_categories \\\n",
    "    .groupBy(\"CATEGORY\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(7) \\\n",
    "    .select(\"CATEGORY\") \\\n",
    "    .collect()\n",
    "top_lab_categories = [row[\"CATEGORY\"] for row in top_lab_categories]\n",
    "\n",
    "# Filter to only include top categories\n",
    "labevents_top_categories = labevents_with_categories.filter(\n",
    "    col(\"CATEGORY\").isin(top_lab_categories)\n",
    ")\n",
    "\n",
    "patient_lab_category_stats = labevents_top_categories.groupBy(\"HADM_ID\", \"CATEGORY\") \\\n",
    "    .agg(\n",
    "        sum(\"VALUENUM\").alias(\"sum_val\"),\n",
    "        count(\"*\").alias(\"count_val\")\n",
    "    )\n",
    "\n",
    "# Pivot both metrics\n",
    "sum_pivot = patient_lab_category_stats.groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"CATEGORY\", top_lab_categories) \\\n",
    "    .sum(\"sum_val\")\n",
    "\n",
    "count_pivot = patient_lab_category_stats.groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"CATEGORY\", top_lab_categories) \\\n",
    "    .sum(\"count_val\")\n",
    "\n",
    "# Rename count columns before joining\n",
    "for category in top_lab_categories:\n",
    "    count_pivot = count_pivot.withColumnRenamed(\n",
    "        category, f\"{category}_count\"\n",
    "    )\n",
    "    sum_pivot = sum_pivot.withColumnRenamed(\n",
    "        category, f\"{category}_sum\"\n",
    "    )\n",
    "\n",
    "# Join the results\n",
    "final_lab_stats = sum_pivot.join(count_pivot, \"HADM_ID\", \"inner\")\n",
    "\n",
    "final_lab_stats = final_lab_stats.fillna(0)\n",
    "# Show the final result\n",
    "final_lab_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cd034-b129-46b0-b33f-cf903ee438ae",
   "metadata": {},
   "source": [
    "## Diagnosis ICD\n",
    "\n",
    "**Purpose**: Extract diagnosis patterns as ML features from ICD-9 codes.\n",
    "\n",
    "**Process**:\n",
    "1. **Top 3**: Get top 3 diagnoses by person, using HADM_ID, to future join with other tables. \n",
    "2. **Encode**: Encode the ICD9 diagnoses into a wide range of diagnoses.\n",
    "3. **Pivot**: Pivot to create the 3 columns with the encoded diagnose type.\n",
    "4. **Handle Missing Values**: Input -1 in the NULL entries of the table.\n",
    "\n",
    "\n",
    "**Features Created**:\n",
    "- **TOTAL_DIAGNOSES**: Count of all diagnoses (comorbidity indicator)\n",
    "- **PRIMARY_DIAGNOSIS**: Most significant diagnose, encoded.\n",
    "- **SECONDARY_DIAGNOSIS**: Second most significant diagnose, encoded.\n",
    "- **TERCIARY_DIAGNOSIS**: Third most significant diagnose, encoded.\n",
    "\n",
    "**Result**: ??????????????????????????????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3682a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icd9_to_chapter(code):\n",
    "    # Convert to string and clean\n",
    "    code_str = str(code).strip()\n",
    "    \n",
    "    # Handle V codes (supplementary classification)\n",
    "    if code_str.startswith('V'):\n",
    "        return 18 #'Supplemental'\n",
    "    \n",
    "    # Handle E codes (external causes of injury)\n",
    "    if code_str.startswith('E'):\n",
    "        return 19 #'External_Injury'\n",
    "    \n",
    "    # Extract first 3 digits for numeric codes\n",
    "    try:\n",
    "        # Handle codes like '4280' (convert to 428) or '486' (stays 486)\n",
    "        numeric_part = code_str.split('.')[0] if '.' in code_str else code_str\n",
    "        code_num = float(numeric_part[:3])\n",
    "    except:\n",
    "        return 0 #'Unknown'\n",
    "    \n",
    "    # Map to chapters\n",
    "    if 1 <= code_num <= 139: return 1 #'Infectious'\n",
    "    elif 140 <= code_num <= 239: return 2 # 'Neoplasms'\n",
    "    elif 240 <= code_num <= 279: return 3 #'Endocrine'\n",
    "    elif 280 <= code_num <= 289: return 4 #'Blood'\n",
    "    elif 290 <= code_num <= 319: return 5 #'Mental'\n",
    "    elif 320 <= code_num <= 389: return 6 #'Nervous'\n",
    "    elif 390 <= code_num <= 459: return 7 #'Circulatory'\n",
    "    elif 460 <= code_num <= 519: return 8 #'Respiratory'\n",
    "    elif 520 <= code_num <= 579: return 9 #'Digestive'\n",
    "    elif 580 <= code_num <= 629: return 10 #'Genitourinary'\n",
    "    elif 630 <= code_num <= 679: return 11 #'Pregnancy'\n",
    "    elif 680 <= code_num <= 709: return 12 #'Skin'\n",
    "    elif 710 <= code_num <= 739: return 13 #'Musculoskeletal'\n",
    "    elif 740 <= code_num <= 759: return 14 #'Congenital'\n",
    "    elif 760 <= code_num <= 779: return 15 #'Perinatal'\n",
    "    elif 780 <= code_num <= 799: return 16 #'Ill-defined'\n",
    "    elif 800 <= code_num <= 999: return 17 #'Injury'\n",
    "    else: return 20 #'Other' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a414c1-7139-4e27-a5e4-767919a3eead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè• Creating diagnosis features (optimized pipeline)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:41:44 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Optimized diagnosis features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6301:=========>    (22 + 8) / 32][Stage 6302:>               (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-----------------+-------------------+------------------+\n",
      "|HADM_ID|TOTAL_DIAGNOSES|PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|\n",
      "+-------+---------------+-----------------+-------------------+------------------+\n",
      "|152943 |7              |7                |6                  |6                 |\n",
      "|163177 |7              |9                |8                  |5                 |\n",
      "|110159 |12             |17               |9                  |8                 |\n",
      "|109820 |11             |1                |8                  |8                 |\n",
      "|181763 |12             |7                |17                 |4                 |\n",
      "|150954 |6              |7                |8                  |18                |\n",
      "|177309 |16             |1                |12                 |10                |\n",
      "|110972 |13             |17               |7                  |17                |\n",
      "|197549 |15             |17               |7                  |17                |\n",
      "|109131 |12             |7                |2                  |7                 |\n",
      "|135117 |5              |17               |14                 |19                |\n",
      "|178506 |6              |17               |8                  |16                |\n",
      "|184688 |2              |2                |4                  |-1                |\n",
      "|108676 |14             |10               |8                  |13                |\n",
      "|178380 |18             |7                |7                  |17                |\n",
      "|117313 |12             |1                |8                  |17                |\n",
      "|123482 |16             |1                |16                 |10                |\n",
      "|187714 |6              |18               |15                 |15                |\n",
      "|156406 |9              |1                |2                  |3                 |\n",
      "|125602 |13             |2                |8                  |3                 |\n",
      "+-------+---------------+-----------------+-------------------+------------------+\n",
      "\n",
      "‚è∞ Completed in: 1.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nüè• Creating diagnosis features (optimized pipeline)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. First filter to only top 3 diagnoses per admission\n",
    "window_spec = Window.partitionBy(\"HADM_ID\").orderBy(\"SEQ_NUM\")\n",
    "\n",
    "top_3_filtered = diagnoses_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") <= 3) \\\n",
    "    .cache()\n",
    "\n",
    "# 2. Register UDF with Integer return type\n",
    "icd9_chapter_udf = udf(icd9_to_chapter, IntegerType())  # Changed to IntegerType\n",
    "\n",
    "# 3. Encode ONLY the top 3 diagnoses\n",
    "top_3_encoded = top_3_filtered.withColumn(\n",
    "    \"DISEASE_CHAPTER\", \n",
    "    icd9_chapter_udf(col(\"ICD9_CODE\"))\n",
    ")\n",
    "\n",
    "\n",
    "diagnosis_count = diagnoses_df.groupBy(\"HADM_ID\").count().withColumnRenamed(\"count\", \"TOTAL_DIAGNOSES\")\n",
    "\n",
    "\n",
    "diagnoses_df.unpersist()\n",
    "\n",
    "\n",
    "# 4. Pivot to create columns\n",
    "diagnosis_features = top_3_encoded \\\n",
    "    .groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"row_num\", [1, 2, 3]) \\\n",
    "    .agg(first(\"DISEASE_CHAPTER\")) \\\n",
    "    .select(\n",
    "        \"HADM_ID\",\n",
    "        col(\"1\").alias(\"PRIMARY_DIAGNOSIS\").cast(IntegerType()),\n",
    "        col(\"2\").alias(\"SECONDARY_DIAGNOSIS\").cast(IntegerType()),\n",
    "        col(\"3\").alias(\"TERTIARY_DIAGNOSIS\").cast(IntegerType())\n",
    "    ) \\\n",
    "    .join(diagnosis_count, \"HADM_ID\", \"left\")\n",
    "\n",
    "# 5. Fill NULLs and ensure consistent types\n",
    "diagnosis_features = diagnosis_features.fillna(-1, subset=[\n",
    "    \"PRIMARY_DIAGNOSIS\",\n",
    "    \"SECONDARY_DIAGNOSIS\",\n",
    "    \"TERTIARY_DIAGNOSIS\"\n",
    "])\n",
    "\n",
    "\n",
    "print(\"üìä Optimized diagnosis features:\")\n",
    "diagnosis_features.select(\n",
    "    \"HADM_ID\",\n",
    "    \"TOTAL_DIAGNOSES\",\n",
    "    \"PRIMARY_DIAGNOSIS\",\n",
    "    \"SECONDARY_DIAGNOSIS\",\n",
    "    \"TERTIARY_DIAGNOSIS\"\n",
    ").show(20, truncate=False)\n",
    "\n",
    "print(f\"‚è∞ Completed in: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d9140-230d-4bc0-91ae-fd9db043e5a5",
   "metadata": {},
   "source": [
    "## Joining All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe76c11-fb64-4b12-9a37-8efccf76d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Joining all features and selecting final features for regression modeling...\n",
      "‚úÖ Final modeling dataset created with 12 records\n",
      "üìã Sample of final modeling dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+----------------+------------------+------------------+-----------+-----------+-------------+-----------------+-------------------+-----------------+-------------------+------------------+------------------+------------------+----------------+---------------+---------------+-----------------+-------------------+------------------+---------------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|Routine Vital Signs__sum|Routine Vital Signs__count|Respiratory__sum|Respiratory__count|Labs__sum         |Labs__count|Alarms__sum|Alarms__count|Neurological__sum|Neurological__count|Hemodynamics__sum|Hemodynamics__count|Hematology_sum    |Chemistry_sum     |Blood Gas_sum     |Hematology_count|Chemistry_count|Blood Gas_count|PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+----------------+------------------+------------------+-----------+-----------+-------------+-----------------+-------------------+-----------------+-------------------+------------------+------------------+------------------+----------------+---------------+---------------+-----------------+-------------------+------------------+---------------+\n",
      "|1.2198      |52                  |1            |0           |0            |2                     |1                |1                     |1               |3                 |0               |21690.4                 |285                       |20308.37        |201               |8817.480000000001 |146        |2743.0     |61           |75.0             |21                 |7460.600000000001|292                |3908.620000000001 |1009.1999999999999|7738.740000000001 |86              |30             |121            |7                |17                 |4                 |12             |\n",
      "|1.2597      |73                  |0            |0           |1            |1                     |1                |3                     |1               |3                 |0               |12080.9                 |162                       |19498.05        |178               |10046.969999999996|151        |3068.0     |79           |79.0             |21                 |3450.4           |151                |5684.192999999999 |3274.5999999999995|6955.1500000000015|131             |80             |114            |7                |7                  |17                |18             |\n",
      "|1.7742      |59                  |0            |0           |0            |2                     |1                |2                     |2               |1                 |0               |23436.7                 |246                       |4813.0          |95                |4515.8            |99         |5394.0     |100          |285.0            |57                 |54.0             |11                 |2485.6699999999996|6695.7            |745.61            |60              |139            |13             |2                |8                  |3                 |13             |\n",
      "|1.0576      |51                  |1            |0           |0            |1                     |1                |2                     |1               |3                 |0               |7644.599999999999       |98                        |2782.0          |52                |10229.11          |36         |1513.0     |40           |246.0            |60                 |0.0              |0                  |2561.709          |36463.219999999994|594.69            |58              |89             |12             |10               |8                  |13                |14             |\n",
      "|0.9792      |30                  |0            |0           |0            |1                     |4                |2                     |0               |1                 |0               |NULL                    |NULL                      |NULL            |NULL              |NULL              |NULL       |NULL       |NULL         |NULL             |NULL               |NULL             |NULL               |5934.809999999998 |2371.9000000000005|5814.920000000001 |161             |71             |59             |2                |4                  |-1                |2              |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+----------------+------------------+------------------+-----------+-----------+-------------+-----------------+-------------------+-----------------+-------------------+------------------+------------------+------------------+----------------+---------------+---------------+-----------------+-------------------+------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Joining all features and selecting final features for regression modeling...\")\n",
    "\n",
    "# Define feature columns to exclude\n",
    "exclude_columns = {\"ICUSTAY_ID\", \"HADM_ID\", \"SUBJECT_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"}\n",
    "\n",
    "# Join all features and immediately select desired columns\n",
    "modeling_dataset = base_icu_df \\\n",
    "    .join(patient_category_stats, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(final_lab_stats, \"HADM_ID\", \"left\") \\\n",
    "    .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "    .select(*[name for name in base_icu_df \\\n",
    "        .join(patient_category_stats, \"ICUSTAY_ID\", \"left\") \\\n",
    "        .join(final_lab_stats, \"HADM_ID\", \"left\") \\\n",
    "        .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "        .columns if name not in exclude_columns])\n",
    "\n",
    "# Cleanup\n",
    "base_icu_df.unpersist()\n",
    "patient_category_stats.unpersist()\n",
    "final_lab_stats.unpersist()\n",
    "diagnosis_features.unpersist()\n",
    "\n",
    "# Display final info\n",
    "print(f\"‚úÖ Final modeling dataset created with {modeling_dataset.count()} records\")\n",
    "print(\"üìã Sample of final modeling dataset:\")\n",
    "modeling_dataset.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8fbbc",
   "metadata": {},
   "source": [
    "## Normalization & Handeling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87356b3",
   "metadata": {},
   "source": [
    "Display Missing Values by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a7674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ICU_LOS_DAYS': 0, 'AGE_AT_ICU_ADMISSION': 0, 'GENDER_BINARY': 0, 'CAME_FROM_ER': 0, 'HAS_INSURANCE': 0, 'ADMISSION_TYPE_ENCODED': 0, 'ETHNICITY_ENCODED': 0, 'MARITAL_STATUS_ENCODED': 0, 'RELIGION_ENCODED': 0, 'FIRST_UNIT_ENCODED': 0, 'CHANGED_ICU_UNIT': 0, 'Routine Vital Signs__sum': 2, 'Routine Vital Signs__count': 2, 'Respiratory__sum': 2, 'Respiratory__count': 2, 'Labs__sum': 2, 'Labs__count': 2, 'Alarms__sum': 2, 'Alarms__count': 2, 'Neurological__sum': 2, 'Neurological__count': 2, 'Hemodynamics__sum': 2, 'Hemodynamics__count': 2, 'Hematology_sum': 0, 'Chemistry_sum': 0, 'Blood Gas_sum': 0, 'Hematology_count': 0, 'Chemistry_count': 0, 'Blood Gas_count': 0, 'PRIMARY_DIAGNOSIS': 0, 'SECONDARY_DIAGNOSIS': 0, 'TERTIARY_DIAGNOSIS': 0, 'TOTAL_DIAGNOSES': 0}\n"
     ]
    }
   ],
   "source": [
    "null_counts = modeling_dataset.select(\n",
    "    [sum(col(c).isNull().cast(\"int\")).alias(c) for c in modeling_dataset.columns]\n",
    ").collect()[0]\n",
    "\n",
    "null_counts_dict = {col: null_counts[col] for col in modeling_dataset.columns}\n",
    "print(null_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a097880",
   "metadata": {},
   "source": [
    "## Normalization & Missing Values Strategy\n",
    "\n",
    "**Approach**: Min-Max scaling chosen over standardization because -1 represents missing values. With standardization (Gaussian approximation), -1 could correspond to an actual test result rather than indicating a missing/non-existent test result.\n",
    "\n",
    "**Implementation**: Applied only to float columns since others are binary or integer (like age). Final results limited to 3 decimal places maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215404ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Filling NULL entries with 0...\n",
      "üìä Applying StandardScaling to _sum columns...\n",
      "‚úÖ Scaled 9 _sum columns\n",
      "‚úÖ Data set ready for Machine Learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+------------------+------------------+------------------+-----------+------------------+-------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+-------------------+----------------+---------------+---------------+-----------------+-------------------+------------------+---------------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|Routine Vital Signs__sum|Routine Vital Signs__count|Respiratory__sum  |Respiratory__count|Labs__sum         |Labs__count|Alarms__sum       |Alarms__count|Neurological__sum  |Neurological__count|Hemodynamics__sum |Hemodynamics__count|Hematology_sum    |Chemistry_sum      |Blood Gas_sum      |Hematology_count|Chemistry_count|Blood Gas_count|PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+------------------+------------------+------------------+-----------+------------------+-------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+-------------------+----------------+---------------+---------------+-----------------+-------------------+------------------+---------------+\n",
      "|1.2198      |52                  |1            |0           |0            |2                     |1                |1                     |1               |3                 |0               |0.8563180428875837      |285                       |0.8305221358844384|201               |1.0490774422667029|146        |0.5848607833330087|61           |0.13489073762705459|21                 |2.3644957647925864|292                |1.1904302072517885|0.095122506140945  |2.4568030845545183 |86              |30             |121            |7                |17                 |4                 |12             |\n",
      "|1.2488      |69                  |1            |0           |1            |2                     |1                |1                     |3               |3                 |0               |0.4977765129067416      |169                       |0.3795026049151365|103               |0.9300290419604855|132        |0.6851805057384701|66           |0.28237127743263424|43                 |0.9339957937489948|123                |0.868216821041056 |0.35741631320497763|1.690066828046005  |92              |72             |89             |7                |2                  |7                 |12             |\n",
      "|1.2597      |73                  |0            |0           |1            |1                     |1                |3                     |1               |3                 |0               |0.47694337791468155     |162                       |0.7973836468205756|178               |1.1953584913297552|151        |0.6541570846757823|79           |0.1420849103004975 |21                 |1.0935388825081547|151                |1.7312082144207326|0.30864859156672464|2.208038256039014  |131             |80             |114            |7                |7                  |17                |18             |\n",
      "|8.9163      |69                  |0            |0           |0            |1                     |4                |1                     |0               |2                 |0               |3.7937940246806803      |1101                      |2.813806642428124 |709               |3.7280102927273258|357        |3.7935993645865445|361          |3.642049915930474  |575                |2.7614202073074283|282                |3.8096270159210945|2.037258846964845  |2.0870733858782624 |224             |281            |80             |7                |6                  |6                 |7              |\n",
      "|2.8701      |67                  |1            |0           |1            |1                     |2                |3                     |2               |1                 |0               |0.8777236252121903      |259                       |2.7406485776063927|582               |0.6972005435323004|104        |1.0402973976965912|128          |0.3327304861467346 |48                 |0.0               |0                  |1.0398226430767061|0.3942230300579692 |0.18586487385327433|63              |52             |13             |1                |8                  |8                 |11             |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+------------------+------------------+------------------+-----------+------------------+-------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+-------------------+----------------+---------------+---------------+-----------------+-------------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "Final DataSet shape: (12, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Filling NULL entries with 0...\")\n",
    "modeling_dataset = modeling_dataset.na.fill(0)\n",
    "\n",
    "print(\"üìä Applying StandardScaling to _sum columns...\")\n",
    "std_columns = [c for c in modeling_dataset.columns if c.endswith('_sum')]\n",
    "\n",
    "if std_columns:\n",
    "    # Create vector assembler for the columns to scale\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=std_columns,\n",
    "        outputCol=\"features_to_scale\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    # Create MinMaxScaler\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_to_scale\",\n",
    "        outputCol=\"scaled_features\"\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    \n",
    "    # Fit and transform\n",
    "    scaler_model = pipeline.fit(modeling_dataset)\n",
    "    scaled_data = scaler_model.transform(modeling_dataset)\n",
    "    \n",
    "    # Extract scaled values back to individual columns\n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "    \n",
    "    # Replace original columns with scaled values\n",
    "    for i, col_name in enumerate(std_columns):\n",
    "        scaled_data = scaled_data.withColumn(\n",
    "            col_name,\n",
    "            scaled_data[\"scaled_array\"][i]\n",
    "        )\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    modeling_dataset = scaled_data.drop(\"features_to_scale\", \"scaled_features\", \"scaled_array\")\n",
    "    \n",
    "    print(f\"‚úÖ Scaled {len(std_columns)} _sum columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No _sum columns found to scale\")\n",
    "    \n",
    "\n",
    "print(\"‚úÖ Data set ready for Machine Learning!\")\n",
    "modeling_dataset.show(5, truncate=False)\n",
    "\n",
    "num_rows = modeling_dataset.count()\n",
    "num_cols = len(modeling_dataset.columns)\n",
    "print(f\"Final DataSet shape: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f8928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Applying MinMaxScaling to _count columns...\n",
      "‚úÖ Scaled 9 _count columns using MinMax scaling\n",
      "‚úÖ Data set ready for Machine Learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n",
      "|ICU_LOS_DAYS|AGE_AT_ICU_ADMISSION|GENDER_BINARY|CAME_FROM_ER|HAS_INSURANCE|ADMISSION_TYPE_ENCODED|ETHNICITY_ENCODED|MARITAL_STATUS_ENCODED|RELIGION_ENCODED|FIRST_UNIT_ENCODED|CHANGED_ICU_UNIT|Routine Vital Signs__sum|Routine Vital Signs__count|Respiratory__sum   |Respiratory__count|Labs__sum         |Labs__count       |Alarms__sum        |Alarms__count     |Neurological__sum  |Neurological__count|Hemodynamics__sum |Hemodynamics__count|Hematology_sum    |Chemistry_sum      |Blood Gas_sum       |Hematology_count  |Chemistry_count   |Blood Gas_count   |PRIMARY_DIAGNOSIS|SECONDARY_DIAGNOSIS|TERTIARY_DIAGNOSIS|TOTAL_DIAGNOSES|\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n",
      "|1.0576      |51                  |1            |0           |0            |1                     |1                |2                     |1               |3                 |0               |0.3018021295438729      |0.8900999091734787        |0.1137714440908112 |0.7334273624823696|1.2170289646775214|1.0084033613445378|0.32260093517420424|1.10803324099723  |0.44244161941673904|1.0434782608695652 |0.0               |0.0                |0.7802077909310118|3.4368538132863935 |0.18879510441670433 |1.6161616161616161|2.67175572519084  |0.9917355371900827|10               |8                  |13                |14             |\n",
      "|1.2488      |69                  |1            |0           |1            |2                     |1                |1                     |3               |3                 |0               |0.4977765129067416      |1.5349682107175295        |0.3795026049151365 |1.4527503526093088|0.9300290419604855|3.697478991596639 |0.6851805057384701 |1.8282548476454292|0.28237127743263424|0.7478260869565218 |0.9339957937489948|4.212328767123288  |0.868216821041056 |0.35741631320497763|1.690066828046005   |3.3333333333333335|2.0229007633587788|7.355371900826446 |7                |2                  |7                 |12             |\n",
      "|2.8701      |67                  |1            |0           |1            |1                     |2                |3                     |2               |1                 |0               |0.8777236252121903      |2.3524069028156225        |2.7406485776063927 |8.208744710860367 |0.6972005435323004|2.913165266106443 |1.0402973976965912 |3.5457063711911356|0.3327304861467346 |0.8347826086956522 |0.0               |0.0                |1.0398226430767061|0.3942230300579692 |0.18586487385327433 |1.8686868686868687|1.2595419847328244|1.0743801652892562|1                |8                  |8                 |11             |\n",
      "|2.2913      |78                  |1            |0           |1            |1                     |1                |4                     |2               |3                 |0               |0.8827966929523451      |2.5431425976385107        |0.29931459356601264|1.904090267983075 |0.4319734143951543|1.7647058823529413|0.6584214724507222 |1.8282548476454292|0.2140266370349266 |0.4173913043478261 |0.0               |0.0                |2.5642638798271657|0.5682938231030517 |0.5628137971245889  |8.181818181818182 |4.656488549618321 |2.56198347107438  |17               |7                  |17                |13             |\n",
      "|1.8064      |80                  |1            |0           |1            |1                     |1                |1                     |0               |1                 |0               |0.6266798084583335      |1.8619436875567668        |0.23584468658221   |1.4245416078984485|0.3464158537873154|1.5126050420168067|0.6452018703484085 |1.8836565096952909|0.8309269437826563 |1.9130434782608696 |0.0               |0.0                |0.5206964910033548|0.42758942638564906|9.206574901350998E-4|1.8686868686868687|1.7938931297709924|0.1652892561983471|1                |8                  |17                |12             |\n",
      "+------------+--------------------+-------------+------------+-------------+----------------------+-----------------+----------------------+----------------+------------------+----------------+------------------------+--------------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "Final DataSet shape: (12, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Applying MinMaxScaling to _count columns...\")\n",
    "minmax_columns = [c for c in modeling_dataset.columns if c.endswith('_count')]\n",
    "\n",
    "if minmax_columns:\n",
    "    # Create vector assembler for the columns to scale\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=minmax_columns,\n",
    "        outputCol=\"features_to_scale\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    # Create MinMaxScaler\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features_to_scale\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        min=0,  # Default minimum value after scaling\n",
    "        max=10   # Default maximum value after scaling\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    \n",
    "    # Fit and transform\n",
    "    scaler_model = pipeline.fit(modeling_dataset)\n",
    "    scaled_data = scaler_model.transform(modeling_dataset)\n",
    "    \n",
    "    # Extract scaled values back to individual columns\n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "    \n",
    "    # Replace original columns with scaled values\n",
    "    for i, col_name in enumerate(minmax_columns):\n",
    "        scaled_data = scaled_data.withColumn(\n",
    "            col_name,\n",
    "            scaled_data[\"scaled_array\"][i]\n",
    "        )\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    modeling_dataset = scaled_data.drop(\"features_to_scale\", \"scaled_features\", \"scaled_array\")\n",
    "    \n",
    "    print(f\"‚úÖ Scaled {len(minmax_columns)} _count columns using MinMax scaling\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No _count columns found to scale\")\n",
    "\n",
    "print(\"‚úÖ Data set ready for Machine Learning!\")\n",
    "modeling_dataset.show(5, truncate=False)\n",
    "\n",
    "num_rows = modeling_dataset.count()\n",
    "num_cols = len(modeling_dataset.columns)\n",
    "print(f\"Final DataSet shape: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31c2db",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f975-668b-4ab3-a228-4af82187778e",
   "metadata": {},
   "source": [
    "## Preparing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa7d59-2d01-4dcf-868f-900431b44be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 1: Creating train/test split...\n",
      "‚úÖ Data split completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÜ Training samples: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üß™ Test samples: 2\n",
      "Feature columns: ['AGE_AT_ICU_ADMISSION', 'GENDER_BINARY', 'CAME_FROM_ER', 'HAS_INSURANCE', 'ADMISSION_TYPE_ENCODED', 'ETHNICITY_ENCODED', 'MARITAL_STATUS_ENCODED', 'RELIGION_ENCODED', 'FIRST_UNIT_ENCODED', 'CHANGED_ICU_UNIT', 'Routine Vital Signs__sum', 'Routine Vital Signs__count', 'Respiratory__sum', 'Respiratory__count', 'Labs__sum', 'Labs__count', 'Alarms__sum', 'Alarms__count', 'Neurological__sum', 'Neurological__count', 'Hemodynamics__sum', 'Hemodynamics__count', 'Hematology_sum', 'Chemistry_sum', 'Blood Gas_sum', 'Hematology_count', 'Chemistry_count', 'Blood Gas_count', 'PRIMARY_DIAGNOSIS', 'SECONDARY_DIAGNOSIS', 'TERTIARY_DIAGNOSIS', 'TOTAL_DIAGNOSES']\n",
      "Target column: ICU_LOS_DAYS\n",
      "üìä Step 2: Creating the final vectorized train/test datasets...\n",
      "‚úÖ Final datasets prepared:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÜ Training features shape: (11, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üß™ Test features shape: (1, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 1: Creating train/test split...\")\n",
    "train_data, test_data = modeling_dataset.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "print(\"‚úÖ Data split completed.\")\n",
    "print(f\"   üöÜ Training samples: {train_data.count()}\")\n",
    "print(f\"   üß™ Test samples: {test_data.count()}\")\n",
    "\n",
    "\n",
    "feature_columns = [col for col in modeling_dataset.columns if col != 'ICU_LOS_DAYS']\n",
    "print(\"Feature columns:\", feature_columns)\n",
    "target_column = 'ICU_LOS_DAYS'\n",
    "print(\"Target column:\", target_column)\n",
    "\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,  \n",
    "    outputCol=\"features\"     \n",
    ")\n",
    "\n",
    "print(\"üìä Step 2: Creating the final vectorized train/test datasets...\")\n",
    "train_final = feature_assembler.transform(train_data).select(\n",
    "    \"features\", \n",
    "    target_column\n",
    ").withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "test_final = feature_assembler.transform(test_data).select(\n",
    "    \"features\", \n",
    "    target_column\n",
    ").withColumnRenamed(target_column, \"label\")\n",
    "\n",
    "train_final.cache()\n",
    "test_final.cache()\n",
    "\n",
    "print(\"‚úÖ Final datasets prepared:\")\n",
    "print(f\"   üöÜ Training features shape: ({train_final.count()}, {len(feature_columns)})\")\n",
    "print(f\"   üß™ Test features shape: ({test_final.count()}, {len(feature_columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c98beb-ae1b-445a-aa7f-9002b4d9a012",
   "metadata": {},
   "source": [
    "## Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7d1c4-c1ef-4261-a3b5-e05db415626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 1: Setting up evaluation metrics...\n",
      "‚úÖ Evaluation metrics configured: RMSE, MAE, R¬≤\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Step 1: Setting up evaluation metrics...\")\n",
    "\n",
    "# Create regression evaluators\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured: RMSE, MAE, R¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f74323-98d0-44fd-b21f-f2e543449457",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25b20c-1f60-4782-b27c-6fda0121af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Step 2: Training Linear Regression model...\n",
      "üïê Started at: 12:42:02\n",
      "   üîÑ Training Linear Regression...\n",
      "   üîÑ Linear Regression - Making predictions (test data)...\n",
      "   üîÑ Linear Regression - Evaluation...\n",
      "‚úÖ Linear Regression Results:\n",
      "   üìâ RMSE: 1.166 days\n",
      "   üìä MAE: 1.166 days\n",
      "   üìà R¬≤: -inf\n",
      "üïê Completed at: 12:42:03\n",
      "‚è±Ô∏è Total elapsed time: 1.42 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà Step 2: Training Linear Regression model...\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Linear Regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=200,                    # Increased for better convergence\n",
    "    regParam=0.001,                 # Lower regularization for healthcare data\n",
    "    elasticNetParam=0.1,            # Slight L1 penalty for feature selection\n",
    "    tol=1e-8,                       # Tighter tolerance for precision\n",
    "    standardization=False,          # We're doing manual scaling\n",
    "    fitIntercept=True,\n",
    "    aggregationDepth=3,             # Better for distributed training\n",
    "    loss=\"squaredError\",\n",
    "    solver=\"normal\"                 # Best for small-medium datasets\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"   üîÑ Training Linear Regression...\")\n",
    "lr_model = lr.fit(train_final)\n",
    "\n",
    "print(\"   üîÑ Linear Regression - Making predictions (test data)...\")\n",
    "lr_predictions = lr_model.transform(test_final)\n",
    "\n",
    "print(\"   üîÑ Linear Regression - Evaluation...\")\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_mae = mae_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"‚úÖ Linear Regression Results:\")\n",
    "print(f\"   üìâ RMSE: {lr_rmse:.3f} days\")\n",
    "print(f\"   üìä MAE: {lr_mae:.3f} days\")\n",
    "print(f\"   üìà R¬≤: {lr_r2:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847f98b-4da8-41ff-b26f-f2943c8baa38",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52707ea4-eb6a-449d-8160-7013057b2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ Step 3: Training Random Forest model...\n",
      "üïê Started at: 12:42:03\n",
      "   üîÑ Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 12:42:04 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 11 (= number of training instances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Random Forest - Making predictions (test data)...\n",
      "   üîÑ Random Forest - Evaluation...\n",
      "‚úÖ Random Forest Results:\n",
      "   üìâ RMSE: 1.511 days\n",
      "   üìä MAE: 1.511 days\n",
      "   üìà R¬≤: -inf\n",
      "üïê Completed at: 12:42:07\n",
      "‚è±Ô∏è Total elapsed time: 3.46 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüå≤ Step 3: Training Random Forest model...\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=200,                   # More trees = better accuracy (if enough cores/memory)\n",
    "    maxDepth=12,                    # Deeper trees capture more complexity\n",
    "    minInstancesPerNode=2,          # Allows more granular splits\n",
    "    subsamplingRate=0.9,            # Slightly higher sample rate for stability\n",
    "    featureSubsetStrategy=\"sqrt\",   # Good default for regression\n",
    "    seed=42                         # Reproducibility\n",
    ")\n",
    "\n",
    "print(\"   üîÑ Training Random Forest...\")\n",
    "rf_model = rf.fit(train_final)\n",
    "\n",
    "print(\"   üîÑ Random Forest - Making predictions (test data)...\")\n",
    "rf_predictions = rf_model.transform(test_final)\n",
    "\n",
    "print(\"   üîÑ Random Forest - Evaluation...\")\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = mae_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"‚úÖ Random Forest Results:\")\n",
    "print(f\"   üìâ RMSE: {rf_rmse:.3f} days\")\n",
    "print(f\"   üìä MAE: {rf_mae:.3f} days\")\n",
    "print(f\"   üìà R¬≤: {rf_r2:.3f}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28b690",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59901b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Linear Regression Predictions:\n",
      "+----------+-------------+--------------+-------------+\n",
      "|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+-------------+--------------+-------------+\n",
      "|1.2597    |2.426        |1.166         |92.57        |\n",
      "+----------+-------------+--------------+-------------+\n",
      "\n",
      "\n",
      "üå≤ Random Forest Predictions:\n",
      "+----------+-------------+--------------+-------------+\n",
      "|Actual_LOS|Predicted_LOS|Absolute_Error|Percent_Error|\n",
      "+----------+-------------+--------------+-------------+\n",
      "|1.2597    |2.771        |1.511         |119.98       |\n",
      "+----------+-------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n",
    "\n",
    "print(\"\\nüìà Linear Regression Predictions:\")\n",
    "lr_display = lr_predictions.select(\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ")\n",
    "\n",
    "lr_display.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Predictions\n",
    "print(\"\\nüå≤ Random Forest Predictions:\")\n",
    "rf_display = rf_predictions.select(\n",
    "    col(\"label\").alias(\"Actual_LOS\"),\n",
    "    round(col(\"prediction\"), 3).alias(\"Predicted_LOS\"),\n",
    "    round(abs(col(\"label\") - col(\"prediction\")), 3).alias(\"Absolute_Error\"),\n",
    "    round(((abs(col(\"label\") - col(\"prediction\")) / col(\"label\")) * 100), 2).alias(\"Percent_Error\")\n",
    ")\n",
    "\n",
    "rf_display.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0d610-37c0-4c99-9fc0-289846b54033",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff000d-87eb-4b26-976a-65db81b057f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Step 5: Model Performance Comparison...\n",
      "üìä Model Performance Summary:\n",
      "+-----------------+------------------+------------------+---------+\n",
      "|Model            |RMSE              |MAE               |R2       |\n",
      "+-----------------+------------------+------------------+---------+\n",
      "|Linear Regression|1.1661060025268237|1.1661060025268237|-Infinity|\n",
      "|Random Forest    |1.5113595333333345|1.5113595333333345|-Infinity|\n",
      "+-----------------+------------------+------------------+---------+\n",
      "\n",
      "\n",
      "ü•á Best Models:\n",
      "   üéØ Lowest RMSE: Linear Regression (1.166 days)\n",
      "   üìà Highest R¬≤: Linear Regression (-inf)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüèÜ Step 5: Model Performance Comparison...\")\n",
    "\n",
    "# Create comparison summary\n",
    "results_data = [\n",
    "    (\"Linear Regression\", lr_rmse, lr_mae, lr_r2),\n",
    "    (\"Random Forest\", rf_rmse, rf_mae, rf_r2)\n",
    "]\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, [\"Model\", \"RMSE\", \"MAE\", \"R2\"])\n",
    "\n",
    "print(\"üìä Model Performance Summary:\")\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "\n",
    "best_rmse_model = builtins.min(results_data, key=operator.itemgetter(1))\n",
    "best_r2_model = builtins.max(results_data, key=operator.itemgetter(3))\n",
    "\n",
    "print(f\"\\nü•á Best Models:\")\n",
    "print(f\"   üéØ Lowest RMSE: {best_rmse_model[0]} ({best_rmse_model[1]:.3f} days)\")\n",
    "print(f\"   üìà Highest R¬≤: {best_r2_model[0]} ({best_r2_model[3]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4810d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485f2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
