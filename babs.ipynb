{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e628fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration flags\n",
    "#SAMPLE_ENABLE = False\n",
    "#SAMPLE_SIZE = 20000\n",
    "#MIMIC_PATH = \"mimic-db-short\"\n",
    "#\n",
    "#\n",
    "#\n",
    "#print(\"üè• Loading MIMIC-III data...\")\n",
    "#\n",
    "## Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "#print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "#\n",
    "#\n",
    "#chartevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "#print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "## Step 2: Load ICUSTAYS \n",
    "#print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "#icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "#\n",
    "#\n",
    "#\n",
    "## Step 3: Apply sampling if enabled\n",
    "#if SAMPLE_ENABLE:\n",
    "#    print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "#    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "#    icustays_df.cache()\n",
    "#    actual_sample_size = icustays_df.count()\n",
    "#    print(f\"‚úÖ Final sample: {actual_sample_size} ICU stays\")\n",
    "#else:\n",
    "#    icustays_df.cache()\n",
    "#    actual_sample_size = icustays_df.count()\n",
    "#\n",
    "#    \n",
    "#    \n",
    "## Step 4: Create efficient lookup tables\n",
    "#print(\"üìã Creating ID lookup tables...\")\n",
    "#icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "#hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "#subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "#\n",
    "#icu_lookup.count()  # Trigger caching\n",
    "#hadm_lookup.count()\n",
    "#subject_lookup.count()\n",
    "#\n",
    "## Step 5: Load other tables with optimized joins\n",
    "#print(\"üìÇ Loading PATIENTS table...\")\n",
    "#patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "#patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "#\n",
    "#print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "#admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "#admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "#print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "#diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "#diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "## Step 6: Load and filter CHARTEVENTS efficiently\n",
    "#print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "#chartevents_df = chartevents_df \\\n",
    "#    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "#    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "#\n",
    "## Step 7: Load LABEVENTS\n",
    "#print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "#\n",
    "#labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "#\n",
    "#\n",
    "#labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "#\n",
    "## Final summary\n",
    "#print(\"\\n‚úÖ Data loading complete!\")\n",
    "#print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "#print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "#print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "#print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "#print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "#print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "#print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa415f02",
   "metadata": {},
   "source": [
    "# Tentativa #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18d9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier  # or any other algorithm\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator  # or MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import coalesce, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d7878",
   "metadata": {},
   "source": [
    "## 0. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d909d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/02 10:32:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataMLProject\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5f879",
   "metadata": {},
   "source": [
    "## 1. Load Data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336f5bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./mimic-db-short\"\n",
    "\n",
    "items = spark.read.csv(f\"{DATA_PATH}/D_ITEMS.csv\", header=True, inferSchema=True)\n",
    "procedures = spark.read.csv(f\"{DATA_PATH}/PROCEDURES_ICD.csv\", header=True, inferSchema=True)\n",
    "\n",
    "chartevents = spark.read.csv(f\"{DATA_PATH}/CHARTEVENTS.csv\", header=True, inferSchema=True)\n",
    "patients = spark.read.csv(f\"{DATA_PATH}/PATIENTS.csv\", header=True, inferSchema=True)\n",
    "admissions = spark.read.csv(f\"{DATA_PATH}/ADMISSIONS.csv\", header=True, inferSchema=True)\n",
    "diagnoses = spark.read.csv(f\"{DATA_PATH}/DIAGNOSES_ICD.csv\", header=True, inferSchema=True)\n",
    "icustays = spark.read.csv(f\"{DATA_PATH}/ICUSTAYS.csv\", header=True, inferSchema=True)\n",
    "inputevents = spark.read.csv(f\"{DATA_PATH}/INPUTEVENTS_MV.csv\", header=True, inferSchema=True)\n",
    "labevents = spark.read.csv(f\"{DATA_PATH}/LABEVENTS.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Data Loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d30ae",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde8d6c",
   "metadata": {},
   "source": [
    "The tables were chose based on its properties regarding a persons' illness, bodily atributes or gravity of the situation. The tables were:\n",
    "* ChartEVents\n",
    "* Admissions\n",
    "* Patients\n",
    "* Diagnoses\n",
    "* ICUStays\n",
    "* InputEvents_MV\n",
    "* Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4aa7c",
   "metadata": {},
   "source": [
    "### A. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9195d1",
   "metadata": {},
   "source": [
    "The majoriry of the atributes are not useful to predict the duration of a ICU stay, therefore, for a initial analysis, we will choose the ones we consider relevant.\n",
    "\n",
    "First of all, we create temporary views of all tables into a SQL-like table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4224427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 10:33:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "chartevents.createOrReplaceTempView(\"chartevents\")\n",
    "patients.createOrReplaceTempView(\"patients\")\n",
    "admissions.createOrReplaceTempView(\"admissions\")\n",
    "diagnoses.createOrReplaceTempView(\"diagnoses\")\n",
    "icustays.createOrReplaceTempView(\"icustays\")\n",
    "inputevents.createOrReplaceTempView(\"inputevents\")\n",
    "labevents.createOrReplaceTempView(\"labevents\")\n",
    "procedures.createOrReplaceTempView(\"procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134f8e1",
   "metadata": {},
   "source": [
    "SO QUEREMOS PESSOAS QUE TENHAM ICUSTAY_ID PQ: given a person admitted int he icu i have to predict for how long they are gonna be there, therefore in pre processing i will ignore the data from people that were never in the icu. The most efficient approach is to create temporary views that pre-filter the data before joining tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4915eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW icu_filtered AS SELECT * FROM icustays WHERE icustay_id IS NOT NULL\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW chart_filtered AS SELECT * FROM chartevents WHERE icustay_id IS NOT NULL\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW input_filtered AS SELECT * FROM inputevents WHERE icustay_id IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596afa5",
   "metadata": {},
   "source": [
    "To get procedures done to icu patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831615a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW proc_filtered AS\n",
    "SELECT \n",
    "    p.icd9_code,\n",
    "    icu.icustay_id\n",
    "FROM procedures p\n",
    "JOIN icustays icu ON p.hadm_id = icu.hadm_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4459387",
   "metadata": {},
   "source": [
    "To handle labevents time to be only during the icustay: get only one row by labevent, the first valid (not null result) the person did when enterying the icu. To predcit the legth someone will stay, when they first enter, we will only have the first test, so we will train the machine learning using this first value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e297e838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW lab_filtered AS\n",
    "SELECT \n",
    "    lab.subject_id,\n",
    "    lab.hadm_id,\n",
    "    lab.itemid,\n",
    "    lab.valuenum,\n",
    "    lab.valueuom,\n",
    "    icu.icustay_id,\n",
    "    lab.charttime \n",
    "FROM labevents lab\n",
    "JOIN admissions adm ON lab.hadm_id = adm.hadm_id\n",
    "JOIN icustays icu ON adm.hadm_id = icu.hadm_id\n",
    "WHERE \n",
    "    lab.charttime BETWEEN icu.intime AND icu.outtime\n",
    "    AND lab.hadm_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW lab_filtered2 AS\n",
    "SELECT \n",
    "    subject_id,\n",
    "    hadm_id,\n",
    "    itemid,\n",
    "    FIRST_VALUE(valuenum) IGNORE NULLS OVER (\n",
    "        PARTITION BY icustay_id, itemid \n",
    "        ORDER BY charttime\n",
    "    ) AS valuenum,\n",
    "    icustay_id\n",
    "FROM lab_filtered\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfc8b8",
   "metadata": {},
   "source": [
    "Then, we create a table, using a SQL query, with the relevant features for predicting the icu stay duration, and its own column (LOS from ICUStays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eaab2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+------------+-----------+-----+----------+---------+-------+-------------+------------+-----------+-----------+-------------+--------------+--------------+------+\n",
      "|icustay_id|admission_type|  admission_location|chart_itemid|chart_value|error|lab_itemid|lab_value|seq_num|diagnose_code|input_itemid|     amount|       rate|patientweight|procedure_code|first_careunit|   LOS|\n",
      "+----------+--------------+--------------------+------------+-----------+-----+----------+---------+-------+-------------+------------+-----------+-----------+-------------+--------------+--------------+------+\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225158|0.033333335|  2.0000001|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225158|0.033333335|  2.0000001|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      223258|0.033333335|  2.0000001|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      223258|0.033333335|  2.0000001|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225154|  2.5000002|       NULL|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225154|  2.5000002|       NULL|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      226372|      630.0|       NULL|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      226372|      630.0|       NULL|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225158| 5.74844658|  4.9986492|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225158| 5.74844658|  4.9986492|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      223258| 5.74844658|  4.9986492|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      223258| 5.74844658|  4.9986492|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225158| 2.31481474|  9.9206346|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225158| 2.31481474|  9.9206346|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      221749|0.555555588|0.400025625|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      221749|0.555555588|0.400025625|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      220949|      100.0|       NULL|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      220949|      100.0|       NULL|         99.2|          3522|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225850|        2.0|       NULL|         99.2|          3961|          CSRU|1.2488|\n",
      "|    207525|      ELECTIVE|PHYS REFERRAL/NOR...|      224001|       NULL|    0|     51301|     14.6|     12|        45829|      225850|        2.0|       NULL|         99.2|          3522|          CSRU|1.2488|\n",
      "+----------+--------------+--------------------+------------+-----------+-----+----------+---------+-------+-------------+------------+-----------+-----------+-------------+--------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                 (0 + 1) / 1][Stage 112:>                (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        icu.icustay_id,\n",
    "        adm.admission_type,\n",
    "        adm.admission_location,\n",
    "        chart.itemid AS chart_itemid,\n",
    "        chart.valuenum AS chart_value,\n",
    "        chart.error,\n",
    "        lab.itemid AS lab_itemid,\n",
    "        lab.valuenum AS lab_value,\n",
    "        diag.seq_num,\n",
    "        diag.icd9_code as diagnose_code,\n",
    "        input.itemid AS input_itemid,\n",
    "        input.amount,\n",
    "        input.rate,\n",
    "        input.patientweight,\n",
    "        proc.icd9_code as procedure_code,\n",
    "        icu.first_careunit,\n",
    "        icu.LOS\n",
    "    FROM icu_filtered icu\n",
    "    JOIN admissions adm ON icu.hadm_id = adm.hadm_id\n",
    "    LEFT JOIN chart_filtered chart ON icu.icustay_id = chart.icustay_id\n",
    "    LEFT JOIN lab_filtered2 lab ON icu.icustay_id = lab.icustay_id\n",
    "    LEFT JOIN diagnoses diag ON icu.hadm_id = diag.hadm_id\n",
    "    LEFT JOIN input_filtered input ON icu.icustay_id = input.icustay_id\n",
    "    LEFT JOIN proc_filtered proc ON proc.icustay_id=icu.icustay_id\n",
    "\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940abee4",
   "metadata": {},
   "source": [
    "merdas para justificar no relatorio:\n",
    "\n",
    "Juntei as tabelas icu e admissions por admissao ao hospital e nao por pessoa, pq pode haver mais doq uma admissao por pessoa. Fiz outter join pelo msm motivo, mas dps tenho de testar isso com os dados todos e nao so com as tabelas parciais\n",
    "\n",
    "na chartevents, juntei por icu stay, Each row associated with one ITEMID (e.g. 212) corresponds to an instantiation of the same measurement (e.g. heart rate) entao nao vale a pena por a coluna VALUEUOM is the unit of measurement, pq cada teste ja e feito numa unidade de medida, n vale a pena tar a dar mais do msm a ml. Vou por o erro pq se houve erro ent conta menos para a ml mas ig que e melhor q nada??? problema para a ml e nao para nos.\n",
    "\n",
    "aqui cada chartevent, ou seja, analie/teste/raiox etc corresponde as uma linha, mais tarde temos de resolver este problema pq o objetivo e prever o tempo de internacao POR ICUSTAY_ID, ent deviamos ter apenas uma linha por admissao//icustay\n",
    "\n",
    "na tabela dos diagnosticos SEQ_NUM provides the order in which the ICD diagnoses relate to the patient, o quao importante a doenca e no caso da pessoa, e ICD9_CODE contains the actual code corresponding to the diagnosis, ou seja a doenca id, por isso para dar join dou por paciente ou por admissao? aceito opinioes, por agr pus admissoes pq e oq pus no resto\n",
    "\n",
    "input events dei join pela icustay ja que so queremos analisar pacientes que ja tenham estado. Each row associated with one ITEMID which corresponds to an instantiation of the same measurement (e.g. norepinephrine) AMOUNT - the amount of a drug or substance administered to the patient (ignorando se esta em ml dl ou l pelo motivo referido acima)\n",
    "\n",
    "\n",
    "When predicting ICU length of stay (LOS), incorporating lab results can significantly improve your model's performance, but you're right that joining requires careful handling since LABEVENTS doesn't directly contain ICU stay IDs. juntar pelo ham id ou subject e dps escolher com base no tempo\n",
    "\n",
    "\n",
    "the id is the icustays\n",
    "\n",
    "justificar o left join:\n",
    "For your use case, keep the LEFT JOINs but understand why:\n",
    "\n",
    "    LEFT JOIN (icu ‚Üí others) is correct because:\n",
    "\n",
    "        You want all ICU stays (base table)\n",
    "\n",
    "        You want to keep ICU stays even if they're missing some diagnoses/procedures\n",
    "\n",
    "    Don't use FULL OUTER JOIN because:\n",
    "\n",
    "        It would include diagnoses/procedures for non-ICU patients (if any exist)\n",
    "\n",
    "        Could create NULL ICU stay records which you don't want\n",
    "\n",
    "    Your current approach is good for:\n",
    "\n",
    "        One row per combination of ICU stay + diagnosis + procedure\n",
    "\n",
    "        Preserving all relationships\n",
    "\n",
    "Example of What You'll Get\n",
    "\n",
    "For a patient with:\n",
    "\n",
    "    1 ICU stay\n",
    "\n",
    "    4 diagnoses\n",
    "\n",
    "    5 procedures\n",
    "\n",
    "    10 lab results\n",
    "\n",
    "Your query will produce 4 √ó 5 √ó 10 = 200 rows for this patient (all combinations).\n",
    "\n",
    "\n",
    "FALTA:\n",
    "altura / bmi do paciente\n",
    "genero do paciente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c546ea",
   "metadata": {},
   "source": [
    "Attribute/column documentation for the table created:\n",
    "\n",
    " 1. Patient Identification\n",
    "- **icustay_id**: Unique ICU stay identifier (primary key for ICU stays)\n",
    "\n",
    " 2. Admission Information\n",
    "- **admission_type**: Type of admission (ELECTIVE, EMERGENCY, URGENT, etc.)\n",
    "- **admission_location**: Source of admission (TRANSFER FROM HOSPITAL, CLINIC REFERRAL, etc.)\n",
    "- **first_careunit**: Initial ICU care unit (MICU, SICU, CSRU, etc.)\n",
    "- **LOS**: Length of stay in ICU (in hours or days)\n",
    "\n",
    " 3. Clinical Measurements\n",
    "- **chart_itemid**: Identifier for charted measurement (foreign key to D_ITEMS)\n",
    "- **chart_value**: Numeric value of the clinical measurement\n",
    "- **chart_error**: Error flag for the measurement (if exists)\n",
    "- **lab_itemid**: Identifier for laboratory test (foreign key to D_ITEMS)\n",
    "- **lab_value**: Result value of the laboratory test\n",
    "\n",
    " 4. Diagnostic Information\n",
    "- **diagnose_code**: ICD-9 diagnosis code\n",
    "- **seq_num**: Priority/sequence number of the diagnosis (1=primary)\n",
    "\n",
    " 5. Treatment Information\n",
    "- **input_itemid**: Identifier for input event (medications/fluids)\n",
    "- **input_amount**: Quantity administered\n",
    "- **input_rate**: Rate of administration\n",
    "- **input_patientweight**: Patient weight at time of input (if recorded)\n",
    "\n",
    " 6. Procedural Information\n",
    "- **procedure_code**: ICD-9 procedure code performed during stay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52cb96",
   "metadata": {},
   "source": [
    "transform your multiple rows per ICU stay into a single row format suitable for ML modeling while preserving both continuous values and procedure/diagnosis information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cf026",
   "metadata": {},
   "source": [
    "### B. Get one row by icu stay ID "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a048721",
   "metadata": {},
   "source": [
    "Handle LabEvents and InputEvents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e828d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_pivot_topN(df, top_n=20):\n",
    "    ## Step 1: Get top N most frequent lab and input items ##\n",
    "    \n",
    "    # Get top N lab_itemids\n",
    "    top_lab_items = (\n",
    "        df.filter(\"lab_itemid IS NOT NULL\")\n",
    "        .groupBy(\"lab_itemid\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .limit(top_n)\n",
    "        .select(\"lab_itemid\")\n",
    "        .collect()\n",
    "    )\n",
    "    lab_itemids = [row['lab_itemid'] for row in top_lab_items]\n",
    "    \n",
    "    # Get top N input_itemids\n",
    "    top_input_items = (\n",
    "        df.filter(\"input_itemid IS NOT NULL\")\n",
    "        .groupBy(\"input_itemid\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .limit(top_n)\n",
    "        .select(\"input_itemid\")\n",
    "        .collect()\n",
    "    )\n",
    "    input_itemids = [row['input_itemid'] for row in top_input_items]\n",
    "    \n",
    "    ## Step 2: Pivot only top N lab data ##\n",
    "    lab_window = Window.partitionBy(\"icustay_id\", \"lab_itemid\").orderBy(\"lab_value\")\n",
    "    \n",
    "    lab_pivoted = (\n",
    "        df.filter(F.col(\"lab_itemid\").isin(lab_itemids))  # Only top N items\n",
    "        .withColumn(\"rn\", F.row_number().over(lab_window))\n",
    "        .filter(\"rn = 1\")\n",
    "        .groupBy(\"icustay_id\")\n",
    "        .pivot(\"lab_itemid\", lab_itemids)\n",
    "        .agg(F.first(\"lab_value\"))\n",
    "        .fillna(-1)\n",
    "    )\n",
    "    \n",
    "    ## Step 3: Pivot only top N input events ##\n",
    "    input_window = Window.partitionBy(\"icustay_id\", \"input_itemid\").orderBy(F.desc(\"amount\"))\n",
    "    \n",
    "    input_pivoted = (\n",
    "        df.filter(F.col(\"input_itemid\").isin(input_itemids))  # Only top N items\n",
    "        .withColumn(\"rn\", F.row_number().over(input_window))\n",
    "        .filter(\"rn = 1\")\n",
    "        .groupBy(\"icustay_id\")\n",
    "        .pivot(\"input_itemid\", input_itemids)\n",
    "        .agg(\n",
    "            F.first(\"amount\").alias(\"amount\"),\n",
    "            F.first(\"rate\").alias(\"rate\")\n",
    "        )\n",
    "        .fillna(-1)\n",
    "    )\n",
    "    \n",
    "    ## Step 4: Get patientweight ##\n",
    "    patientweight_df = (\n",
    "        df.filter(\"patientweight IS NOT NULL\")\n",
    "        .groupBy(\"icustay_id\")\n",
    "        .agg(F.last(\"patientweight\").alias(\"patientweight\"))\n",
    "    )\n",
    "    \n",
    "    ## Step 5: Get base columns ##\n",
    "    base_cols = [\n",
    "        \"icustay_id\", \"admission_type\", \"admission_location\", \n",
    "        \"chart_itemid\", \"chart_value\", \"error\", \"lab_itemid\", \n",
    "        \"lab_value\", \"seq_num\", \"diagnose_code\", \"procedure_code\",\n",
    "        \"first_careunit\", \"LOS\"\n",
    "    ]\n",
    "    base_df = df.select(base_cols).distinct()\n",
    "    \n",
    "    ## Step 6: Join all components ##\n",
    "    df_final = (\n",
    "        base_df\n",
    "        .join(lab_pivoted, \"icustay_id\", \"left\")\n",
    "        .join(input_pivoted, \"icustay_id\", \"left\")\n",
    "        .join(patientweight_df, \"icustay_id\", \"left\")\n",
    "        .fillna(-1)\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "264d9220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 11:57:47 WARN CacheManager: Asked to cache already cached data.\n",
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 208:>  (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/barbara/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/barbara/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Repartition BEFORE processing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mcache()  \u001b[38;5;66;03m# Cache the repartitioned dataframe\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial DF partitions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39mgetNumPartitions()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should show 200\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Then apply your pivot function\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df_2 \u001b[38;5;241m=\u001b[39m efficient_pivot_topN(df, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:214\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m<class 'pyspark.rdd.RDD'>\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[1;32m    216\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 12:01:15 WARN MemoryStore: Not enough space to cache rdd_883_1 in memory! (computed 3.4 GiB so far)\n",
      "25/06/02 12:01:15 WARN BlockManager: Persisting block rdd_883_1 to disk instead.\n",
      "25/06/02 12:37:22 WARN MemoryStore: Not enough space to cache rdd_883_1 in memory! (computed 2.3 GiB so far)\n",
      "25/06/02 12:42:48 WARN MemoryStore: Not enough space to cache rdd_883_14 in memory! (computed 2.4 GiB so far)\n",
      "25/06/02 12:42:48 WARN BlockManager: Persisting block rdd_883_14 to disk instead.\n",
      "25/06/02 13:35:24 WARN MemoryStore: Not enough space to cache rdd_883_78 in memory! (computed 2.9 GiB so far)\n",
      "25/06/02 13:35:24 WARN BlockManager: Persisting block rdd_883_78 to disk instead.\n",
      "[Stage 195:>  (0 + 1) / 1][Stage 196:>  (0 + 1) / 1][Stage 234:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# First repartition your input dataframe\n",
    "df = df.repartition(200, \"icustay_id\")  # Repartition BEFORE processing\n",
    "df.cache()  # Cache the repartitioned dataframe\n",
    "\n",
    "print(f\"Initial DF partitions: {df.rdd.getNumPartitions()}\")  # Should show 200\n",
    "\n",
    "# Then apply your pivot function\n",
    "df_2 = efficient_pivot_topN(df, top_n=20)\n",
    "\n",
    "# Cache the final result if you'll be using it multiple times\n",
    "df_2.cache()\n",
    "print(f\"Final DF partitions: {df_2.rdd.getNumPartitions()}\")  # May be different after operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5acdfb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 208:>  (0 + 1) / 1]  \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/barbara/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/barbara/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_2 \u001b[38;5;241m=\u001b[39m \u001b[43mefficient_pivot_topN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_2\u001b[38;5;241m.\u001b[39mcache()\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mefficient_pivot_topN\u001b[0;34m(df, top_n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mefficient_pivot_topN\u001b[39m(df, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m## Step 1: Get top N most frequent lab and input items ##\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Get top N lab_itemids\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     top_lab_items \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m         \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlab_itemid IS NOT NULL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlab_itemid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlab_itemid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     lab_itemids \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlab_itemid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m top_lab_items]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Get top N input_itemids\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_2 = efficient_pivot_topN(df, top_n=20)\n",
    "df_2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587230e0",
   "metadata": {},
   "source": [
    "Handle Procedures and Diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 196:>  (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/barbara/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/barbara/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m top_procedures_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Create one-hot encoding using pivot (faster than CASE WHEN)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m procedure_encoded \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     15\u001b[0m     \u001b[43mdf_final\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_procedures_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocedure_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Broadcast small DF\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43micustay_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocedure_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mfirst(F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Pivot with 1/0 encoding\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Rename columns to match your original pattern\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;241m*\u001b[39m[F\u001b[38;5;241m.\u001b[39mcol(code)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocedure_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m     25\u001b[0m         [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocedure_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m top_procedures_df\u001b[38;5;241m.\u001b[39mcollect()]]\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 3. Join back using optimized join strategy\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df_final_with_procedures \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     31\u001b[0m     df_final\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(procedure_encoded, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocedure_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m top_procedures_df])\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/group.py:521\u001b[0m, in \u001b[0;36mGroupedData.pivot\u001b[0;34m(self, pivot_col, values)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03mPivots a column of the current :class:`DataFrame` and perform the specified aggregation.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03mThere are two versions of the pivot function: one that requires the caller\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m+----+-----+------+\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     jgd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpivot_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     jgd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39mpivot(pivot_col, values)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def add_top_features(df_final, top_n=20):\n",
    "    \"\"\"Add top N procedure and diagnosis features to the final DataFrame\"\"\"\n",
    "    \n",
    "    # 1. TOP PROCEDURES ======================================================\n",
    "    top_procedures = (\n",
    "        df_final.filter(\"procedure_code IS NOT NULL\")\n",
    "        .groupBy(\"procedure_code\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .limit(top_n)\n",
    "        .select(\"procedure_code\")\n",
    "        .collect()\n",
    "    )\n",
    "    top_procedure_codes = [row['procedure_code'] for row in top_procedures]\n",
    "    \n",
    "    # One-hot encode only top procedures\n",
    "    procedure_encoded = (\n",
    "        df_final\n",
    "        .filter(F.col(\"procedure_code\").isin(top_procedure_codes))\n",
    "        .groupBy(\"icustay_id\")\n",
    "        .pivot(\"procedure_code\", top_procedure_codes)\n",
    "        .agg(F.first(F.lit(1)).otherwise(0))\n",
    "        .fillna(0)\n",
    "        .select(\n",
    "            \"icustay_id\",\n",
    "            *[F.col(code).alias(f\"procedure_{code}\") for code in top_procedure_codes]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2. TOP DIAGNOSES ======================================================\n",
    "    top_diagnoses = (\n",
    "        df_final.filter(\"diagnose_code IS NOT NULL\")\n",
    "        .groupBy(\"diagnose_code\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .limit(top_n)\n",
    "        .select(\"diagnose_code\")\n",
    "        .collect()\n",
    "    )\n",
    "    top_diagnosis_codes = [row['diagnose_code'] for row in top_diagnoses]\n",
    "    \n",
    "    # Importance-weighted diagnosis encoding (only for top diagnoses)\n",
    "    window_spec = Window.partitionBy(\"icustay_id\", \"diagnose_code\").orderBy(\"seq_num\")\n",
    "    \n",
    "    diag_pivoted = (\n",
    "        df_final\n",
    "        .filter(F.col(\"diagnose_code\").isin(top_diagnosis_codes))\n",
    "        .withColumn(\"importance\", F.lit(1)/F.col(\"seq_num\"))\n",
    "        .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .groupBy(\"icustay_id\")\n",
    "        .pivot(\"diagnose_code\", top_diagnosis_codes)\n",
    "        .agg(F.first(\"importance\"))\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    # Rename diagnosis columns\n",
    "    for col in diag_pivoted.columns:\n",
    "        if col != \"icustay_id\":\n",
    "            diag_pivoted = diag_pivoted.withColumnRenamed(col, f\"diag_{col}\")\n",
    "    \n",
    "    # 3. JOIN ALL FEATURES ==================================================\n",
    "    df_final_with_features = (\n",
    "        df_final\n",
    "        .join(procedure_encoded, \"icustay_id\", \"left\")\n",
    "        .join(diag_pivoted, \"icustay_id\", \"left\")\n",
    "        .fillna(0, subset=(\n",
    "            [f\"procedure_{code}\" for code in top_procedure_codes] +\n",
    "            [f\"diag_{code}\" for code in top_diagnosis_codes]\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    return df_final_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  \n",
    "df_2 = df_2.repartition(20, \"icustay_id\")\n",
    "\n",
    "df_final = add_top_features(df_2, top_n=20)\n",
    "df_final.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe35563",
   "metadata": {},
   "source": [
    "### C. Every entry has to be numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01b28e",
   "metadata": {},
   "source": [
    "See data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeea381",
   "metadata": {},
   "source": [
    "### D. Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545b1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad30fee",
   "metadata": {},
   "source": [
    "## 3. Target variable preparation (if categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(inputCol=target_col, outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91a495",
   "metadata": {},
   "source": [
    "## 4. Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stages = indexers + [assembler, scaler, label_indexer]\n",
    "preprocessing_pipeline = Pipeline(stages=pipeline_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05217c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ad849",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = processed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"Training count: {train_data.count()}\")\n",
    "print(f\"Test count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b9b79",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6042b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2abab",
   "metadata": {},
   "source": [
    "## 7. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac762e4e",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e14538",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiclass classification\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(f\"Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed6fd7",
   "metadata": {},
   "source": [
    "## 9. Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592609c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
