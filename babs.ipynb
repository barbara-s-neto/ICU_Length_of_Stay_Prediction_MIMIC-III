{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa415f02",
   "metadata": {},
   "source": [
    "# Tentativa #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18d9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier  # or any other algorithm\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator  # or MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import coalesce, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d7878",
   "metadata": {},
   "source": [
    "## 0. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d909d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/31 16:16:03 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/05/31 16:16:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/31 16:16:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataMLProject\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5f879",
   "metadata": {},
   "source": [
    "## 1. Load Data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336f5bff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/user/msi/bdcc/proj2/babs-bdcc2/mimic-db-short/D_ITEMS.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m DATA_PATH = \u001b[33m\"\u001b[39m\u001b[33m./mimic-db-short\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m items = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/D_ITEMS.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m procedures = spark.read.csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/D_ICD_PROCEDURES.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m chartevents = spark.read.csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/CHARTEVENTS.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/home/user/msi/bdcc/proj2/babs-bdcc2/mimic-db-short/D_ITEMS.csv."
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./mimic-db-short\"\n",
    "\n",
    "items = spark.read.csv(f\"{DATA_PATH}/D_ITEMS.csv\", header=True, inferSchema=True)\n",
    "procedures = spark.read.csv(f\"{DATA_PATH}/D_ICD_PROCEDURES.csv\", header=True, inferSchema=True)\n",
    "\n",
    "chartevents = spark.read.csv(f\"{DATA_PATH}/CHARTEVENTS.csv\", header=True, inferSchema=True)\n",
    "patients = spark.read.csv(f\"{DATA_PATH}/PATIENTS.csv\", header=True, inferSchema=True)\n",
    "admissions = spark.read.csv(f\"{DATA_PATH}/ADMISSIONS.csv\", header=True, inferSchema=True)\n",
    "diagnoses = spark.read.csv(f\"{DATA_PATH}/DIAGNOSES_ICD.csv\", header=True, inferSchema=True)\n",
    "icustays = spark.read.csv(f\"{DATA_PATH}/ICUSTAYS.csv\", header=True, inferSchema=True)\n",
    "inputevents = spark.read.csv(f\"{DATA_PATH}/INPUTEVENTS_MV.csv\", header=True, inferSchema=True)\n",
    "labevents = spark.read.csv(f\"{DATA_PATH}/LABEVENTS.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Data Loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d30ae",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde8d6c",
   "metadata": {},
   "source": [
    "The tables were chose based on its properties regarding a persons' illness, bodily atributes or gravity of the situation. The tables were:\n",
    "* ChartEVents\n",
    "* Admissions\n",
    "* Patients\n",
    "* Diagnoses\n",
    "* ICUStays\n",
    "* InputEvents_MV\n",
    "* Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4aa7c",
   "metadata": {},
   "source": [
    "### A. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9195d1",
   "metadata": {},
   "source": [
    "The majoriry of the atributes are not useful to predict the duration of a ICU stay, therefore, for a initial analysis, we will choose the ones we consider relevant.\n",
    "\n",
    "First of all, we create temporary views of all tables into a SQL-like table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4224427",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chartevents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchartevents\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchartevents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m admissions\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmissions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m diagnoses\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnoses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chartevents' is not defined"
     ]
    }
   ],
   "source": [
    "chartevents.createOrReplaceTempView(\"chartevents\")\n",
    "patients.createOrReplaceTempView(\"patients\")\n",
    "admissions.createOrReplaceTempView(\"admissions\")\n",
    "diagnoses.createOrReplaceTempView(\"diagnoses\")\n",
    "icustays.createOrReplaceTempView(\"icustays\")\n",
    "inputevents.createOrReplaceTempView(\"inputevents\")\n",
    "labevents.createOrReplaceTempView(\"labevents\")\n",
    "procedures.createOrReplaceTempView(\"procedures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134f8e1",
   "metadata": {},
   "source": [
    "SO QUEREMOS PESSOAS QUE TENHAM ICUSTAY_ID PQ: given a person admitted int he icu i have to predict for how long they are gonna be there, therefore in pre processing i will ignore the data from people that were never in the icu. The most efficient approach is to create temporary views that pre-filter the data before joining tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW icu_filtered AS SELECT * FROM icustays WHERE icustay_id IS NOT NULL\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW chart_filtered AS SELECT * FROM chartevents WHERE icustay_id IS NOT NULL\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW input_filtered AS SELECT * FROM inputevents WHERE icustay_id IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596afa5",
   "metadata": {},
   "source": [
    "To get procedures done to icu patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831615a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW proc_filtered AS\n",
    "SELECT \n",
    "    p.icd9_code,\n",
    "    icu.icustay_id\n",
    "FROM procedures p\n",
    "JOIN icustays icu ON p.hadm_id = icu.hadm_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4459387",
   "metadata": {},
   "source": [
    "To handle labevents time to be only during the icustay: get only one row by labevent, the first valid (not null result) the person did when enterying the icu. To predcit the legth someone will stay, when they first enter, we will only have the first test, so we will train the machine learning using this first value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297e838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW lab_filtered AS\n",
    "SELECT \n",
    "    lab.subject_id,\n",
    "    lab.hadm_id,\n",
    "    lab.itemid,\n",
    "    lab.valuenum,\n",
    "    lab.valueuom,\n",
    "    icu.icustay_id \n",
    "FROM labevents lab\n",
    "JOIN admissions adm ON lab.hadm_id = adm.hadm_id\n",
    "JOIN icustays icu ON adm.hadm_id = icu.hadm_id\n",
    "WHERE \n",
    "    lab.charttime BETWEEN icu.intime AND icu.outtime\n",
    "    AND lab.hadm_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW lab_filtered2 AS\n",
    "SELECT \n",
    "    subject_id,\n",
    "    hadm_id,\n",
    "    itemid,\n",
    "    FIRST_VALUE(valuenum) IGNORE NULLS OVER (\n",
    "        PARTITION BY icustay_id, itemid \n",
    "        ORDER BY charttime\n",
    "    ) AS valuenum,\n",
    "    icustay_id\n",
    "FROM lab_filtered\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfc8b8",
   "metadata": {},
   "source": [
    "Then, we create a table, using a SQL query, with the relevant features for predicting the icu stay duration, and its own column (LOS from ICUStays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaab2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+------------+-----------+-----+----------+---------+-------+-------------+------------+------+----+-------------+--------------+--------------+-------+\n",
      "|icustay_id|admission_type|  admission_location|chart_itemid|chart_value|error|lab_itemid|lab_value|seq_num|diagnose_code|input_itemid|amount|rate|patientweight|procedure_code|first_careunit|    LOS|\n",
      "+----------+--------------+--------------------+------------+-----------+-----+----------+---------+-------+-------------+------------+------+----+-------------+--------------+--------------+-------+\n",
      "|    285977|        URGENT|TRANSFER FROM HOS...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          CSRU| 0.8181|\n",
      "|    279205|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|         TSICU| 6.6602|\n",
      "|    247324|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|           CCU| 4.0284|\n",
      "|    221620|     EMERGENCY|TRANSFER FROM HOS...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 2.7713|\n",
      "|    235270|       NEWBORN|PHYS REFERRAL/NOR...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          NICU| 3.3084|\n",
      "|    239661|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          SICU| 0.5826|\n",
      "|    210582|       NEWBORN|PHYS REFERRAL/NOR...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          NICU|  0.091|\n",
      "|    235149|     EMERGENCY|PHYS REFERRAL/NOR...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          CSRU| 4.1009|\n",
      "|    221404|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 3.3105|\n",
      "|    214854|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 1.5035|\n",
      "|    271990|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|           CCU| 6.4001|\n",
      "|    203392|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 1.7411|\n",
      "|    232833|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|         TSICU| 1.4444|\n",
      "|    294610|     EMERGENCY|CLINIC REFERRAL/P...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|           CCU| 3.2903|\n",
      "|    281427|      ELECTIVE|PHYS REFERRAL/NOR...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          CSRU|17.1131|\n",
      "|    233499|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 2.4009|\n",
      "|    298816|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 2.2156|\n",
      "|    249354|     EMERGENCY|EMERGENCY ROOM ADMIT|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          MICU| 0.7986|\n",
      "|    225173|     EMERGENCY|TRANSFER FROM HOS...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|           CCU| 6.0799|\n",
      "|    268049|     EMERGENCY|PHYS REFERRAL/NOR...|        NULL|       NULL| NULL|      NULL|     NULL|   NULL|         NULL|        NULL|  NULL|NULL|         NULL|          NULL|          CSRU| 3.0277|\n",
      "+----------+--------------+--------------------+------------+-----------+-----+----------+---------+-------+-------------+------------+------+----+-------------+--------------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        icu.icustay_id,\n",
    "        adm.admission_type,\n",
    "        adm.admission_location,\n",
    "        chart.itemid AS chart_itemid,\n",
    "        chart.valuenum AS chart_value,\n",
    "        chart.error,\n",
    "        lab.itemid AS lab_itemid,\n",
    "        lab.valuenum AS lab_value,\n",
    "        diag.seq_num,\n",
    "        diag.icd9_code as diagnose_code,\n",
    "        input.itemid AS input_itemid,\n",
    "        input.amount,\n",
    "        input.rate,\n",
    "        input.patientweight,\n",
    "        proc.icd9_code as procedure_code,\n",
    "        icu.first_careunit,\n",
    "        icu.LOS\n",
    "    FROM icu_filtered icu\n",
    "    JOIN admissions adm ON icu.hadm_id = adm.hadm_id\n",
    "    LEFT JOIN chart_filtered chart ON icu.icustay_id = chart.icustay_id\n",
    "    LEFT JOIN lab_filtered2 lab ON icu.icustay_id = lab.icustay_id\n",
    "    LEFT JOIN diagnoses diag ON icu.hadm_id = diag.hadm_id\n",
    "    LEFT JOIN input_filtered input ON icu.icustay_id = input.icustay_id\n",
    "    LEFT JOIN proc_filtered proc ON proc.icustay_id=icu.icustay_id\n",
    "\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940abee4",
   "metadata": {},
   "source": [
    "merdas para justificar no relatorio:\n",
    "\n",
    "Juntei as tabelas icu e admissions por admissao ao hospital e nao por pessoa, pq pode haver mais doq uma admissao por pessoa. Fiz outter join pelo msm motivo, mas dps tenho de testar isso com os dados todos e nao so com as tabelas parciais\n",
    "\n",
    "na chartevents, juntei por icu stay, Each row associated with one ITEMID (e.g. 212) corresponds to an instantiation of the same measurement (e.g. heart rate) entao nao vale a pena por a coluna VALUEUOM is the unit of measurement, pq cada teste ja e feito numa unidade de medida, n vale a pena tar a dar mais do msm a ml. Vou por o erro pq se houve erro ent conta menos para a ml mas ig que e melhor q nada??? problema para a ml e nao para nos.\n",
    "\n",
    "aqui cada chartevent, ou seja, analie/teste/raiox etc corresponde as uma linha, mais tarde temos de resolver este problema pq o objetivo e prever o tempo de internacao POR ICUSTAY_ID, ent deviamos ter apenas uma linha por admissao//icustay\n",
    "\n",
    "na tabela dos diagnosticos SEQ_NUM provides the order in which the ICD diagnoses relate to the patient, o quao importante a doenca e no caso da pessoa, e ICD9_CODE contains the actual code corresponding to the diagnosis, ou seja a doenca id, por isso para dar join dou por paciente ou por admissao? aceito opinioes, por agr pus admissoes pq e oq pus no resto\n",
    "\n",
    "input events dei join pela icustay ja que so queremos analisar pacientes que ja tenham estado. Each row associated with one ITEMID which corresponds to an instantiation of the same measurement (e.g. norepinephrine) AMOUNT - the amount of a drug or substance administered to the patient (ignorando se esta em ml dl ou l pelo motivo referido acima)\n",
    "\n",
    "\n",
    "When predicting ICU length of stay (LOS), incorporating lab results can significantly improve your model's performance, but you're right that joining requires careful handling since LABEVENTS doesn't directly contain ICU stay IDs. juntar pelo ham id ou subject e dps escolher com base no tempo\n",
    "\n",
    "\n",
    "the id is the icustays\n",
    "\n",
    "justificar o left join:\n",
    "For your use case, keep the LEFT JOINs but understand why:\n",
    "\n",
    "    LEFT JOIN (icu → others) is correct because:\n",
    "\n",
    "        You want all ICU stays (base table)\n",
    "\n",
    "        You want to keep ICU stays even if they're missing some diagnoses/procedures\n",
    "\n",
    "    Don't use FULL OUTER JOIN because:\n",
    "\n",
    "        It would include diagnoses/procedures for non-ICU patients (if any exist)\n",
    "\n",
    "        Could create NULL ICU stay records which you don't want\n",
    "\n",
    "    Your current approach is good for:\n",
    "\n",
    "        One row per combination of ICU stay + diagnosis + procedure\n",
    "\n",
    "        Preserving all relationships\n",
    "\n",
    "Example of What You'll Get\n",
    "\n",
    "For a patient with:\n",
    "\n",
    "    1 ICU stay\n",
    "\n",
    "    4 diagnoses\n",
    "\n",
    "    5 procedures\n",
    "\n",
    "    10 lab results\n",
    "\n",
    "Your query will produce 4 × 5 × 10 = 200 rows for this patient (all combinations).\n",
    "\n",
    "\n",
    "FALTA:\n",
    "altura / bmi do paciente\n",
    "genero do paciente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c546ea",
   "metadata": {},
   "source": [
    "Attribute/column documentation for the table created:\n",
    "\n",
    " 1. Patient Identification\n",
    "- **icustay_id**: Unique ICU stay identifier (primary key for ICU stays)\n",
    "\n",
    " 2. Admission Information\n",
    "- **admission_type**: Type of admission (ELECTIVE, EMERGENCY, URGENT, etc.)\n",
    "- **admission_location**: Source of admission (TRANSFER FROM HOSPITAL, CLINIC REFERRAL, etc.)\n",
    "- **first_careunit**: Initial ICU care unit (MICU, SICU, CSRU, etc.)\n",
    "- **LOS**: Length of stay in ICU (in hours or days)\n",
    "\n",
    " 3. Clinical Measurements\n",
    "- **chart_itemid**: Identifier for charted measurement (foreign key to D_ITEMS)\n",
    "- **chart_value**: Numeric value of the clinical measurement\n",
    "- **chart_error**: Error flag for the measurement (if exists)\n",
    "- **lab_itemid**: Identifier for laboratory test (foreign key to D_ITEMS)\n",
    "- **lab_value**: Result value of the laboratory test\n",
    "\n",
    " 4. Diagnostic Information\n",
    "- **diagnose_code**: ICD-9 diagnosis code\n",
    "- **seq_num**: Priority/sequence number of the diagnosis (1=primary)\n",
    "\n",
    " 5. Treatment Information\n",
    "- **input_itemid**: Identifier for input event (medications/fluids)\n",
    "- **input_amount**: Quantity administered\n",
    "- **input_rate**: Rate of administration\n",
    "- **input_patientweight**: Patient weight at time of input (if recorded)\n",
    "\n",
    " 6. Procedural Information\n",
    "- **procedure_code**: ICD-9 procedure code performed during stay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52cb96",
   "metadata": {},
   "source": [
    "transform your multiple rows per ICU stay into a single row format suitable for ML modeling while preserving both continuous values and procedure/diagnosis information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cf026",
   "metadata": {},
   "source": [
    "### B. Get one row by icu stay ID - GHOST CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a048721",
   "metadata": {},
   "source": [
    "Handle LabEvents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"original_data\")\n",
    "\n",
    "# Step 1 & 2: Pivot in a single SQL operation\n",
    "pivot_query = \"\"\"\n",
    "WITH lab_data AS (\n",
    "    SELECT \n",
    "        icustay_id,\n",
    "        lab_itemid,\n",
    "        FIRST(lab_value) OVER (PARTITION BY icustay_id, lab_itemid ORDER BY lab_value) AS lab_value\n",
    "    FROM original_data\n",
    "    WHERE lab_itemid IS NOT NULL\n",
    "    GROUP BY icustay_id, lab_itemid, lab_value\n",
    ")\n",
    "SELECT \n",
    "    icustay_id,\n",
    "    {pivot_columns}\n",
    "FROM lab_data\n",
    "PIVOT (\n",
    "    FIRST(lab_value)\n",
    "    FOR lab_itemid IN ({itemid_list})\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Get distinct lab_itemids (for large datasets, consider sampling first)\n",
    "itemids = [str(row['lab_itemid']) for row in df.filter(\"lab_itemid IS NOT NULL\").select(\"lab_itemid\").distinct().collect()]\n",
    "\n",
    "# Generate the pivot columns and IN clause\n",
    "pivot_columns = \", \".join(itemids)\n",
    "itemid_list = \", \".join([f\"'{id}'\" for id in itemids])\n",
    "\n",
    "# Execute the pivot\n",
    "lab_pivoted = spark.sql(pivot_query.format(\n",
    "    pivot_columns=pivot_columns,\n",
    "    itemid_list=itemid_list\n",
    "))\n",
    "\n",
    "# Step 3: Fill nulls (now in SQL)\n",
    "fill_nulls_query = \"\"\"\n",
    "SELECT\n",
    "    icustay_id,\n",
    "    {coalesce_expressions}\n",
    "FROM lab_pivoted\n",
    "\"\"\"\n",
    "\n",
    "coalesce_exprs = [f\"COALESCE({col}, -1) AS {col}\" for col in itemids]\n",
    "lab_pivoted_filled = spark.sql(fill_nulls_query.format(\n",
    "    coalesce_expressions=\",\\n    \".join(coalesce_exprs)\n",
    "))\n",
    "\n",
    "# Step 4 & 5: Final join in a single SQL operation\n",
    "final_query = \"\"\"\n",
    "SELECT \n",
    "    base.*,\n",
    "    {lab_columns}\n",
    "FROM (\n",
    "    SELECT \n",
    "        icustay_id,\n",
    "        {base_columns}\n",
    "    FROM original_data\n",
    "    GROUP BY icustay_id, {base_columns}\n",
    ") base\n",
    "LEFT JOIN lab_pivoted_filled lab ON base.icustay_id = lab.icustay_id\n",
    "\"\"\"\n",
    "\n",
    "# Get all columns except lab columns\n",
    "base_cols = [col for col in df.columns if col not in [\"lab_itemid\", \"lab_value\"]]\n",
    "lab_cols = \", \".join(itemids)\n",
    "\n",
    "df_final = spark.sql(final_query.format(\n",
    "    lab_columns=lab_cols,\n",
    "    base_columns=\", \".join(base_cols),\n",
    "    base_columns_list=\", \".join(base_cols)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa594e52",
   "metadata": {},
   "source": [
    "Handle InputEvents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"combined_data\")\n",
    "\n",
    "# Step 1: Pivot input events in SQL (more efficient for big data)\n",
    "input_pivot_sql = \"\"\"\n",
    "SELECT \n",
    "    icustay_id,\n",
    "    {pivot_expressions}\n",
    "FROM (\n",
    "    SELECT \n",
    "        icustay_id,\n",
    "        input_itemid,\n",
    "        amount,\n",
    "        rate,\n",
    "        patientweight,\n",
    "        ROW_NUMBER() OVER (PARTITION BY icustay_id, input_itemid ORDER BY amount DESC) as rn\n",
    "    FROM combined_data\n",
    "    WHERE input_itemid IS NOT NULL\n",
    ") \n",
    "WHERE rn = 1  -- Get one record per icustay_id and input_itemid\n",
    "GROUP BY icustay_id\n",
    "\"\"\"\n",
    "\n",
    "# Dynamically generate pivot expressions for all itemids\n",
    "# First get distinct itemids (for big data, sample first if too many)\n",
    "itemids = spark.sql(\"SELECT DISTINCT input_itemid FROM combined_data WHERE input_itemid IS NOT NULL\").collect()\n",
    "itemids = [str(row['input_itemid']) for row in itemids]\n",
    "\n",
    "pivot_exprs = []\n",
    "for itemid in itemids:\n",
    "    pivot_exprs.append(f\"MAX(CASE WHEN input_itemid = {itemid} THEN amount END) AS {itemid}_amount\")\n",
    "    pivot_exprs.append(f\"MAX(CASE WHEN input_itemid = {itemid} THEN rate END) AS {itemid}_rate\")\n",
    "\n",
    "pivot_sql = input_pivot_sql.format(pivot_expressions=\",\\n    \".join(pivot_exprs))\n",
    "\n",
    "# Execute the pivot\n",
    "input_pivoted = spark.sql(pivot_sql)\n",
    "\n",
    "# Step 2: Get the latest patientweight (using SQL)\n",
    "patientweight_sql = \"\"\"\n",
    "SELECT \n",
    "    icustay_id,\n",
    "    LAST(patientweight, true) as patientweight  -- true ignores nulls\n",
    "FROM combined_data\n",
    "WHERE patientweight IS NOT NULL\n",
    "GROUP BY icustay_id\n",
    "\"\"\"\n",
    "patientweight_df = spark.sql(patientweight_sql)\n",
    "\n",
    "# Step 3: Join the pivoted data with patientweight\n",
    "input_final_sql = \"\"\"\n",
    "SELECT \n",
    "    p.*,\n",
    "    w.patientweight\n",
    "FROM input_pivoted p\n",
    "LEFT JOIN patientweight_df w ON p.icustay_id = w.icustay_id\n",
    "\"\"\"\n",
    "input_final = spark.sql(input_final_sql)\n",
    "\n",
    "# Step 4: Create final DataFrame by joining with original data (minus input columns)\n",
    "final_sql = \"\"\"\n",
    "SELECT \n",
    "    orig.*,\n",
    "    {input_columns},\n",
    "    inp.patientweight\n",
    "FROM (\n",
    "    SELECT DISTINCT \n",
    "        icustay_id,\n",
    "        admission_type,\n",
    "        admission_location,\n",
    "        chart_itemid,\n",
    "        chart_value,\n",
    "        error,\n",
    "        lab_itemid,\n",
    "        lab_value,\n",
    "        seq_num,\n",
    "        diagnose_code,\n",
    "        procedure_code,\n",
    "        first_careunit,\n",
    "        LOS\n",
    "    FROM combined_data\n",
    ") orig\n",
    "LEFT JOIN input_final inp ON orig.icustay_id = inp.icustay_id\n",
    "\"\"\"\n",
    "\n",
    "# Generate column list for input columns\n",
    "input_columns = [f\"inp.{itemid}_amount, inp.{itemid}_rate\" for itemid in itemids]\n",
    "input_columns = \",\\n    \".join(input_columns)\n",
    "\n",
    "final_sql = final_sql.format(input_columns=input_columns)\n",
    "\n",
    "# Execute final query\n",
    "df_final = spark.sql(final_sql)\n",
    "\n",
    "# Fill nulls with -1 for input columns\n",
    "input_cols = [f\"{itemid}_amount\" for itemid in itemids] + [f\"{itemid}_rate\" for itemid in itemids]\n",
    "df_final = df_final.fillna(-1, subset=input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587230e0",
   "metadata": {},
   "source": [
    "Handle Procedures:\n",
    "\n",
    "Ajustar o tamanho dos top procedures para tmb n ter uma tabela com 50000000 colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First get top N most frequent procedures\n",
    "top_procedures = df_final.filter(\"procedure_code IS NOT NULL\") \\\n",
    "                         .groupBy(\"procedure_code\") \\\n",
    "                         .count() \\\n",
    "                         .orderBy(\"count\", ascending=False) \\\n",
    "                         .limit(100) \\\n",
    "                         .collect()\n",
    "top_procedure_codes = [row['procedure_code'] for row in top_procedures]\n",
    "\n",
    "# 2. Create one-hot encoded columns using CASE WHEN\n",
    "select_exprs = [\n",
    "    f\"MAX(CASE WHEN procedure_code = '{code}' THEN 1 ELSE 0 END) AS procedure_{code}\"\n",
    "    for code in top_procedure_codes\n",
    "]\n",
    "\n",
    "procedure_encoded = df_final.groupBy(\"icustay_id\") \\\n",
    "                           .agg(*select_exprs)\n",
    "\n",
    "# 3. Join back to main DataFrame\n",
    "df_final_with_procedures = df_final.join(procedure_encoded, on=\"icustay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9f508",
   "metadata": {},
   "source": [
    "Handle Diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate importance score (1/seq_num) and get the most important record\n",
    "window_spec = Window.partitionBy(\"icustay_id\", \"diagnose_code\").orderBy(\"seq_num\")\n",
    "\n",
    "diagnoses = (df_final\n",
    "    .filter(F.col(\"diagnose_code\").isNotNull())\n",
    "    .withColumn(\"importance\", F.lit(1)/F.col(\"seq_num\"))  # Calculate 1/seq_num\n",
    "    .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rn\") == 1)  # Take only the first occurrence\n",
    "    .select(\"icustay_id\", \"diagnose_code\", \"importance\")\n",
    ")\n",
    "\n",
    "# Step 2: Pivot with importance scores\n",
    "diag_pivoted = (diagnoses\n",
    "    .groupBy(\"icustay_id\")\n",
    "    .pivot(\"diagnose_code\")\n",
    "    .agg(F.first(\"importance\"))  # Use the importance score instead of seq_num\n",
    "    .fillna(0)  # 0 indicates diagnosis not present (since 1/seq_num will always be > 0)\n",
    ")\n",
    "\n",
    "# Rename columns to add 'diag_' prefix\n",
    "for col in diag_pivoted.columns:\n",
    "    if col != \"icustay_id\":\n",
    "        diag_pivoted = diag_pivoted.withColumnRenamed(col, f\"diag_{col}\")\n",
    "\n",
    "# Step 3: Join back to main DataFrame\n",
    "df_final_with_diag = df_final.join(diag_pivoted, on=\"icustay_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = df_final_with_diag\n",
    "df_ready.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe35563",
   "metadata": {},
   "source": [
    "### C. Every entry has to be numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01b28e",
   "metadata": {},
   "source": [
    "See data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeea381",
   "metadata": {},
   "source": [
    "### D. Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545b1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad30fee",
   "metadata": {},
   "source": [
    "## 3. Target variable preparation (if categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(inputCol=target_col, outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91a495",
   "metadata": {},
   "source": [
    "## 4. Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_stages = indexers + [assembler, scaler, label_indexer]\n",
    "preprocessing_pipeline = Pipeline(stages=pipeline_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05217c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocessing_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ad849",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = processed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"Training count: {train_data.count()}\")\n",
    "print(f\"Test count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b9b79",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6042b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2abab",
   "metadata": {},
   "source": [
    "## 7. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac762e4e",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e14538",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiclass classification\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(f\"Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed6fd7",
   "metadata": {},
   "source": [
    "## 9. Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592609c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
