{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c6b365",
   "metadata": {},
   "source": [
    "# Predicting Lenght Of Stay at the ICU using Machine Learning\n",
    "\n",
    "## **Introduction**  \n",
    "\n",
    "This work focuses on predicting ICU length of stay (LOS) using the **MIMIC-III** dataset, a large-scale collection of electronic health records from over 26,000 ICU patients. The dataset includes diverse clinical variables such as vital signs, lab results, and treatment records, presenting both temporal and high-dimensional challenges. Given the volume and complexity of the data, we employ scalable machine learning techniques to preprocess, model, and interpret predictive patterns. The analysis aims to assist in resource allocation and clinical decision-making while addressing computational constraints inherent in big healthcare data.\n",
    "\n",
    "Given the dataset's size, traditional single-node tools (e.g., Pandas, Scikit-learn) would be inefficient due to memory and computational limitations. Instead, we leverage **PySpark**  for both Data Processing and Machine Learning.\n",
    "\n",
    "1. **Scalability** ‚Äì PySpark efficiently processes large datasets by distributing computations across a cluster, making it ideal for big data workloads.  \n",
    "2. **In-Memory Processing** ‚Äì Spark's in-memory execution significantly speeds up iterative operations, crucial for machine learning pipelines.  \n",
    "3. **Integration with MLlib** ‚Äì PySpark provides MLlib, a scalable machine learning library that supports preprocessing, feature engineering, and model training on distributed datasets.  \n",
    "4. **Fault Tolerance & Optimization** ‚Äì Spark‚Äôs lazy evaluation and DAG-based execution optimize performance, while its fault-tolerant design ensures reliability.  \n",
    "5. **Compatibility with Big Data Ecosystems** ‚Äì PySpark integrates seamlessly with storage systems (HDFS, S3) and SQL-based tools (Spark SQL), facilitating efficient data handling.  \n",
    "\n",
    "\n",
    "Our analysis follows a structured machine learning pipeline:  \n",
    "\n",
    "1. **Load Data & Initialize Session** - Get the data from the Google Cloud Storage.\n",
    "2. **Exploratory Data Analysis** - Statiscial overview of the data.\n",
    "3. **Feature Engineering** ‚Äì Aggregating patient records over a chosen window size to balance predictive power and computational feasibility.  \n",
    "4. **Data Preprocessing** ‚Äì Handling missing values, encoding categorical variables, and normalizing features using PySpark‚Äôs DataFrame API. \n",
    "5. **Model Training & Validation** ‚Äì Employing distributed ML algorithms via MLlib, with hyperparameter tuning using Cross Validaton.  \n",
    "6. **Performance Profiling** ‚Äì Monitoring execution time and resource usage to identify bottlenecks.  \n",
    "7. **Interpretation of Results** ‚Äì Evaluating model performance using metrics, discussing feature importance and justifying the chosen window size.  \n",
    "\n",
    "By leveraging **PySpark**, we ensure that our pipeline is both scalable and efficient, enabling robust analysis of ICU patient data while maintaining computational feasibility. The following sections detail our approach, results, and key insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b0d2d",
   "metadata": {},
   "source": [
    "## **1.** Load Data & Initialize Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe124a5",
   "metadata": {},
   "source": [
    "Import needed libraries:\n",
    "* Pyspark SQL\n",
    "* Pyspark ML feature, functions, regression, evaluation and tuning\n",
    "* Datatime and Time\n",
    "* \n",
    "* OS, SYS and warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8476828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All imports loaded successfully!\n",
      "‚è∞ Notebook started at: 2025-06-07 19:05:48\n",
      "üêç Python version: 3.10.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/07 19:05:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Spark version: 4.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# üì¶ PYSPARK CORE & DATAFRAME OPERATIONS\n",
    "# =============================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,    lit,    sum as sql_sum,    count,    mean,    stddev,    min,    max,\n",
    "    percentile_approx,    when,    coalesce,    isnull,    datediff,    row_number,\n",
    "    first,    broadcast,    floor,    udf,    expr\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# üîß DATA PROCESSING & FEATURE ENGINEERING\n",
    "# =============================================\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StandardScaler,\n",
    "    StringIndexer,\n",
    "    MinMaxScaler,\n",
    "    Imputer,\n",
    "    Bucketizer\n",
    ")\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# ü§ñ MACHINE LEARNING MODELS\n",
    "# =============================================\n",
    "from pyspark.ml.regression import (\n",
    "    RandomForestRegressor,\n",
    "    LinearRegression,\n",
    "    GBTRegressor\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# üìä MODEL EVALUATION & HYPERPARAMETER TUNING\n",
    "# =============================================\n",
    "from pyspark.ml.evaluation import (\n",
    "    RegressionEvaluator,\n",
    "    BinaryClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.tuning import (\n",
    "    CrossValidator, \n",
    "    ParamGridBuilder\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# üõ†Ô∏è UTILITY LIBRARIES\n",
    "# =============================================\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# ‚öôÔ∏è SYSTEM CONFIGURATION\n",
    "# =============================================\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ All imports loaded successfully!\")\n",
    "print(f\"‚è∞ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
    "print(f\"üî• Spark version: {SparkSession.builder.getOrCreate().version}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020f9f6",
   "metadata": {},
   "source": [
    "Initialize spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3f7894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created successfully!\n",
      "üìä Spark Version: 4.0.0\n",
      "üîß Application Name: pyspark-shell\n",
      "üíæ Available cores: 8\n",
      "\n",
      "‚è∞ Spark session initialised at: 2025-06-07 19:05:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:05:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Forecast-LOS\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\", \"32MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"300s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"300s\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20s\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"128MB\") \\\n",
    "    \\\n",
    "    .config(\"spark.executor.memoryOffHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memoryOffHeap.size\", \"1g\") \\\n",
    "    \\\n",
    "    .getOrCreate()\n",
    "print(\"‚úÖ Spark session created successfully!\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"üîß Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üíæ Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"\\n‚è∞ Spark session initialised at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa5338",
   "metadata": {},
   "source": [
    "Load and prepare the MIMIC-III dataset for analysis in a Spark environment. It first configures data path, then loads ICU stay records and filters related clinical data (vitals, lab results, diagnoses, and demographics) to only include patients with ICU stays.  \n",
    "\n",
    "**Optimizations:** The script improves performance by caching frequently used ICU stay data, using broadcast joins for efficient table merging, and converting CSV files to parquet format for faster reads. It also supports sampling for smaller test datasets.  \n",
    "\n",
    "Finally, report row counts for key tables, including filtered ICU stays, patient records, admissions, diagnoses, measurements, and lab results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69e73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\\n\\nprint(\"üè• Loading MIMIC-III data...\")\\n\\nprint(\"üìÇ Loading CHARTEVENTS...\")\\n\\ntry:\\n   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\\n   print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\\nexcept:\\n   print(\"üìÑ Converting CHARTEVENTS.csv.gz to parquet...\")\\n   chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\\n   chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\\n   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\\n   print(\"‚úÖ Converted and loaded CHARTEVENTS\")\\n\\n\\nprint(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\\nicustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\\n\\nicustays_df.cache()\\nactual_sample_size = icustays_df.count()\\n\\n   \\nprint(\"üìã Creating ID lookup tables...\")\\nicu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\\nhadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\\nsubject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\\n\\nicu_lookup.count()  \\nhadm_lookup.count()\\nsubject_lookup.count()\\n\\n\\nprint(\"üìÇ Loading PATIENTS table...\")\\npatients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\\npatients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\\n\\nprint(\"üìÇ Loading ADMISSIONS table...\")\\nadmissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\\nadmissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\\n\\nprint(\"üìÇ Loading DIAGNOSES_ICD table...\")\\ndiagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\\ndiagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\\nd_items_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/D_ITEMS.csv.gz\")\\n\\n\\nprint(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\\nchartevents_df = chartevents_df    .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\")    .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\\n\\n\\nprint(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\\ntry:\\n   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\\nexcept:\\n   print(\"üìÑ Converting LABEVENTS.csv.gz to parquet...\")\\n   labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\\n   labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\\n   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\\n\\nd_labitems_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/D_LABITEMS.csv.gz\")\\nlabevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\\n\\nprint(\"\\n‚úÖ Data loading complete!\")\\nprint(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\\nprint(f\"üìä PATIENTS: {patients_df.count():,} rows\") \\nprint(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\\nprint(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\\nprint(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\\nprint(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\\nprint(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')}\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIMIC_PATH = \"gs://dataproc-staging-europe-west2-851143487985-hir6gfre/mimic-data\"\n",
    "\n",
    "print(\"üè• Loading MIMIC-III data...\")\n",
    "\n",
    "print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "\n",
    "try:\n",
    "   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "   print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "except:\n",
    "   print(\"üìÑ Converting CHARTEVENTS.csv.gz to parquet...\")\n",
    "   chartevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv.gz\")\n",
    "   chartevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "   chartevents_df = spark.read.parquet(f\"{MIMIC_PATH}/CHARTEVENTS.parquet\")\n",
    "   print(\"‚úÖ Converted and loaded CHARTEVENTS\")\n",
    "\n",
    "\n",
    "print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv.gz\")\n",
    "\n",
    "icustays_df.cache()\n",
    "actual_sample_size = icustays_df.count()\n",
    "\n",
    "   \n",
    "print(\"üìã Creating ID lookup tables...\")\n",
    "icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "\n",
    "icu_lookup.count()  \n",
    "hadm_lookup.count()\n",
    "subject_lookup.count()\n",
    "\n",
    "\n",
    "print(\"üìÇ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv.gz\")\n",
    "patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv.gz\")\n",
    "admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv.gz\")\n",
    "diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "d_items_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/D_ITEMS.csv.gz\")\n",
    "\n",
    "\n",
    "print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "chartevents_df = chartevents_df \\\n",
    "   .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "   .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    "\n",
    "\n",
    "print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "try:\n",
    "   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "except:\n",
    "   print(\"üìÑ Converting LABEVENTS.csv.gz to parquet...\")\n",
    "   labevents_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv.gz\")\n",
    "   labevents_csv.write.mode(\"overwrite\").parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "   labevents_df = spark.read.parquet(f\"{MIMIC_PATH}/LABEVENTS.parquet\")\n",
    "\n",
    "d_labitems_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(f\"{MIMIC_PATH}/D_LABITEMS.csv.gz\")\n",
    "labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"\\n‚úÖ Data loading complete!\")\n",
    "print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac850f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Loading MIMIC-III data...\n",
      "üìÇ Loading CHARTEVENTS...\n",
      "‚úÖ Loaded CHARTEVENTS from parquet\n",
      "\n",
      "üìÇ Loading and filtering ICUSTAYS...\n",
      "üìã Creating ID lookup tables...\n",
      "üìÇ Loading PATIENTS table...\n",
      "üìÇ Loading ADMISSIONS table...\n",
      "üìÇ Loading DIAGNOSES_ICD table...\n",
      "üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\n",
      "üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\n",
      "\n",
      "‚úÖ Data loading complete!\n",
      "üìä ICUSTAYS: 20 rows\n",
      "üìä PATIENTS: 20 rows\n",
      "üìä ADMISSIONS: 20 rows\n",
      "üìä DIAGNOSES_ICD: 212 rows\n",
      "üìä CHARTEVENTS (filtered): 57,973 rows\n",
      "üìä LABEVENTS (filtered): 5,895 rows\n",
      "\n",
      "‚è∞ Data loaded at: 2025-06-07 19:05:59\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SAMPLE_ENABLE = False\n",
    "SAMPLE_SIZE = 20000\n",
    "MIMIC_PATH = \"mimic-db-short\"\n",
    "print(\"üè• Loading MIMIC-III data...\")\n",
    "# Step 1: First, find ICUSTAY_IDs that have ALL required vital signs\n",
    "print(\"üìÇ Loading CHARTEVENTS...\")\n",
    "chartevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/CHARTEVENTS.csv\")\n",
    "d_items_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/D_ITEMS.csv\")\n",
    "print(\"‚úÖ Loaded CHARTEVENTS from parquet\")\n",
    "# Step 2: Load ICUSTAYS \n",
    "print(\"\\nüìÇ Loading and filtering ICUSTAYS...\")\n",
    "icustays_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ICUSTAYS.csv\")\n",
    "# Step 3: Apply sampling if enabled\n",
    "if SAMPLE_ENABLE:\n",
    "    print(f\"üéØ Sampling {SAMPLE_SIZE} ICU stays...\")\n",
    "    icustays_df = icustays_df.limit(SAMPLE_SIZE)\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "    print(f\"‚úÖ Final sample: {actual_sample_size} ICU stays\")\n",
    "else:\n",
    "    icustays_df.cache()\n",
    "    actual_sample_size = icustays_df.count()\n",
    "   \n",
    "   \n",
    "# Step 4: Create efficient lookup tables\n",
    "print(\"üìã Creating ID lookup tables...\")\n",
    "icu_lookup = icustays_df.select(\"ICUSTAY_ID\").distinct().cache()\n",
    "hadm_lookup = icustays_df.select(\"HADM_ID\").distinct().cache()\n",
    "subject_lookup = icustays_df.select(\"SUBJECT_ID\").distinct().cache()\n",
    "icu_lookup.count()  # Trigger caching\n",
    "hadm_lookup.count()\n",
    "subject_lookup.count()\n",
    "# Step 5: Load other tables with optimized joins\n",
    "print(\"üìÇ Loading PATIENTS table...\")\n",
    "patients_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/PATIENTS.csv\")\n",
    "patients_df = patients_df.join(broadcast(subject_lookup), \"SUBJECT_ID\", \"inner\")\n",
    "print(\"üìÇ Loading ADMISSIONS table...\")\n",
    "admissions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "print(\"üìÇ Loading DIAGNOSES_ICD table...\")\n",
    "diagnoses_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/DIAGNOSES_ICD.csv\")\n",
    "diagnoses_df = diagnoses_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "# Step 6: Load and filter CHARTEVENTS efficiently\n",
    "print(\"üìÇ Loading CHARTEVENTS table... [FILTERING BY ICUSTAY_ID]\")\n",
    "chartevents_df = chartevents_df \\\n",
    "     .select(\"ICUSTAY_ID\", \"CHARTTIME\", \"ITEMID\", \"VALUE\", \"VALUEUOM\", \"VALUENUM\") \\\n",
    "     .join(broadcast(icu_lookup), \"ICUSTAY_ID\", \"inner\")\n",
    " # Step 7: Load LABEVENTS\n",
    "print(\"üìÇ Loading LABEVENTS table... [FILTERING BY HADM_ID]\")\n",
    "labevents_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/LABEVENTS.csv\")\n",
    "labevents_df = labevents_df.join(broadcast(hadm_lookup), \"HADM_ID\", \"inner\")\n",
    "d_labitems_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{MIMIC_PATH}/D_LABITEMS.csv\")\n",
    "# Final summary\n",
    "print(\"\\n‚úÖ Data loading complete!\")\n",
    "print(f\"üìä ICUSTAYS: {icustays_df.count():,} rows\")\n",
    "print(f\"üìä PATIENTS: {patients_df.count():,} rows\") \n",
    "print(f\"üìä ADMISSIONS: {admissions_df.count():,} rows\")\n",
    "print(f\"üìä DIAGNOSES_ICD: {diagnoses_df.count():,} rows\")\n",
    "print(f\"üìä CHARTEVENTS (filtered): {chartevents_df.count():,} rows\")\n",
    "print(f\"üìä LABEVENTS (filtered): {labevents_df.count():,} rows\")\n",
    "print(f\"\\n‚è∞ Data loaded at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3ba1f",
   "metadata": {},
   "source": [
    "## **2.** Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248189c6",
   "metadata": {},
   "source": [
    "@Leandro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4cefce",
   "metadata": {},
   "source": [
    "## **3.** Feature Engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8891d1",
   "metadata": {},
   "source": [
    "### Base ICUSTAY, ADMISSION, PATIENTS Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359393a6",
   "metadata": {},
   "source": [
    " **1. ICU Stay Identifiers**  \n",
    "- ICUSTAY_ID - Unique ICU admission identifier  \n",
    "- SUBJECT_ID - Unique patient identifier  \n",
    "- HADM_ID - Unique hospital admission identifier  \n",
    "\n",
    " **2. Target Variable**  \n",
    "- ICU_LOS_DAYS (from LOS) - Length of ICU stay in days  \n",
    "\n",
    " **3. ICU Characteristics**  \n",
    "- FIRST_CAREUNIT - Initial ICU unit type (e.g., MICU, SICU)  \n",
    "- LAST_CAREUNIT - Final ICU unit before discharge  \n",
    "- ICU_INTIME - ICU admission timestamp  \n",
    "- ICU_OUTTIME - ICU discharge timestamp  \n",
    "\n",
    " **4. Patient Demographics**  \n",
    "- GENDER - Patient's sex (M/F)  \n",
    "- PATIENT_DIED (from EXPIRE_FLAG) - Mortality flag (0=survived)  \n",
    "- DOB - Date of birth  \n",
    "\n",
    " **5. Hospital Admission Details**  \n",
    "- ADMISSION_TYPE - Emergency/elective/etc. \n",
    "- ADMISSION_LOCATION - Where was the admission \n",
    "- INSURANCE - Medicare/private/etc.  \n",
    "- ETHNICITY - Patient's ethnicity  \n",
    "- MARITAL_STATUS/RELIGION - Demographic details  \n",
    "\n",
    " **6. Derived Column**  \n",
    "- AGE_AT_ICU_ADMISSION - Computed age in years (using ICU_INTIME - DOB), filtered to adults (18-80)  \n",
    "\n",
    "**Creation Process:**\n",
    "1. Joined ICUSTAYS, PATIENTS, and ADMISSIONS tables  \n",
    "2. Filtered to:  \n",
    "   - Adult patients (18-80 years)  \n",
    "   - Survivors (PATIENT_DIED=0)  \n",
    "3. Dropped cached data to free memory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f3f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating base ICU dataset with patient demographics...\n",
      "‚úÖ Created base ICU dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Creating base ICU dataset with patient demographics...\")\n",
    "\n",
    "base_icu_df = icustays_df.alias(\"icu\") \\\n",
    "    .join(patients_df.alias(\"pat\"), \"SUBJECT_ID\", \"inner\") \\\n",
    "    .join(admissions_df.alias(\"adm\"), [\"SUBJECT_ID\", \"HADM_ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        col(\"icu.ICUSTAY_ID\"),\n",
    "        col(\"icu.SUBJECT_ID\"), \n",
    "        col(\"icu.HADM_ID\"),\n",
    "        \n",
    "        col(\"icu.LOS\").alias(\"ICU_LOS_DAYS\"),\n",
    "\n",
    "        col(\"icu.FIRST_CAREUNIT\"),\n",
    "        col(\"icu.LAST_CAREUNIT\"), \n",
    "        col(\"icu.INTIME\").alias(\"ICU_INTIME\"),\n",
    "        col(\"icu.OUTTIME\").alias(\"ICU_OUTTIME\"),\n",
    "        \n",
    "        col(\"pat.GENDER\"),\n",
    "        col(\"pat.EXPIRE_FLAG\").alias(\"PATIENT_DIED\"),\n",
    "        col(\"pat.DOB\"),\n",
    "        \n",
    "        col(\"adm.ADMISSION_TYPE\"),\n",
    "        col(\"adm.ADMISSION_LOCATION\"),\n",
    "        col(\"adm.INSURANCE\"),\n",
    "        col(\"adm.ETHNICITY\"),\n",
    "        col(\"adm.MARITAL_STATUS\"),\n",
    "        col(\"adm.RELIGION\")\n",
    "    )\n",
    "\n",
    "icustays_df.unpersist()\n",
    "\n",
    "print(\"‚úÖ Created base ICU dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90488fac",
   "metadata": {},
   "source": [
    "#### Removing Outliers\n",
    "\n",
    "To ensure a clinically relevant and statistically robust analysis, we applied the following filters to the dataset:\n",
    "\n",
    "1. Age Restriction (18‚Äì80 years): Pediatric patients (<18) and elderly populations (>65) were excluded due to differing physiological responses and treatment protocols.\n",
    "\n",
    "2. Exclusion of Deceased Patients: Only survivors (PATIENT_DIED = 0) were retained to focus on ICU outcomes for living patients.\n",
    "\n",
    "3. ICU Stay Duration (0‚Äì9.1 days): Based on exploratory data analysis (EDA), ICU stays beyond 9.1 days were identified as statistical outliers and removed to reduce bias from extreme cases.\n",
    "\n",
    "4. Exclusion of cases with invalid timeframes.\n",
    "\n",
    "These filters improve data quality by retaining only the most representative cases for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44bcdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing outlier cases: 20\n",
      "Number of rows after removing outlier cases: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows before removing outlier cases: {base_icu_df.count()}\")\n",
    "\n",
    "base_icu_df = (base_icu_df\n",
    "    .withColumn(\"AGE_AT_ICU_ADMISSION\", \n",
    "                floor(datediff(col(\"ICU_INTIME\"), col(\"DOB\")) / 365.25))\n",
    "    .filter(col(\"AGE_AT_ICU_ADMISSION\").between(18, 65))      \n",
    "    .filter(col(\"PATIENT_DIED\") == 0)                         \n",
    "    .filter(datediff(col(\"ICU_OUTTIME\"), col(\"ICU_INTIME\")) <= 9.1) \n",
    "    .filter(col(\"ICU_INTIME\") < col(\"ICU_OUTTIME\"))\n",
    ")\n",
    "\n",
    "print(f\"Number of rows after removing outlier cases: {base_icu_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4c17b",
   "metadata": {},
   "source": [
    "#### Categorical Feature Engineering  \n",
    "\n",
    "**1. Binary Encodings**  \n",
    "- GENDER_BINARY - Male (1) vs Female (0)  \n",
    "- CAME_FROM_ER - Emergency Room admission (1) vs other sources (0)  \n",
    "- HAS_INSURANCE - Medicare insurance (1) vs other types (0)  \n",
    "- CHANGED_ICU_UNIT - ICU unit transfer occurred (1) vs no transfer (0)  \n",
    "\n",
    "**2. Numeric Encodings**  \n",
    "- ADMISSION_TYPE_ENCODED:  1=EMERGENCY  2=ELECTIVE  3=URGENT  0=Other  \n",
    "\n",
    "- ETHNICITY_ENCODED:  1=WHITE  2=BLACK  3=HISPANIC  4=ASIAN  5=Other  \n",
    "\n",
    "- MARITAL_STATUS_ENCODED:  1=MARRIED  2=SINGLE  3=DIVORCED  4=WIDOWED  5=SEPARATED  6=LIFE PARTNER  0=Other  \n",
    "\n",
    "- RELIGION_ENCODED:  1=CATHOLIC  2=PROTESTANT  3=JEWISH  0=Other  \n",
    "\n",
    "- FIRST_UNIT_ENCODED:  1=MICU (Medical ICU)  2=SICU (Surgical ICU)  3=CSRU (Cardiac Surgery)  4=CCU (Coronary Care)  5=TSICU (Trauma/Surgical)  0=Other  \n",
    "\n",
    "**Creation Process**:  \n",
    "1. Converted categorical variables to numeric representations  \n",
    "2. Created binary flags for key clinical indicators  \n",
    "3. Maintained consistent encoding schemes across similar variables  \n",
    "4. Deleted the original columns\n",
    "\n",
    "These engineered features enable machine learning algorithms to process categorical patient characteristics effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1e15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Engineering categorical features...\n",
      "üìä Dropping useless columns...\n",
      "‚úÖ Base ICU, ADMISSIONS, PATIENTS Table - Finalized\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Engineering categorical features...\")\n",
    "base_icu_df = base_icu_df \\\n",
    "    .withColumn(\"GENDER_BINARY\", when(col(\"GENDER\") == \"M\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"CAME_FROM_ER\", when(col(\"ADMISSION_LOCATION\").contains(\"EMERGENCY\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"HAS_INSURANCE\", when(col(\"INSURANCE\") == \"Medicare\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"ADMISSION_TYPE_ENCODED\", \n",
    "                when(col(\"ADMISSION_TYPE\") == \"EMERGENCY\", 1)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"ELECTIVE\", 2)\n",
    "                .when(col(\"ADMISSION_TYPE\") == \"URGENT\", 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"ETHNICITY_ENCODED\",\n",
    "                when(col(\"ETHNICITY\").contains(\"WHITE\"), 1)\n",
    "                .when(col(\"ETHNICITY\").contains(\"BLACK\"), 2)\n",
    "                .when(col(\"ETHNICITY\").contains(\"HISPANIC\"), 3)\n",
    "                .when(col(\"ETHNICITY\").contains(\"ASIAN\"), 4)\n",
    "                .otherwise(5)) \\\n",
    "    .withColumn(\"MARITAL_STATUS_ENCODED\",\n",
    "                when(col(\"MARITAL_STATUS\") == \"MARRIED\", 1)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"SINGLE\", 2)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"DIVORCED\", 3)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"WIDOWED\", 4)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"SEPARATED\", 5)\n",
    "                .when(col(\"MARITAL_STATUS\") == \"LIFE PARTNER\", 6)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"RELIGION_ENCODED\",\n",
    "                when(col(\"RELIGION\").contains(\"CATHOLIC\"), 1)\n",
    "                .when(col(\"RELIGION\").contains(\"PROTESTANT\"), 2)\n",
    "                .when(col(\"RELIGION\").contains(\"JEWISH\"), 3)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"FIRST_UNIT_ENCODED\", \n",
    "                when(col(\"FIRST_CAREUNIT\") == \"MICU\", 1)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"SICU\", 2)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CSRU\", 3)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"CCU\", 4)\n",
    "                .when(col(\"FIRST_CAREUNIT\") == \"TSICU\", 5)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"CHANGED_ICU_UNIT\", \n",
    "                when(col(\"FIRST_CAREUNIT\") != col(\"LAST_CAREUNIT\"), 1).otherwise(0))\n",
    "\n",
    "print(\"üìä Dropping useless columns...\")\n",
    "\n",
    "drop_cols = [\n",
    "    \"FIRST_CAREUNIT\",\n",
    "    \"LAST_CAREUNIT\",\n",
    "    \"GENDER\",\n",
    "    \"PATIENT_DIED\",\n",
    "    \"DOB\",\n",
    "    \"ADMISSION_TYPE\",\n",
    "    \"ADMISSION_LOCATION\",\n",
    "    \"INSURANCE\",\n",
    "    \"ETHNICITY\",\n",
    "    \"MARITAL_STATUS\",\n",
    "    \"RELIGION\"\n",
    "]\n",
    "\n",
    "base_icu_df = base_icu_df.drop(*drop_cols)\n",
    "\n",
    "print(\"‚úÖ Base ICU, ADMISSIONS, PATIENTS Table - Finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b6594",
   "metadata": {},
   "source": [
    "Base ICUSTAY, ADMISSION, PATIENTS Table final schema:\n",
    "\n",
    "**Core Identifiers**\n",
    "- ICUSTAY_ID - Unique identifier for each ICU admission\n",
    "- SUBJECT_ID - Unique patient identifier\n",
    "- HADM_ID - Unique hospital admission identifier\n",
    "\n",
    "**ICU Stay Metrics**\n",
    "- ICU_LOS_DAYS - Length of stay in ICU (in days)\n",
    "- ICU_INTIME - Timestamp of ICU admission\n",
    "- ICU_OUTTIME - Timestamp of ICU discharge\n",
    "\n",
    "**Patient Demographics**\n",
    "- AGE_AT_ICU_ADMISSION - Patient age at ICU admission (years)\n",
    "- GENDER_BINARY - Gender encoded as binary (1=Male, 0=Female)\n",
    "\n",
    "**Admission Characteristics**\n",
    "- CAME_FROM_ER - Binary flag for ER origin (1=Yes, 0=No)\n",
    "- HAS_INSURANCE - Insurance status (1=Medicare, 0=Other)\n",
    "- ADMISSION_TYPE_ENCODED - Encoded admission type\n",
    "**Demographic Encodings**\n",
    "- ETHNICITY_ENCODED - Numeric ethnicity classification\n",
    "- MARITAL_STATUS_ENCODED - Numeric marital status\n",
    "- RELIGION_ENCODED - Numeric religious affiliation\n",
    "\n",
    "**ICU Unit Information**\n",
    "- FIRST_UNIT_ENCODED - Numeric first unit type\n",
    "- CHANGED_ICU_UNIT - Flag for unit transfers\n",
    "\n",
    "This schema represents the complete feature set derived from joining and processing the core MIMIC-III tables (ICUSTAYS, PATIENTS, and ADMISSIONS), with all categorical variables appropriately encoded for analytical use. The dataset contains both raw temporal data (timestamps) and derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07afd6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[HADM_ID: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icu_stay_ids = base_icu_df.select(\"ICUSTAY_ID\").distinct()\n",
    "icu_stay_ids.cache()\n",
    "\n",
    "hadm_ids = base_icu_df.select(\"HADM_ID\").distinct()\n",
    "hadm_ids.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d7e22",
   "metadata": {},
   "source": [
    "### Clinical Events (CHARTEVENTS) Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953ca15",
   "metadata": {},
   "source": [
    "**Features Processing Pipeline:**\n",
    "\n",
    "**1. Initial Filtering & Time Window Selection**\n",
    "- Extracted key measurement data: ICUSTAY_ID, ITEMID, numeric values (VALUENUM) and timestamps (CHARTTIME)\n",
    "- Applied three-way joining to:\n",
    "  - Filter to only ICU stays in our base table\n",
    "  - Incorporate ICU admission timestamps\n",
    "- Implemented strict quality filters:\n",
    "  - Removed records with null measurement values or timestamps\n",
    "  - Restricted to measurements taken within the **first 24 hours** of ICU admission (using ICU_INTIME)\n",
    "- Cached the resulting dataset for efficient downstream processing\n",
    "\n",
    "2. **Category Enrichment**\n",
    "   - Joined with D_ITEMS table to add CATEGORY information\n",
    "   - Cached the enriched dataset for multiple uses\n",
    "\n",
    "3. **Top Category Analysis**\n",
    "   - Identified top 7 most frequent measurement categories\n",
    "   - Filtered out null categories\n",
    "   - Created dataset containing only measurements from top categories\n",
    "\n",
    "4. **Statistical Aggregation**\n",
    "   - Generated per-patient statistics for each measurement category:\n",
    "     - Sum of all values (`_sum` suffix)\n",
    "     - Count of measurements (`_count` suffix)\n",
    "   - Filled null values with 0 for consistent analysis\n",
    "   - Cleaned up cached DataFrames to free memory\n",
    "\n",
    "**Output Schema:**\n",
    "The final dataFrame contains:\n",
    "- ICUSTAY_ID as the primary key\n",
    "- For each of the top 7 measurement categories:\n",
    "  - [CATEGORY]_sum - Sum of all measurements in this category\n",
    "  - [CATEGORY]_count - Number of measurements in this category\n",
    "\n",
    "This processing creates a feature-rich dataset where each ICU stay has aggregated statistics about the clinical measurements taken during their stay, organized by measurement category. The output is optimized for subsequent machine learning or analytical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9372ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:01 WARN DAGScheduler: Broadcasting large task binary with size 1052.5 KiB\n",
      "25/06/07 19:06:02 WARN DAGScheduler: Broadcasting large task binary with size 1149.0 KiB\n",
      "25/06/07 19:06:02 WARN DAGScheduler: Broadcasting large task binary with size 1157.1 KiB\n",
      "25/06/07 19:06:02 WARN DAGScheduler: Broadcasting large task binary with size 1174.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clinical Events Table - Finalized\n"
     ]
    }
   ],
   "source": [
    "chartevents_filtered = chartevents_df.select(\n",
    "    \"ICUSTAY_ID\", \"ITEMID\", \"VALUENUM\", \"CHARTTIME\"\n",
    ").join(\n",
    "    icu_stay_ids, \"ICUSTAY_ID\", \"inner\"\n",
    ").join(\n",
    "    base_icu_df.select(\"ICUSTAY_ID\", \"ICU_INTIME\"), \"ICUSTAY_ID\", \"inner\"\n",
    ").filter(\n",
    "    (col(\"VALUENUM\").isNotNull()) & \n",
    "    (col(\"CHARTTIME\").isNotNull()) &\n",
    "    col(\"CHARTTIME\").between(\n",
    "        col(\"ICU_INTIME\"), \n",
    "        col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "    )\n",
    ").cache()\n",
    "\n",
    "\n",
    "chartevents_with_categories = chartevents_filtered.join(\n",
    "    d_items_df.select(\"ITEMID\", \"CATEGORY\"), \"ITEMID\", \"left\"\n",
    ").cache()\n",
    "\n",
    "\n",
    "top_categories = chartevents_with_categories.groupBy(\"CATEGORY\").agg(\n",
    "    count(\"*\").alias(\"count\")\n",
    ").orderBy(\n",
    "    col(\"count\").desc()\n",
    ").limit(7).select(\"CATEGORY\").collect()\n",
    "\n",
    "top_categories = [cat for cat in [row[\"CATEGORY\"] for row in top_categories] if cat != None  ]\n",
    "\n",
    "chartevents_top_categories = chartevents_with_categories.filter(\n",
    "    col(\"CATEGORY\").isin(top_categories)\n",
    ").cache()\n",
    "\n",
    "patient_category_stats = chartevents_top_categories.groupBy(\"ICUSTAY_ID\").pivot(\n",
    "    \"CATEGORY\", top_categories\n",
    ").agg(\n",
    "    sql_sum(\"VALUENUM\").alias(\"_sum\"), \n",
    "    count(\"VALUENUM\").alias(\"_count\")\n",
    ") \n",
    "\n",
    "\n",
    "chartevents_filtered.unpersist()\n",
    "chartevents_with_categories.unpersist()\n",
    "chartevents_top_categories.unpersist()\n",
    "\n",
    "print(\"‚úÖ Clinical Events Table - Finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fde71d",
   "metadata": {},
   "source": [
    "### Laboratorial Events (LABEVENTS) Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77df4f3",
   "metadata": {},
   "source": [
    "**Features Processing Pipeline**\n",
    "\n",
    "**1. Initial Filtering & Extended Time Window Selection**\n",
    "- Extracted key lab test data: HADM_ID, ITEMID, numeric values (VALUENUM) and timestamps (CHARTTIME)\n",
    "- Applied three-way joining to:\n",
    "  - Filter to only hospital admissions in our base table\n",
    "  - Incorporate ICU admission timestamps (ICU_INTIME)\n",
    "- Implemented strict quality filters:\n",
    "  - Removed records with null lab values or timestamps\n",
    "  - Expanded window to include tests from **6 hours before to 24 hours after** ICU admission\n",
    "- Cached the resulting dataset for efficient downstream processing\n",
    "\n",
    "**2. Category Enrichment**\n",
    "- Joined with D_LABITEMS table to add test CATEGORY information\n",
    "- Maintained the enriched dataset for multiple transformation steps\n",
    "\n",
    "**3. Top Category Analysis**\n",
    "- Identified top 7 most frequent lab test categories\n",
    "- Filtered to include only these clinically significant categories\n",
    "- Created optimized dataset focused on top categories\n",
    "\n",
    "**4. Dual-Metric Statistical Aggregation**\n",
    "- Generated comprehensive per-admission statistics:\n",
    "  - Sum of all test values (using `_sum` suffix)\n",
    "  - Count of tests performed (using `_count` suffix)\n",
    "- Executed parallel pivot operations to maintain metric clarity\n",
    "\n",
    "**Output Schema:**\n",
    "The final DataFrame contains:\n",
    "- HADM_ID as the primary key\n",
    "- For each of the top 7 lab test categories:\n",
    "  - [CATEGORY]_sum - Sum of all test values in category\n",
    "  - [CATEGORY]_count - Number of tests performed in category\n",
    "\n",
    "\n",
    "This processing creates a temporally-aware lab dataset where each hospital admission has an aggregated test results from the critical 30-hour window around ICU admission. The pre-ICU window provides offers a complete picture of patient lab status during transition to intensive care. Besides that, the data set provides insights of both magnitude and frequency metrics for each test category, having numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ad3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:03 WARN DAGScheduler: Broadcasting large task binary with size 1052.2 KiB\n",
      "25/06/07 19:06:04 WARN DAGScheduler: Broadcasting large task binary with size 1147.4 KiB\n",
      "25/06/07 19:06:04 WARN DAGScheduler: Broadcasting large task binary with size 1173.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Laboratorial Events Table - Finalized\n"
     ]
    }
   ],
   "source": [
    "labevents_filtered = labevents_df \\\n",
    "    .select(\n",
    "        \"HADM_ID\",\n",
    "        \"ITEMID\",\n",
    "        col(\"VALUENUM\").cast(\"double\").alias(\"VALUENUM\"),\n",
    "        col(\"CHARTTIME\").cast(\"timestamp\").alias(\"CHARTTIME\")\n",
    "    ) \\\n",
    "    .join(hadm_ids, \"HADM_ID\", \"inner\") \\\n",
    "    .join(\n",
    "        base_icu_df.select(\"HADM_ID\", \"ICU_INTIME\"),\n",
    "        \"HADM_ID\", \"inner\"\n",
    "    ) \\\n",
    "    .filter(\n",
    "        col(\"VALUENUM\").isNotNull()\n",
    "    ) \\\n",
    "    .filter(col(\"CHARTTIME\").isNotNull()) \\\n",
    "    .filter(\n",
    "        col(\"CHARTTIME\").between(\n",
    "            col(\"ICU_INTIME\") - expr(\"INTERVAL 6 HOURS\"),\n",
    "            col(\"ICU_INTIME\") + expr(\"INTERVAL 24 HOURS\")\n",
    "        )\n",
    "    ) \\\n",
    "    .cache()\n",
    "\n",
    "labevents_with_categories = labevents_filtered \\\n",
    "    .join(d_labitems_df.select(\"ITEMID\", \"CATEGORY\"), \"ITEMID\", \"left\")\n",
    "\n",
    "top_lab_categories = labevents_with_categories \\\n",
    "    .groupBy(\"CATEGORY\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(7) \\\n",
    "    .select(\"CATEGORY\") \\\n",
    "    .collect()\n",
    "top_lab_categories = [row[\"CATEGORY\"] for row in top_lab_categories]\n",
    "\n",
    "labevents_top_categories = labevents_with_categories.filter(\n",
    "    col(\"CATEGORY\").isin(top_lab_categories)\n",
    ")\n",
    "\n",
    "patient_lab_category_stats = labevents_top_categories.groupBy(\"HADM_ID\", \"CATEGORY\") \\\n",
    "    .agg(\n",
    "        F.sum(F.col(\"VALUENUM\")).alias(\"sum_val\"),\n",
    "        F.count(F.lit(1)).alias(\"count_val\")\n",
    "    )\n",
    "\n",
    "sum_pivot = patient_lab_category_stats.groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"CATEGORY\", top_lab_categories) \\\n",
    "    .sum(\"sum_val\")\n",
    "\n",
    "count_pivot = patient_lab_category_stats.groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"CATEGORY\", top_lab_categories) \\\n",
    "    .sum(\"count_val\")\n",
    "\n",
    "for category in top_lab_categories:\n",
    "    count_pivot = count_pivot.withColumnRenamed(\n",
    "        category, f\"{category}_count\"\n",
    "    )\n",
    "    sum_pivot = sum_pivot.withColumnRenamed(\n",
    "        category, f\"{category}_sum\"\n",
    "    )\n",
    "\n",
    "final_lab_stats = sum_pivot.join(count_pivot, \"HADM_ID\", \"inner\")\n",
    "\n",
    "print(\"‚úÖ Laboratorial Events Table - Finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9959df4",
   "metadata": {},
   "source": [
    "### DIAGNOSIS Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f5434",
   "metadata": {},
   "source": [
    "**Features Processing Pipeline**\n",
    "\n",
    "**1. ICD-9 Code Classification**\n",
    "- Created mapping function that:\n",
    "  - Handles both numeric and alphanumeric codes (V-codes, E-codes)\n",
    "  - Classifies each code into 20 clinically meaningful chapters:\n",
    "    - 1-17: Standard ICD-9 ranges (Infectious to Injury)\n",
    "    - 18: Supplemental (V-codes)\n",
    "    - 19: External Injury (E-codes) \n",
    "    - 0/20: Unknown/Other categories\n",
    "\n",
    "**2. Top 3 Diagnosis Selection**\n",
    "- Used window functions to:\n",
    "  - Partition by hospital admission (HADM_ID)\n",
    "  - Order by diagnosis sequence number (SEQ_NUM)\n",
    "  - Select the first 3 diagnoses for each admission\n",
    "  - Cached results for efficient processing\n",
    "\n",
    "**3. Diagnosis Chapter Encoding**\n",
    "- Applied UDF to convert ICD-9 codes to chapter numbers\n",
    "- Created new column DISEASE_CHAPTER with encoded values\n",
    "\n",
    "**4. Table Construction**\n",
    "- Generated two feature types:\n",
    "  1. **Top 3 Diagnoses**:\n",
    "     - Pivoted to create columns for primary, secondary, and tertiary diagnoses\n",
    "     - Cast as integer types for modeling\n",
    "     - Filled missing values with -1\n",
    "  2. **Diagnosis Count**:\n",
    "     - Calculated total number of diagnoses per admission\n",
    "     - Joined with top 3 diagnoses\n",
    "\n",
    "\n",
    "**Output Schema:**\n",
    "The final DataFrame contains:\n",
    "- HADM_ID as the primary key\n",
    "- PRIMARY_DIAGNOSIS - Most relevant diagnose\n",
    "- SECONDARY_DIAGNOSIS - Second most relevant diagnose\n",
    "- TERTIARY_DIAGNOSIS - Third most relevant diagnose\n",
    "- TOTAL_DIAGNOSIS - Total amount of diagnoses the patient has\n",
    "\n",
    "**Clinical Value:**\n",
    "\n",
    "This processing transforms raw ICD-9 diagnoses into clinically meaningful features by classifying codes into 20 disease chapters while preserving diagnosis priority. It captures both specific conditions (through top 3 encoded diagnoses) and general comorbidity patterns (via total diagnosis count), using -1 for missing values. The resulting numerical features are immediately usable for comorbidity analysis, risk stratification, outcome prediction, and resource utilization studies, providing a compact yet information-rich representation of patient diagnoses for analytical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e7fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icd9_to_chapter(code):\n",
    "    code_str = str(code).strip()\n",
    "    \n",
    "    if code_str.startswith('V'):\n",
    "        return 18 #'Supplemental'\n",
    "    \n",
    "    if code_str.startswith('E'):\n",
    "        return 19 #'External_Injury'\n",
    "    \n",
    "    try:\n",
    "        numeric_part = code_str.split('.')[0] if '.' in code_str else code_str\n",
    "        code_num = float(numeric_part[:3])\n",
    "    except:\n",
    "        return 0 #'Unknown'\n",
    "    \n",
    "    if 1 <= code_num <= 139: return 1 #'Infectious'\n",
    "    elif 140 <= code_num <= 239: return 2 # 'Neoplasms'\n",
    "    elif 240 <= code_num <= 279: return 3 #'Endocrine'\n",
    "    elif 280 <= code_num <= 289: return 4 #'Blood'\n",
    "    elif 290 <= code_num <= 319: return 5 #'Mental'\n",
    "    elif 320 <= code_num <= 389: return 6 #'Nervous'\n",
    "    elif 390 <= code_num <= 459: return 7 #'Circulatory'\n",
    "    elif 460 <= code_num <= 519: return 8 #'Respiratory'\n",
    "    elif 520 <= code_num <= 579: return 9 #'Digestive'\n",
    "    elif 580 <= code_num <= 629: return 10 #'Genitourinary'\n",
    "    elif 630 <= code_num <= 679: return 11 #'Pregnancy'\n",
    "    elif 680 <= code_num <= 709: return 12 #'Skin'\n",
    "    elif 710 <= code_num <= 739: return 13 #'Musculoskeletal'\n",
    "    elif 740 <= code_num <= 759: return 14 #'Congenital'\n",
    "    elif 760 <= code_num <= 779: return 15 #'Perinatal'\n",
    "    elif 780 <= code_num <= 799: return 16 #'Ill-defined'\n",
    "    elif 800 <= code_num <= 999: return 17 #'Injury'\n",
    "    else: return 20 #'Other' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23e8e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Diagnosis Table - Finalized\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"HADM_ID\").orderBy(\"SEQ_NUM\")\n",
    "\n",
    "top_3_filtered = diagnoses_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") <= 3) \\\n",
    "    .cache()\n",
    "\n",
    "icd9_chapter_udf = udf(icd9_to_chapter, IntegerType())\n",
    "\n",
    "top_3_encoded = top_3_filtered.withColumn(\n",
    "    \"DISEASE_CHAPTER\", \n",
    "    icd9_chapter_udf(col(\"ICD9_CODE\"))\n",
    ")\n",
    "\n",
    "diagnosis_count = diagnoses_df.groupBy(\"HADM_ID\").count().withColumnRenamed(\"count\", \"TOTAL_DIAGNOSES\")\n",
    "\n",
    "diagnoses_df.unpersist()\n",
    "\n",
    "diagnosis_features = top_3_encoded \\\n",
    "    .groupBy(\"HADM_ID\") \\\n",
    "    .pivot(\"row_num\", [1, 2, 3]) \\\n",
    "    .agg(first(\"DISEASE_CHAPTER\")) \\\n",
    "    .select(\n",
    "        \"HADM_ID\",\n",
    "        col(\"1\").alias(\"PRIMARY_DIAGNOSIS\").cast(IntegerType()),\n",
    "        col(\"2\").alias(\"SECONDARY_DIAGNOSIS\").cast(IntegerType()),\n",
    "        col(\"3\").alias(\"TERTIARY_DIAGNOSIS\").cast(IntegerType())\n",
    "    ) \\\n",
    "    .join(diagnosis_count, \"HADM_ID\", \"left\")\n",
    "\n",
    "\n",
    "diagnosis_features = diagnosis_features.fillna(-1, subset=[\n",
    "    \"PRIMARY_DIAGNOSIS\",\n",
    "    \"SECONDARY_DIAGNOSIS\",\n",
    "    \"TERTIARY_DIAGNOSIS\"\n",
    "])\n",
    "\n",
    "\n",
    "print(\"‚úÖ Diagnosis Table - Finalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020bc3e",
   "metadata": {},
   "source": [
    "### Joining All Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fbb03",
   "metadata": {},
   "source": [
    "Finally, the final modeling dataset was done by joining all feature tables (base table, clinical events, laboratorial results, and diagnoses) while excluding identifier and timestamp columns. The code performs left joins to preserve all ICU stays and lastly, cleans up by unpersisting intermediate tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b543178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Joining all features from all tables...\n",
      "‚úÖ Final table containing all features created with 6 records and 33 features.\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Joining all features from all tables...\")\n",
    "\n",
    "exclude_columns = {\"ICUSTAY_ID\", \"HADM_ID\", \"SUBJECT_ID\", \"ICU_INTIME\", \"ICU_OUTTIME\"}\n",
    "\n",
    "modeling_dataset = base_icu_df \\\n",
    "    .join(patient_category_stats, \"ICUSTAY_ID\", \"left\") \\\n",
    "    .join(final_lab_stats, \"HADM_ID\", \"left\") \\\n",
    "    .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "    .select(*[name for name in base_icu_df \\\n",
    "        .join(patient_category_stats, \"ICUSTAY_ID\", \"left\") \\\n",
    "        .join(final_lab_stats, \"HADM_ID\", \"left\") \\\n",
    "        .join(diagnosis_features, \"HADM_ID\", \"left\") \\\n",
    "        .columns if name not in exclude_columns])\n",
    "\n",
    "base_icu_df.unpersist()\n",
    "patient_category_stats.unpersist()\n",
    "final_lab_stats.unpersist()\n",
    "diagnosis_features.unpersist()\n",
    "\n",
    "print(f\"‚úÖ Final table containing all features created with {modeling_dataset.count()} records and {len(modeling_dataset.columns)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee4f29",
   "metadata": {},
   "source": [
    "## **4.** Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202a65a",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11ccd0",
   "metadata": {},
   "source": [
    "Our null handling carefully distinguishes between missing tests and zero results. For `_count` columns (test frequency), nulls become 0, correctly indicating no tests were performed. For `_sum` columns (test values), we use -1 for nulls to distinguish true zero results from missing data. This preserves critical clinical distinctions: a zero result differs meaningfully from an untested patient.\n",
    "\n",
    "We retain all ICU stays because test availability varies by clinical need - some patients naturally won't receive certain tests based on their condition. This approach maintains dataset completeness while accurately representing both test presence/absence and actual results. The -1 placeholder prevents algorithms from misinterpreting missing data as zero-value results, which could distort predictive models. We validate this choice by showing the first 5 records and final dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b112aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Filling NULL entries...\n",
      "‚úÖ NULL entries filled!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Filling NULL entries...\")\n",
    "\n",
    "count_columns = [c for c in modeling_dataset.columns if c.endswith('_count')]\n",
    "for column in count_columns:\n",
    "    modeling_dataset = modeling_dataset.withColumn(column, when(col(column).isNull(), 0).otherwise(col(column)))\n",
    "\n",
    "sum_columns = [c for c in modeling_dataset.columns if c.endswith('_sum')]\n",
    "for column in sum_columns:\n",
    "    modeling_dataset = modeling_dataset.withColumn(column, when(col(column).isNull(), -1).otherwise(col(column)))\n",
    "\n",
    "print(\"‚úÖ NULL entries filled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e36997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/06/07 19:06:09 WARN DAGScheduler: Broadcasting large task binary with size 1548.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset doesn't contain NULL entries.\n"
     ]
    }
   ],
   "source": [
    "if modeling_dataset.na.drop().count() < modeling_dataset.count():\n",
    "    print(\"‚ùå Dataset still contains NULL entries.\")\n",
    "    null_counts = modeling_dataset.select(\n",
    "        [sum(col(c).isNull().cast(\"int\")).alias(c) for c in modeling_dataset.columns]\n",
    "    ).collect()[0]\n",
    "    null_counts_dict = {col: null_counts[col] for col in modeling_dataset.columns if null_counts[col]>0}\n",
    "    print(null_counts_dict)\n",
    "else:\n",
    "    print(\"‚úÖ Dataset doesn't contain NULL entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0208b",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f85c5",
   "metadata": {},
   "source": [
    "**Pipeline:**\n",
    "1. **Column Selection**: Identifies all `_sum` columns (aggregated clinical measurements) for normalization\n",
    "2. **Vector Assembly**: Combines selected features into a single vector column for efficient processing\n",
    "3. **Standard Scaling**: Applies Z-score normalization (mean=0, std=1) to ensure equal feature weighting\n",
    "4. **Column Reconstruction**: Splits scaled features back to original column structure while maintaining naming\n",
    "\n",
    "**Technical Implementation:**\n",
    "- Uses Spark ML pipelines for atomic transformation\n",
    "- Preserves invalid/missing values (handleInvalid=\"keep\")\n",
    "- Maintains original dataset structure after processing\n",
    "- Cleans up temporary processing columns\n",
    "\n",
    "**Output Validation:**\n",
    "- Confirms count of scaled columns\n",
    "\n",
    "This standardization ensures all continuous features contribute equally to machine learning models while preserving the dataset's interpretability and structure. The process handles edge cases (no `_sum` columns) gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d53a3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Applying StandardScaling to _sum columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:11 WARN DAGScheduler: Broadcasting large task binary with size 1433.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scaled 9 _sum columns\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Applying StandardScaling to _sum columns...\")\n",
    "std_columns = [c for c in modeling_dataset.columns if c.endswith('_sum')]\n",
    "\n",
    "if std_columns:\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=std_columns,\n",
    "        outputCol=\"features_to_scale\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_to_scale\",\n",
    "        outputCol=\"scaled_features\"\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    \n",
    "    scaler_model = pipeline.fit(modeling_dataset)\n",
    "    scaled_data = scaler_model.transform(modeling_dataset)\n",
    "    \n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "    \n",
    "    for i, col_name in enumerate(std_columns):\n",
    "        scaled_data = scaled_data.withColumn(\n",
    "            col_name,\n",
    "            scaled_data[\"scaled_array\"][i]\n",
    "        )\n",
    "    \n",
    "    modeling_dataset = scaled_data.drop(\"features_to_scale\", \"scaled_features\", \"scaled_array\")\n",
    "    \n",
    "    print(f\"‚úÖ Scaled {len(std_columns)} _sum columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No _sum columns found to scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f9733",
   "metadata": {},
   "source": [
    "The same pipeline was applied, using MinMax scaling (0-10 range) instead to `_count` columns, which track test/measurement frequencies. Unlike our StandardScaler approach for `_sum` values, MinMax better preserves the clinical interpretation of count data by maintaining the absolute zero baseline (where zero clearly indicates no tests performed). The 0-10 bound:\n",
    "- Prevents extreme values from dominating models\n",
    "- Maintains intuitive interpretation (5 = midpoint frequency)\n",
    "- Allows algorithms to properly weight frequently ordered tests versus rare ones.\n",
    "\n",
    "This scaling choice reflects that count variables have different statistical properties than continuous lab values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c477ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Applying MinMaxScaling to _count columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:13 WARN DAGScheduler: Broadcasting large task binary with size 1437.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scaled 9 _count columns using MinMax scaling\n",
      "‚úÖ Data set ready for Machine Learning!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Applying MinMaxScaling to _count columns...\")\n",
    "minmax_columns = [c for c in modeling_dataset.columns if c.endswith('_count')]\n",
    "\n",
    "if minmax_columns:\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=minmax_columns,\n",
    "        outputCol=\"features_to_scale\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"features_to_scale\",\n",
    "        outputCol=\"scaled_features\",\n",
    "        min=0,  \n",
    "        max=10   \n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    \n",
    "    scaler_model = pipeline.fit(modeling_dataset)\n",
    "    scaled_data = scaler_model.transform(modeling_dataset)\n",
    "    \n",
    "    scaled_data = scaled_data.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "    \n",
    "    for i, col_name in enumerate(minmax_columns):\n",
    "        scaled_data = scaled_data.withColumn(\n",
    "            col_name,\n",
    "            scaled_data[\"scaled_array\"][i]\n",
    "        )\n",
    "\n",
    "    modeling_dataset = scaled_data.drop(\"features_to_scale\", \"scaled_features\", \"scaled_array\")\n",
    "    \n",
    "    print(f\"‚úÖ Scaled {len(minmax_columns)} _count columns using MinMax scaling\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No _count columns found to scale\")\n",
    "    \n",
    "\n",
    "print(\"‚úÖ Data set ready for Machine Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e8b48",
   "metadata": {},
   "source": [
    "## **5.** Model Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b68c9a",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "The dataset is split into:\n",
    "- 90% for training\n",
    "- 10% for testing\n",
    "\n",
    "This separation ensures that the final evaluation is performed on completely unseen data, providing an unbiased estimate of model performance.\n",
    "\n",
    "\n",
    "Then, VectorAssembler is used to combine features (all columns except ICU_LOS_DAYS) into a single vector required by Spark ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "238feaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating train/test split...\n",
      "‚úÖ Data split completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:17 WARN DAGScheduler: Broadcasting large task binary with size 1627.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÜ Training samples: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:20 WARN DAGScheduler: Broadcasting large task binary with size 1627.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üß™ Test samples: 1\n",
      "Feature columns: ['AGE_AT_ICU_ADMISSION', 'GENDER_BINARY', 'CAME_FROM_ER', 'HAS_INSURANCE', 'ADMISSION_TYPE_ENCODED', 'ETHNICITY_ENCODED', 'MARITAL_STATUS_ENCODED', 'RELIGION_ENCODED', 'FIRST_UNIT_ENCODED', 'CHANGED_ICU_UNIT', 'Routine Vital Signs__sum', 'Routine Vital Signs__count', 'Respiratory__sum', 'Respiratory__count', 'Hemodynamics__sum', 'Hemodynamics__count', 'Labs__sum', 'Labs__count', 'Alarms__sum', 'Alarms__count', 'Pain/Sedation__sum', 'Pain/Sedation__count', 'Hematology_sum', 'Chemistry_sum', 'Blood Gas_sum', 'Hematology_count', 'Chemistry_count', 'Blood Gas_count', 'PRIMARY_DIAGNOSIS', 'SECONDARY_DIAGNOSIS', 'TERTIARY_DIAGNOSIS', 'TOTAL_DIAGNOSES']\n",
      "Target column: ICU_LOS_DAYS\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Creating train/test split...\")\n",
    "train_data, test_data = modeling_dataset.randomSplit([0.8, 0.2], seed=1)\n",
    "\n",
    "print(\"‚úÖ Data split completed.\")\n",
    "print(f\"   üöÜ Training samples: {train_data.count()}\")\n",
    "print(f\"   üß™ Test samples: {test_data.count()}\")\n",
    "\n",
    "feature_columns = [col for col in modeling_dataset.columns if col != 'ICU_LOS_DAYS']\n",
    "print(\"Feature columns:\", feature_columns)\n",
    "target_column = 'ICU_LOS_DAYS'\n",
    "print(\"Target column:\", target_column)\n",
    "\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,  \n",
    "    outputCol=\"features\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988113a",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics \n",
    "\n",
    "Next we configurate the evaluation metrics:\n",
    "\n",
    "- **RMSE** (Root Mean Squared Error): Measures the average magnitude of prediction errors, giving higher weight to larger errors. Lower is better.\n",
    "- **MAE** (Mean Absolute Error): Measures the average absolute difference between predicted and actual values. Easier to interpret; lower is better.\n",
    "- **R¬≤** (Coefficient of Determination): Indicates how well the model explains the variance in the target variable. Ranges from 0 to 1 (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75966a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìäSetting up evaluation metrics...\n",
      "‚úÖ Evaluation metrics configured\n"
     ]
    }
   ],
   "source": [
    "print(\"üìäSetting up evaluation metrics...\")\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=target_column, \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=target_column,\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=target_column,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "def evaluate_model(model_name, predictions):\n",
    "    rmse = rmse_evaluator.evaluate(predictions)\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name.upper()} Evaluation Metrics:\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   R¬≤: {r2:.4f}\")\n",
    "    \n",
    "    return rmse, mae, r2\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f303b",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "Three regression models are initialized:\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosted Trees Regressor\n",
    "- Linear Regression\n",
    "\n",
    "Each model is associated with a parameter grid for tuning its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9c9e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìäSetting up evaluation models...\n",
      "‚úÖ Models configured.\n"
     ]
    }
   ],
   "source": [
    "print(\"üìäSetting up evaluation models...\")\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=target_column,\n",
    "        predictionCol=\"prediction\"\n",
    "    ),\n",
    "    'GradientBoostedTrees': GBTRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=target_column,\n",
    "        predictionCol=\"prediction\"\n",
    "    ),\n",
    "    'LinearRegression': LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=target_column,\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForest': ParamGridBuilder() \\\n",
    "        .addGrid(models['RandomForest'].numTrees, [10, 20, 30]) \\\n",
    "        .addGrid(models['RandomForest'].maxDepth, [5, 10, 15]) \\\n",
    "        .build(),\n",
    "    'GradientBoostedTrees': ParamGridBuilder() \\\n",
    "        .addGrid(models['GradientBoostedTrees'].maxIter, [10, 20]) \\\n",
    "        .addGrid(models['GradientBoostedTrees'].maxDepth, [5, 10]) \\\n",
    "        .build(),\n",
    "    'LinearRegression': ParamGridBuilder() \\\n",
    "        .addGrid(models['LinearRegression'].regParam, [0.01, 0.1, 1.0]) \\\n",
    "        .addGrid(models['LinearRegression'].elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Models configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6dbe41",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "A 5-fold cross-validation is used to select the best model and hyperparameters on the training data only. During this step, the 90% training data is split internally into 5 folds:\n",
    "- 4 folds are used for training - 72%\n",
    "- 1 fold is used for validation - 18%\n",
    "\n",
    "This is repeated 5 times, rotating the validation fold each time.\n",
    "\n",
    "Although CrossValidator handles splitting internally, we still perform an initial train/test split to keep a separate 20% test set that remains untouched during training or tuning. This ensures reliable performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training models with 5-fold cross validation...\n",
      "\n",
      "üîç Training RANDOMFOREST model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:23 WARN DAGScheduler: Broadcasting large task binary with size 1629.7 KiB\n",
      "25/06/07 19:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:24 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:25 WARN DAGScheduler: Broadcasting large task binary with size 1699.3 KiB\n",
      "25/06/07 19:06:25 WARN DAGScheduler: Broadcasting large task binary with size 1699.7 KiB\n",
      "25/06/07 19:06:25 WARN DAGScheduler: Broadcasting large task binary with size 1699.2 KiB\n",
      "25/06/07 19:06:28 WARN DAGScheduler: Broadcasting large task binary with size 1629.6 KiB\n",
      "25/06/07 19:06:29 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:29 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:30 WARN DAGScheduler: Broadcasting large task binary with size 1699.3 KiB\n",
      "25/06/07 19:06:31 WARN DAGScheduler: Broadcasting large task binary with size 1699.7 KiB\n",
      "25/06/07 19:06:31 WARN DAGScheduler: Broadcasting large task binary with size 1699.2 KiB\n",
      "25/06/07 19:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:32 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:33 WARN DAGScheduler: Broadcasting large task binary with size 1699.3 KiB\n",
      "25/06/07 19:06:33 WARN DAGScheduler: Broadcasting large task binary with size 1699.7 KiB\n",
      "25/06/07 19:06:33 WARN DAGScheduler: Broadcasting large task binary with size 1699.2 KiB\n",
      "25/06/07 19:06:34 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:34 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:34 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:34 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:34 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:35 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:35 WARN DAGScheduler: Broadcasting large task binary with size 1700.6 KiB\n",
      "25/06/07 19:06:35 WARN DAGScheduler: Broadcasting large task binary with size 1701.8 KiB\n",
      "25/06/07 19:06:36 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/07 19:06:36 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:36 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:37 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:37 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:37 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:37 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:37 WARN DAGScheduler: Broadcasting large task binary with size 1700.6 KiB\n",
      "25/06/07 19:06:38 WARN DAGScheduler: Broadcasting large task binary with size 1701.8 KiB\n",
      "25/06/07 19:06:38 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/07 19:06:38 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:38 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:39 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:39 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:39 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:39 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:39 WARN DAGScheduler: Broadcasting large task binary with size 1700.6 KiB\n",
      "25/06/07 19:06:40 WARN DAGScheduler: Broadcasting large task binary with size 1701.8 KiB\n",
      "25/06/07 19:06:40 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/07 19:06:40 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:41 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:41 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:41 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:41 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:41 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:41 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:42 WARN DAGScheduler: Broadcasting large task binary with size 1701.8 KiB\n",
      "25/06/07 19:06:42 WARN DAGScheduler: Broadcasting large task binary with size 1703.1 KiB\n",
      "25/06/07 19:06:42 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/07 19:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:44 WARN DAGScheduler: Broadcasting large task binary with size 1701.8 KiB\n",
      "25/06/07 19:06:44 WARN DAGScheduler: Broadcasting large task binary with size 1703.1 KiB\n",
      "25/06/07 19:06:44 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/07 19:06:45 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:45 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:45 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:45 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:45 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:46 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 3 (= number of training instances)\n",
      "25/06/07 19:06:46 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:46 WARN DAGScheduler: Broadcasting large task binary with size 1701.8 KiB\n",
      "25/06/07 19:06:46 WARN DAGScheduler: Broadcasting large task binary with size 1703.1 KiB\n",
      "25/06/07 19:06:46 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/07 19:06:47 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:47 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:50 WARN DAGScheduler: Broadcasting large task binary with size 1629.7 KiB\n",
      "25/06/07 19:06:51 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:51 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:51 WARN DAGScheduler: Broadcasting large task binary with size 1694.1 KiB\n",
      "25/06/07 19:06:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:06:51 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:51 WARN DAGScheduler: Broadcasting large task binary with size 1699.6 KiB\n",
      "25/06/07 19:06:52 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/06/07 19:06:52 WARN DAGScheduler: Broadcasting large task binary with size 1703.7 KiB\n",
      "25/06/07 19:06:52 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/06/07 19:06:55 WARN DAGScheduler: Broadcasting large task binary with size 1629.6 KiB\n",
      "25/06/07 19:06:55 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Nothing has been added to this summarizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[feature_assembler, models[model_name]])\n\u001b[1;32m     10\u001b[0m cv \u001b[38;5;241m=\u001b[39m CrossValidator(\n\u001b[1;32m     11\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[1;32m     12\u001b[0m     estimatorParamMaps\u001b[38;5;241m=\u001b[39mparam_grids[model_name],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     collectSubModels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m cv_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m best_model \u001b[38;5;241m=\u001b[39m cv_model\u001b[38;5;241m.\u001b[39mbestModel\n\u001b[1;32m     23\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/util.py:435\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[1;32m    434\u001b[0m     session\u001b[38;5;241m.\u001b[39maddTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:120\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43meva\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, metric, model \u001b[38;5;28;01mif\u001b[39;00m collectSubModel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/evaluation.py:110\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:400\u001b[0m, in \u001b[0;36mtry_remote_evaluate.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m deserialize(properties)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Nothing has been added to this summarizer."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/07 19:06:57 WARN DAGScheduler: Broadcasting large task binary with size 1699.6 KiB\n",
      "25/06/07 19:06:57 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/06/07 19:06:57 WARN DAGScheduler: Broadcasting large task binary with size 1703.7 KiB\n",
      "25/06/07 19:06:58 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/06/07 19:06:58 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:06:58 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:06:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1699.6 KiB\n",
      "25/06/07 19:07:00 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/06/07 19:07:00 WARN DAGScheduler: Broadcasting large task binary with size 1703.7 KiB\n",
      "25/06/07 19:07:00 WARN DAGScheduler: Broadcasting large task binary with size 1702.6 KiB\n",
      "25/06/07 19:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:07:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:07:01 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:07:02 WARN DAGScheduler: Broadcasting large task binary with size 1700.9 KiB\n",
      "25/06/07 19:07:02 WARN DAGScheduler: Broadcasting large task binary with size 1707.1 KiB\n",
      "25/06/07 19:07:02 WARN DAGScheduler: Broadcasting large task binary with size 1708.6 KiB\n",
      "25/06/07 19:07:02 WARN DAGScheduler: Broadcasting large task binary with size 1704.9 KiB\n",
      "25/06/07 19:07:03 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:03 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:07:03 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:03 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:04 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:07:04 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:07:04 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:07:04 WARN DAGScheduler: Broadcasting large task binary with size 1700.9 KiB\n",
      "25/06/07 19:07:04 WARN DAGScheduler: Broadcasting large task binary with size 1707.1 KiB\n",
      "25/06/07 19:07:05 WARN DAGScheduler: Broadcasting large task binary with size 1708.6 KiB\n",
      "25/06/07 19:07:05 WARN DAGScheduler: Broadcasting large task binary with size 1704.9 KiB\n",
      "25/06/07 19:07:05 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:06 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:07:06 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:06 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:06 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:07:06 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:07:06 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:07:06 WARN DAGScheduler: Broadcasting large task binary with size 1700.9 KiB\n",
      "25/06/07 19:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1707.1 KiB\n",
      "25/06/07 19:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1708.6 KiB\n",
      "25/06/07 19:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1704.9 KiB\n",
      "25/06/07 19:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:07:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1702.1 KiB\n",
      "25/06/07 19:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1711.5 KiB\n",
      "25/06/07 19:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1714.2 KiB\n",
      "25/06/07 19:07:10 WARN DAGScheduler: Broadcasting large task binary with size 1706.8 KiB\n",
      "25/06/07 19:07:10 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:10 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1694.1 KiB\n",
      "25/06/07 19:07:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1702.1 KiB\n",
      "25/06/07 19:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1711.5 KiB\n",
      "25/06/07 19:07:12 WARN DAGScheduler: Broadcasting large task binary with size 1714.2 KiB\n",
      "25/06/07 19:07:12 WARN DAGScheduler: Broadcasting large task binary with size 1706.8 KiB\n",
      "25/06/07 19:07:12 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:13 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n",
      "25/06/07 19:07:13 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:13 WARN DAGScheduler: Broadcasting large task binary with size 1694.2 KiB\n",
      "25/06/07 19:07:13 WARN DAGScheduler: Broadcasting large task binary with size 1694.0 KiB\n",
      "25/06/07 19:07:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 5 (= number of training instances)\n",
      "25/06/07 19:07:13 WARN DAGScheduler: Broadcasting large task binary with size 1697.5 KiB\n",
      "25/06/07 19:07:14 WARN DAGScheduler: Broadcasting large task binary with size 1702.1 KiB\n",
      "25/06/07 19:07:14 WARN DAGScheduler: Broadcasting large task binary with size 1711.5 KiB\n",
      "25/06/07 19:07:14 WARN DAGScheduler: Broadcasting large task binary with size 1714.2 KiB\n",
      "25/06/07 19:07:14 WARN DAGScheduler: Broadcasting large task binary with size 1706.8 KiB\n",
      "25/06/07 19:07:15 WARN DAGScheduler: Broadcasting large task binary with size 1693.2 KiB\n",
      "25/06/07 19:07:15 WARN DAGScheduler: Broadcasting large task binary with size 1694.3 KiB\n"
     ]
    }
   ],
   "source": [
    "print(\"üèãÔ∏è Training models with 5-fold cross validation...\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nüîç Training {model_name.upper()} model...\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=[feature_assembler, models[model_name]])\n",
    "    \n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_grids[model_name],\n",
    "        evaluator=rmse_evaluator,\n",
    "        numFolds=5,\n",
    "        seed=42,\n",
    "        collectSubModels=False\n",
    "    )\n",
    "    \n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    test_predictions = best_model.transform(test_data)\n",
    "    \n",
    "    test_rmse = rmse_evaluator.evaluate(test_predictions)\n",
    "    test_mae = mae_evaluator.evaluate(test_predictions)\n",
    "    test_r2 = r2_evaluator.evaluate(test_predictions)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'test_metrics': {\n",
    "            'rmse': test_rmse,\n",
    "            'mae': test_mae,\n",
    "            'r2': test_r2\n",
    "        },\n",
    "        'cv_avg_metrics': cv_model.avgMetrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bebc86b",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After cross-validation identifies the best hyperparameter combination, the resulting model is evaluated on the unseen test set using RMSE, MAE, and R¬≤ to measure its predictive accuracy and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228498e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã FINAL RESULTS SUMMARY:\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n‚≠ê {model_name.upper()}:\")\n",
    "    print(f\"   Test RMSE: {result['test_metrics']['rmse']:.4f}\")\n",
    "    print(f\"   Test MAE: {result['test_metrics']['mae']:.4f}\")\n",
    "    print(f\"   Test R¬≤: {result['test_metrics']['r2']:.4f}\")\n",
    "    print(f\"   5-fold Avg RMSE: {np.mean(result['cv_avg_metrics']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccda59",
   "metadata": {},
   "source": [
    "## **6.** Performance Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb81bfb",
   "metadata": {},
   "source": [
    "por fazer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa86118",
   "metadata": {},
   "source": [
    "## **7.** Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6c9c0",
   "metadata": {},
   "source": [
    "por fazer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
